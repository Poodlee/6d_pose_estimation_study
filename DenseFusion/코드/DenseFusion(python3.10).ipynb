{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c098e945",
   "metadata": {},
   "source": [
    "# DenseFusion Framework\n",
    "---\n",
    "This notebook provides an in-depth explanation of the DenseFusion framework, including the training and evaluation pipelines, and detailed descriptions of auxiliary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef658c4",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "DenseFusion is a 6D object pose estimation approach that fuses color and depth information in a pixel-wise manner. Unlike methods that treat RGB-D data as a single input, DenseFusion’s architecture handles each modality independently and preserves their native structures, enabling more nuanced feature extraction. It then performs dense pixel-level fusion, leveraging the intrinsic mapping between color and depth data. Finally, instead of relying on time-consuming post-processing ICP, DenseFusion incorporates a differentiable iterative refinement module directly into its pipeline, jointly training it with the main network to achieve robust and efficient pose estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdc4b4",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "### Dataset: YCB-Video\n",
    "This dataset contains **133,827 RGB-D frames** across 92 videos. It provides:\n",
    "- 21 objects with 6D pose annotations.\n",
    "- Severe occlusions and symmetric objects.\n",
    "\n",
    "Dataset URL: [YCB-Video Dataset](https://rse-lab.cs.washington.edu/projects/posecnn/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0bdd61",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline\n",
    "The `train.py` script implements the training process for DenseFusion. The steps include:\n",
    "\n",
    "1. **Dataset Preparation**: Load and preprocess the YCB dataset.\n",
    "2. **Model Initialization**: Define `PoseNet` for initial pose estimation and `PoseRefineNet` for iterative refinement.\n",
    "3. **Optimization**: Train the model using pose estimation and refinement loss functions.\n",
    "4. **Training Loop**: Perform forward pass, compute losses, backpropagate, and update weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178c90e",
   "metadata": {},
   "source": [
    "#### 3-1. Dataset Preparation\n",
    "\n",
    "- Input Data\n",
    "\n",
    "The following input files are required for each data sample:\n",
    "\n",
    "| **File Type**    | **Description**                                                                                |\n",
    "|-------------------|------------------------------------------------------------------------------------------------|\n",
    "| `-color.png`      | RGB image containing objects in the scene.                                                    |\n",
    "| `-depth.png`      | Depth map representing the distance of each pixel in the scene from the camera.               |\n",
    "| `-label.png`      | Label map where each pixel indicates the class ID of the object it belongs to.                |\n",
    "| `-meta.mat`       | Metadata containing object poses (rotation, translation) and intrinsic camera parameters.     |\n",
    "\n",
    "---\n",
    "\n",
    "- Processing Steps\n",
    "\n",
    "The following steps are applied to prepare the dataset for training and evaluation:\n",
    "\n",
    "1. **Load Data**  \n",
    "   Load RGB (`-color.png`), depth (`-depth.png`), label (`-label.png`), and metadata (`-meta.mat`) files for the selected index.\n",
    "\n",
    "2. **Camera Parameter Selection**  \n",
    "   Choose the intrinsic camera parameters (`cx`, `cy`, `fx`, `fy`) based on the data type (real or synthetic).\n",
    "\n",
    "3. **Background Mask Creation**  \n",
    "   Create a mask for the background by detecting pixels in the label map with a value of `0`.\n",
    "\n",
    "4. **Synthetic Noise Augmentation (Optional)**  \n",
    "   Overlay random objects from the synthetic dataset onto the scene by combining masks and labels.\n",
    "\n",
    "5. **Object Selection**  \n",
    "   Randomly select an object (`obj[idx]`) from the metadata and compute its mask (`mask_label`) by combining label and depth masks.\n",
    "\n",
    "6. **Bounding Box Calculation**  \n",
    "   Compute the bounding box for the selected object using `get_bbox(mask_label)` to crop the relevant region.\n",
    "\n",
    "7. **Crop and Normalize Image**  \n",
    "   Crop the RGB image based on the bounding box and apply color normalization for input to the model.\n",
    "\n",
    "8. **3D Point Cloud Generation**  \n",
    "   Convert depth values within the bounding box to 3D points using the camera parameters (`cx`, `cy`, `fx`, `fy`).\n",
    "\n",
    "9. **Noise in Point Cloud (Optional)**  \n",
    "   Add Gaussian noise or translation noise to the 3D point cloud for augmentation.\n",
    "\n",
    "10. **Model Point Sampling**  \n",
    "    Sample a subset of 3D points from the CAD model of the selected object.\n",
    "\n",
    "11. **Ground-Truth Pose Transformation**  \n",
    "    Apply the ground-truth pose (rotation matrix `target_r` and translation vector `target_t`) to the sampled model points.\n",
    "\n",
    "12. **Point Selection**  \n",
    "    Select a fixed number of points (`num_pt`) from the 3D point cloud. Pad or shuffle if necessary to maintain consistency.\n",
    "\n",
    "---\n",
    "\n",
    "- Output Data\n",
    "\n",
    "The processed dataset outputs the following information:\n",
    "\n",
    "| **Name**         | **Shape**            | **Description**                                                                                  |\n",
    "|-------------------|----------------------|--------------------------------------------------------------------------------------------------|\n",
    "| `cloud`          | `(num_pt, 3)`        | 3D point cloud representing the object's position in the scene.                                  |\n",
    "| `choose`         | `(num_pt,)`          | Indices of valid points selected from the 3D point cloud.                                        |\n",
    "| `img_masked`     | `(3, H, W)`          | Cropped and normalized RGB image containing only the selected object.                            |\n",
    "| `target`         | `(num_pt, 3)`        | 3D target points for the object after applying the ground-truth pose.                            |\n",
    "| `model_points`   | `(num_pt_mesh, 3)`   | 3D model points sampled from the object's CAD model (size depends on `refine` setting).          |\n",
    "| `class_idx`      | `(1,)`               | Class ID of the selected object (zero-indexed).                                                 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebadec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy.ma as ma\n",
    "import random\n",
    "import scipy.io as scio\n",
    "\n",
    "def get_bbox(label: np.ndarray, img_width: int=480, img_length: int=640) -> tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Calculate the bounding box for the non-zero region of a label mask.\n",
    "\n",
    "    This function determines the minimal and maximal rows and columns that enclose \n",
    "    the non-zero values in the label array. It then adjusts the bounding box to align \n",
    "    with predefined border sizes and ensures the bounding box stays within the image boundaries.\n",
    "\n",
    "    Args:\n",
    "        label (np.ndarray): A 2D array representing the label mask where non-zero values \n",
    "                            indicate the region of interest.\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int, int, int]: The adjusted bounding box as (rmin, rmax, cmin, cmax),\n",
    "                                   representing the top, bottom, left, and right boundaries.\n",
    "    \"\"\"\n",
    "    # Crop 경계값 후보 목록 설정 (불러온 label 파일에 있는 물체에 대해)\n",
    "    border_list = [-1, 40, 80, 120, 160, 200, 240, 280, 320, 360, 400, 440, 480, 520, 560, 600, 640, 680]\n",
    "    rows, cols = np.any(label, axis=1), np.any(label, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    rmax, cmax = rmax + 1, cmax + 1 # 끝점 포함\n",
    "    \n",
    "    # 현재 경계 크기 계산\n",
    "    r_b, c_b = rmax - rmin, cmax - cmin\n",
    "    # 설정된 경계 목록에서 가장 가까운 경계값으로 확장\n",
    "    for tt in range(len(border_list)):\n",
    "        if r_b > border_list[tt] and r_b < border_list[tt + 1]:\n",
    "            r_b = border_list[tt + 1]\n",
    "            break\n",
    "    for tt in range(len(border_list)):\n",
    "        if c_b > border_list[tt] and c_b < border_list[tt + 1]:\n",
    "            c_b = border_list[tt + 1]\n",
    "            break\n",
    "    \n",
    "    # 중심을 기준으로 새로운 rmin, rmax, cmin, cmax 계산\n",
    "    center = [(rmin + rmax) // 2, (cmin + cmax) // 2]\n",
    "    rmin, rmax = center[0] - int(r_b // 2), center[0] + int(r_b // 2)\n",
    "    cmin, cmax = center[1] - int(c_b // 2), center[1] + int(c_b // 2)\n",
    "\n",
    "    # 이미지 크기 벗어날 때 조정\n",
    "    if rmin < 0:\n",
    "        delta = -rmin\n",
    "        rmin, rmax = 0, rmax + delta\n",
    "    if cmin < 0:\n",
    "        delta = -cmin\n",
    "        cmin, cmax = 0, cmax + delta\n",
    "    if rmax > img_width:\n",
    "        delta = rmax - img_width\n",
    "        rmin, rmax = max(0, rmin - delta), img_width\n",
    "    if cmax > img_length:\n",
    "        delta = cmax - img_length\n",
    "        cmin, cmax = max(0, cmin - delta), img_length\n",
    "\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, mode: str, num_pt: int, add_noise: bool, root: str, noise_trans: float, refine: bool):\n",
    "        \"\"\"\n",
    "        Initialize the PoseDataset class.\n",
    "\n",
    "        Args:\n",
    "            mode (str): The mode of the dataset, either 'train' or 'test'.\n",
    "            num_pt (int): The number of points to sample for each object.\n",
    "            add_noise (bool): Whether to add noise to the image and point cloud.\n",
    "            root (str): Root directory of the dataset.\n",
    "            noise_trans (float): The amount of noise to add to the point cloud.\n",
    "            refine (bool): Whether to refine the sampled model points.\n",
    "        \"\"\"\n",
    "        self.mode = mode \n",
    "        self.num_pt = num_pt \n",
    "        self.add_noise = add_noise \n",
    "        self.noise_trans = noise_trans\n",
    "        self.refine = refine\n",
    "        self.root = Path(root)\n",
    "\n",
    "        # Load dataset list\n",
    "        data_list_path = Path(f\"datasets/ycb/dataset_config/{mode}_data_list.txt\")\n",
    "        self.list = data_list_path.read_text().splitlines()\n",
    "        self.real = [line for line in self.list if line.startswith(\"data/\")]\n",
    "        self.syn = [line for line in self.list if line.startswith(\"data_syn/\")]\n",
    "\n",
    "        # Load 3D model points for each class\n",
    "        class_file_path = Path(\"datasets/ycb/dataset_config/classes.txt\")\n",
    "        self.cld = self._load_classes(class_file_path)\n",
    "\n",
    "        # Camera intrinsic parameters for two cameras\n",
    "        self.cam_params = {\n",
    "            \"cam_1\": {\"cx\": 312.9869, \"cy\": 241.3109, \"fx\": 1066.778, \"fy\": 1067.487},\n",
    "            \"cam_2\": {\"cx\": 323.7872, \"cy\": 279.6921, \"fx\": 1077.836, \"fy\": 1078.189},\n",
    "        }\n",
    "\n",
    "        # Image transformations\n",
    "        self.trancolor = transforms.ColorJitter(0.2, 0.2, 0.2, 0.05)\n",
    "        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        # Constants for symmetry, point sampling, and minimum points\n",
    "        self.symmetry_obj_idx = [12, 15, 18, 19, 20] # Objects with rotation symmetry\n",
    "        self.num_pt_mesh_small = 500 # N of points for Coarse modles\n",
    "        self.num_pt_mesh_large = 2600 # N of points for refined models\n",
    "        self.minimum_num_pt = 50 # Minimum number of valid points in an object\n",
    "\n",
    "        # Precomputed 2D arrays for pixel coordinates\n",
    "        # These are used to convert depth information into 3D point cloud coordinates.\n",
    "        self.xmap = np.array([[j for i in range(640)] for j in range(480)])\n",
    "        self.ymap = np.array([[i for i in range(640)] for j in range(480)])\n",
    "\n",
    "        # Minimum object number when operate Synthetic noise\n",
    "        self.front_num = 2\n",
    "\n",
    "    def _load_classes(self, class_file_path: Path):\n",
    "        cld = {}\n",
    "        with class_file_path.open() as class_file:\n",
    "            for class_id, class_name in enumerate(class_file, start=1):\n",
    "                points_path = self.root / f\"models/{class_name[:-1]}/points.xyz\"\n",
    "                points = np.loadtxt(points_path)\n",
    "                cld[class_id] = points\n",
    "        return cld\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns:\n",
    "            tuple: A tuple containing the following processed data:\n",
    "                - cloud (torch.Tensor): A (num_pt, 3) tensor of 3D point cloud coordinates.\n",
    "                    Represents the sampled 3D points of the object in the scene.\n",
    "                - choose (torch.Tensor): A (num_pt,) tensor of selected point indices\n",
    "                    from the 2D mask, ensuring a consistent number of points.\n",
    "                - img_masked (torch.Tensor): A (3, H, W) tensor of the cropped and normalized RGB image\n",
    "                    containing the object of interest.\n",
    "                - target (torch.Tensor): A (num_pt, 3) tensor of the target 3D points,\n",
    "                    representing the object transformed by the ground-truth pose.\n",
    "                - model_points (torch.Tensor): A (num_pt_mesh_small or num_pt_mesh_large, 3) tensor\n",
    "                    of the object's 3D model points, sampled from the CAD model.\n",
    "                - obj[idx]-1 (torch.Tensor): A single-element tensor containing the class index (0-based)\n",
    "                    of the selected object in the current sample.\n",
    "        '''\n",
    "        # Load RGB image, depth map, Label map and metadata\n",
    "        img_path = self.root / f\"{self.list[index]}-color.png\"\n",
    "        depth_path = self.root / f\"{self.list[index]}-depth.png\"\n",
    "        label_path = self.root / f\"{self.list[index]}-label.png\"\n",
    "        meta_path = self.root / f\"{self.list[index]}-meta.mat\"\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        depth = np.array(Image.open(depth_path))\n",
    "        label = np.array(Image.open(label_path))\n",
    "        meta = scio.loadmat(meta_path)\n",
    "\n",
    "        # Select camera parameters based on the data type\n",
    "        cam_params = (\n",
    "            self.cam_params[\"cam_2\"]\n",
    "            if self.list[index][:8] != \"data_syn\" and int(self.list[index][5:9]) >= 60\n",
    "            else self.cam_params[\"cam_1\"]\n",
    "        )\n",
    "        cam_cx, cam_cy, cam_fx, cam_fy = cam_params.values()\n",
    "\n",
    "        # Generate a mask for the background (0이면 계산에서 제외 ex. 0,1,1 -> --, 1, 1) 이후 --부분만 True가 됨.\n",
    "        mask_back = ma.getmaskarray(ma.masked_equal(label, 0))\n",
    "\n",
    "        # Add synthetic noise by overlaying random objects\n",
    "        add_front = False\n",
    "        if self.add_noise:\n",
    "            for _ in range(5): # Try up to 5 random objects\n",
    "                seed = random.choice(self.syn) # select random object in synthetic dataset\n",
    "                front = np.array(self.trancolor(Image.open(f\"{self.root}/{seed}-color.png\").convert(\"RGB\")))\n",
    "                front = np.transpose(front, (2, 0, 1))\n",
    "                f_label = np.array(Image.open(f\"{self.root}/{seed}-label.png\"))\n",
    "                front_label = np.unique(f_label).tolist()[1:] # 0(배경)을 제외한 객체 ID 목록 반환\n",
    "                if len(front_label) < self.front_num: # front_num(2개)보다 적게 있으면 현재 label 폐기\n",
    "                    continue\n",
    "                front_label = random.sample(front_label, self.front_num) # front_num(2개)개의 객체 선택 \n",
    "                for f_i in front_label: # 선택된 객체의 마스크 생성 \n",
    "                    mk = ma.getmaskarray(ma.masked_not_equal(f_label, f_i))\n",
    "                    if f_i == front_label[0]:\n",
    "                        mask_front = mk\n",
    "                    else:\n",
    "                        mask_front = mask_front * mk # 겹치는 영역 제거 및 선택된 객체 마스크 결합 (0 * 1 => 0)\n",
    "                t_label = label * mask_front\n",
    "                if len(t_label.nonzero()[0]) > 1000:\n",
    "                    label = t_label\n",
    "                    add_front = True\n",
    "                    break\n",
    "\n",
    "        # Retrieve the class IDs of all objects in the scene\n",
    "        obj = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "\n",
    "        # Randomly select a valid object and compute its mask\n",
    "        while True:\n",
    "            idx = np.random.randint(0, len(obj))\n",
    "            mask_depth = ma.getmaskarray(ma.masked_not_equal(depth, 0))\n",
    "            mask_label = ma.getmaskarray(ma.masked_equal(label, obj[idx]))\n",
    "            mask = mask_label * mask_depth\n",
    "            if len(mask.nonzero()[0]) > self.minimum_num_pt:\n",
    "                break\n",
    "\n",
    "        # Apply color jitter if noise is enabled\n",
    "        if self.add_noise:\n",
    "            img = self.trancolor(img)\n",
    "\n",
    "        # Compute the bounding box for the object (하나의 label에 대해 crop)\n",
    "        rmin, rmax, cmin, cmax = get_bbox(mask_label)\n",
    "        img = np.transpose(np.array(img)[:, :, :3], (2, 0, 1))[:, rmin:rmax, cmin:cmax]\n",
    "\n",
    "        # Synthesize the background if using synthetic data\n",
    "        if self.list[index][:8] == \"data_syn\":\n",
    "            seed = random.choice(self.real)\n",
    "            back = np.array(self.trancolor(Image.open(f\"{self.root}/{seed}-color.png\").convert(\"RGB\")))\n",
    "            back = np.transpose(back, (2, 0, 1))[:, rmin:rmax, cmin:cmax] # 배경 이미지 크롭하여 현재 객체의 bbox에 맞춤\n",
    "            img_masked = back * mask_back[rmin:rmax, cmin:cmax] + img\n",
    "        else:\n",
    "            img_masked = img\n",
    "\n",
    "\n",
    "        if self.add_noise and add_front:\n",
    "            # img_masked = (현재 이미지 객체 부분) + (front 이미지의 배경 부분) => Cutmix와 비슷\n",
    "            img_masked = img_masked * mask_front[rmin:rmax, cmin:cmax] + front[:, rmin:rmax, cmin:cmax] * ~(mask_front[rmin:rmax, cmin:cmax])\n",
    "\n",
    "        if self.list[index][:8] == \"data_syn\":\n",
    "            # 합성 데이터에 대해서 noise 추가\n",
    "            img_masked = img_masked + np.random.normal(loc=0.0, scale=7.0, size=img_masked.shape)\n",
    "\n",
    "        # Extract depth information\n",
    "        depth_masked = depth[rmin:rmax, cmin:cmax].flatten()[mask.flatten().nonzero()[0]]\n",
    "        xmap_masked = self.xmap[rmin:rmax, cmin:cmax].flatten()[mask.flatten().nonzero()[0]]\n",
    "        ymap_masked = self.ymap[rmin:rmax, cmin:cmax].flatten()[mask.flatten().nonzero()[0]]\n",
    "\n",
    "        # Compute 3D Point Cloud (카메라 좌표 기준, Z: pt2, X: pt0, Y: pt1)\n",
    "        # Z = (측정 거리) / 기준 거리, X = (x - cx) * Z / fx, Y = (y - cy) * Z / fy\n",
    "        cam_scale = meta['factor_depth'][0][0]\n",
    "        pt2 = depth_masked / cam_scale\n",
    "        pt0 = (ymap_masked - cam_cx) * pt2 / cam_fx\n",
    "        pt1 = (xmap_masked - cam_cy) * pt2 / cam_fy\n",
    "        cloud = np.stack((pt0, pt1, pt2), axis=-1)\n",
    "\n",
    "        # Add noise to the point cloud\n",
    "        if self.add_noise:\n",
    "            add_t = np.random.uniform(-self.noise_trans, self.noise_trans, 3)\n",
    "            cloud += add_t\n",
    "\n",
    "        # Sample model points for the object (3d 포인트 데이터가 매우 클 때 적절히 샘플링, refine하면 mesh_large만큼 남기고 아니면 mesh_small만 남김)\n",
    "        dellist = [j for j in range(0, len(self.cld[obj[idx]]))]\n",
    "        if self.refine:\n",
    "            dellist = random.sample(dellist, len(self.cld[obj[idx]]) - self.num_pt_mesh_large)\n",
    "        else:\n",
    "            dellist = random.sample(dellist, len(self.cld[obj[idx]]) - self.num_pt_mesh_small)\n",
    "        model_points = np.delete(self.cld[obj[idx]], dellist, axis=0)\n",
    "        \n",
    "        # Compute target transformation(4x4matrix, target_r: rotation, target_t: translation)\n",
    "        target_r = meta['poses'][:, :, idx][:, :3]\n",
    "        target_t = meta['poses'][:, :, idx][:, 3:4].flatten()\n",
    "        target = np.dot(model_points, target_r.T)\n",
    "        if self.add_noise:\n",
    "            target = np.add(target, target_t + add_t)\n",
    "        else:\n",
    "            target = np.add(target, target_t) + target_t        \n",
    "\n",
    "        # mask 값에서 0이 아닌 유효한 포인트의 인덱스 반환 (크기 부족하면 padding)\n",
    "        choose = mask[rmin:rmax, cmin:cmax].flatten().nonzero()[0]\n",
    "        if len(choose) > self.num_pt:\n",
    "            c_mask = np.zeros(len(choose), dtype=int)\n",
    "            c_mask[:self.num_pt] = 1\n",
    "            np.random.shuffle(c_mask)\n",
    "            choose = choose[c_mask.nonzero()]\n",
    "        else:\n",
    "            choose = np.pad(choose, (0, self.num_pt - len(choose)), 'wrap')\n",
    "\n",
    "        return (\n",
    "            torch.tensor(cloud.astype(np.float32)),\n",
    "            torch.tensor(choose.astype(np.int32), dtype=torch.long),\n",
    "            self.norm(torch.tensor(img_masked.astype(np.float32))),\n",
    "            torch.tensor(target.astype(np.float32)),\n",
    "            torch.tensor(model_points.astype(np.float32)),\n",
    "            torch.tensor([obj[idx] - 1], dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list)\n",
    "\n",
    "    def get_sym_list(self):\n",
    "        return self.symmetry_obj_idx\n",
    "\n",
    "    def get_num_points_mesh(self):\n",
    "        return self.num_pt_mesh_large if self.refine else self.num_pt_mesh_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af07ab",
   "metadata": {},
   "source": [
    "#### 3-2. Model Initialization\n",
    "\n",
    "PoseNet is the primary network designed to estimate the pose of objects in 3D space. It uses a ResNet-based PSPNet as a backbone for feature extraction and processes the combined features from RGB images and point cloud data.\n",
    "\n",
    "0. **PSPNet**: Deep learning model designed for pixel-wise semantic segmentation\n",
    "1. **PoseNet**: The primary network for pose estimation.\n",
    "2. **PoseRefineNet**: An optional refinement network for improving the initial pose predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e27028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSPNet \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import lib.extractors as extractors  # ResNet과 같은 백본을 제공하는 라이브러리\n",
    "\n",
    "# PSPModule: Pyramid Scene Parsing Module\n",
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
    "        super(PSPModule, self).__init__()\n",
    "        # Create a stage for each pooling size\n",
    "        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n",
    "        # Bottleneck layer to combine all features (4 * stages + original)\n",
    "        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def _make_stage(self, features, size):\n",
    "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))  # Adaptive average pooling (channel 두고 w,h를 줄읾)\n",
    "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)  # 1x1 convolution\n",
    "        return nn.Sequential(prior, conv)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        h, w = feats.size(2), feats.size(3)  # Get the height and width of the input\n",
    "        # Apply each stage and upsample to the original size\n",
    "        priors = [F.upsample(input=stage(feats), size=(h, w), mode='bilinear') for stage in self.stages] + [feats]\n",
    "        # Concatenate original features and pyramid features\n",
    "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
    "        return self.relu(bottle)  # Apply ReLU activation\n",
    "    \n",
    "# PSPUpsample: Upsampling Module for PSPNet\n",
    "class PSPUpsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(PSPUpsample, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),  # Upsampling by a factor of 2\n",
    "            nn.Conv2d(in_channels, out_channels, kernel=3, padding=1),  # 3x3 convolution\n",
    "            nn.PReLU()  # Parametric ReLU activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# PSPNet: Pyramid Scene Parsing Network\n",
    "class PSPNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Pyramid Scene Parsing Network (PSPNet).\n",
    "    Combines a feature extractor, a PSPModule, and an upsampling module\n",
    "    to perform pixel-wise classification(segmentation).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=21, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024,\n",
    "                    backend='resnet18', pretrained=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes: Number of output classes.\n",
    "            sizes: Pooling sizes for the PSPModule.\n",
    "            psp_size: Number of channels in the PSPModule's input.\n",
    "            deep_features_size: Number of channels for the deep feature classifier.\n",
    "            backend: Backbone model (e.g., 'resnet18', 'resnet50').\n",
    "            pretrained: Whether to use pretrained weights for the backbone.\n",
    "        \"\"\"\n",
    "        super(PSPNet, self).__init__()\n",
    "        # Load the feature extractor backend (e.g., resnet18) from extractors\n",
    "        self.feats = getattr(extractors, backend)(pretrained)\n",
    "        # PSP module for multi-scale feature aggregation\n",
    "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
    "        self.drop_1 = nn.Dropout2d(p=0.3)  # Dropout after the PSP module\n",
    "\n",
    "        # Upsampling modules for progressively refining the feature map\n",
    "        self.up_1 = PSPUpsample(1024, 256)\n",
    "        self.up_2 = PSPUpsample(256, 64)\n",
    "        self.up_3 = PSPUpsample(64, 64)\n",
    "\n",
    "        self.drop_2 = nn.Dropout2d(p=0.15)  # Dropout during upsampling\n",
    "        # Final convolution layer to output class probabilities\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=1),  # Reduce to 32 channels\n",
    "            nn.LogSoftmax(dim=1)  # Logarithm of softmax for multi-class probabilities\n",
    "        )\n",
    "\n",
    "        # Optional classifier for deep features (not used in the main segmentation pipeline)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(deep_features_size, 256),  # Fully connected layer\n",
    "            nn.ReLU(),  # Activation\n",
    "            nn.Linear(256, n_classes)  # Output layer for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the backbone\n",
    "        f, class_f = self.feats(x) \n",
    "        # Apply the PSP module\n",
    "        p = self.psp(f)\n",
    "        p = self.drop_1(p)  # Apply dropout\n",
    "\n",
    "        # Upsample and refine the feature map\n",
    "        p = self.up_1(p)\n",
    "        p = self.drop_2(p)\n",
    "        p = self.up_2(p)\n",
    "        p = self.drop_2(p)\n",
    "        p = self.up_3(p)\n",
    "\n",
    "        # Compute final pixel-wise class probabilities\n",
    "        return self.final(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2cf53",
   "metadata": {},
   "source": [
    "**PoseNet**\n",
    "1. **Input Data**:\n",
    "   - RGB image (`img`): Shape `(batch_size, 3, height, width)`\n",
    "   - Point cloud (`x`): Shape `(batch_size, 3, num_points)`\n",
    "   - Point-pixel mapping (`choose`): Maps point cloud to corresponding RGB pixels.\n",
    "   - Object indices (`obj`): Category indices for the objects.\n",
    "\n",
    "2. **Semantic Segmentation Features**:\n",
    "   - Pass the RGB image through `ModifiedResnet` to obtain semantic segmentation-like features.\n",
    "   - Output shape: `(batch_size, depth, height, width)`.\n",
    "\n",
    "3. **Point-Pixel Feature Mapping**:\n",
    "   - Flatten and gather RGB features (`emb`) corresponding to the points using `choose`.\n",
    "   - `emb`: Shape `(batch_size, depth, num_points)`.\n",
    "\n",
    "4. **Point Cloud and RGB Fusion**:\n",
    "   - Pass the point cloud (`x`) and gathered RGB features (`emb`) to `PoseNetFeat` for fusion.\n",
    "   - `PoseNetFeat` extracts a dense, fused feature map: Shape `(batch_size, 1408, num_points)`.\n",
    "\n",
    "5. **Pose Prediction**:\n",
    "   - Pass the fused feature map through fully connected layers to predict:\n",
    "     - **Rotation (`rx`)**: Quaternion, shape `(batch_size, num_objects, 4, num_points)`\n",
    "     - **Translation (`tx`)**: 3D vector, shape `(batch_size, num_objects, 3, num_points)`\n",
    "     - **Confidence (`cx`)**: Scalar, shape `(batch_size, num_objects, 1, num_points)`\n",
    "\n",
    "6. **Object-Specific Output**:\n",
    "   - Extract the pose outputs (`rx`, `tx`, `cx`) for specific objects using the object indices (`obj`).\n",
    "\n",
    "7. **Return Outputs**:\n",
    "   - Return rotation, translation, confidence, and detached RGB features for further processing or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoseNet\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lib.pspnet import PSPNet\n",
    "\n",
    "# PSPNet Models (Segmentation 수행)\n",
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "    'resnet101': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),\n",
    "    'resnet152': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152')\n",
    "}\n",
    "\n",
    "# Semantic segmentation 결과\n",
    "class ModifiedResnet(nn.Module):\n",
    "    def __init__(self, usegpu=True):\n",
    "        super().__init__()\n",
    "        # Use PSPNet with resnet18 backbone\n",
    "        self.model = psp_models['resnet18'.lower()]()\n",
    "        self.model = nn.DataParallel(self.model)  # Multi-GPU support\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone\n",
    "        return self.model(x)\n",
    "\n",
    "# Feature Extraction for PoseNet\n",
    "class PoseNetFeat(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        \"\"\"\n",
    "        Feature extraction module for PoseNet. \n",
    "        This module fuses RGB features (emb) and point cloud features (x) at a dense, per-point level.\n",
    "        \n",
    "        Args:\n",
    "            num_points (int): The number of points in the input point cloud.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        # 포인트 클라우드 및 임베딩 특징 추출기\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)  # 3D 포인트 클라우드 -> 64 채널\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)  # 64 -> 128 채널\n",
    "        self.e_conv1 = nn.Conv1d(32, 64, 1)  # 이미지 임베딩 -> 64 채널\n",
    "        self.e_conv2 = nn.Conv1d(64, 128, 1)  # 64 -> 128 채널\n",
    "        # 결합된 특징 처리기\n",
    "        self.conv5 = nn.Conv1d(256, 512, 1)  # 결합된 특징 -> 512 채널\n",
    "        self.conv6 = nn.Conv1d(512, 1024, 1)  # 512 -> 1024 채널\n",
    "        # 평균 풀링을 통한 전역 특징 추출\n",
    "        self.ap1 = nn.AvgPool1d(num_points)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        x = F.relu(self.conv1(x)) # process point cloud, Shape: (batch_size, 64, )\n",
    "        emb = F.relu(self.e_conv1(emb)) # process RGB embedding features, Shape: (batch_size, 64, )\n",
    "        pointfeat_1 = torch.cat((x, emb), dim=1) # Fuse point cloud and RGB features, Shape: (batch_size, 128, )\n",
    "\n",
    "        # Second layer of feature extraction\n",
    "        x = F.relu(self.conv2(x)) \n",
    "        emb = F.relu(self.e_conv2(emb))\n",
    "        pointfeat_2 = torch.cat((x, emb), dim=1) # Shape: (batch_size, 256, )\n",
    "\n",
    "        # process combined featrues\n",
    "        x = F.relu(self.conv5(pointfeat_2))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        \n",
    "        # Global featrue by average pooling\n",
    "        ap_x = self.ap1(x).view(-1, 1024, 1).repeat(1, 1, self.num_points)  # Shape: (batch_size, 1024, num_points)\n",
    "\n",
    "        return torch.cat([pointfeat_1, pointfeat_2, ap_x], 1)  # Shape: (batch_size, 128+256+1024, num_points)\n",
    "\n",
    "# PoseNet: Main Pose Estimation Network# PoseNet: Main Pose Estimation Network\n",
    "class PoseNet(nn.Module):\n",
    "    def __init__(self, num_points, num_obj):\n",
    "        \"\"\"\n",
    "        Main pose estimation network that predicts rotation, translation, and confidence for objects.\n",
    "\n",
    "        Args:\n",
    "            num_points (int): The number of points in the point cloud.\n",
    "            num_obj (int): The number of object categories.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "        # Submodules: Backbone and Feature Extractor\n",
    "        self.cnn = ModifiedResnet()  # ResNet-based feature extractor (pixel-wise segmentation)\n",
    "        self.feat = PoseNetFeat(num_points)  # Point cloud feature extractor\n",
    "\n",
    "        # Fully connected layers for rotation, translation, and confidence estimation\n",
    "        self.conv1_r = nn.Conv1d(1408, 640, 1) # For rotation\n",
    "        self.conv1_t = nn.Conv1d(1408, 640, 1) # For translation\n",
    "        self.conv1_c = nn.Conv1d(1408, 640, 1) # For confidence\n",
    "        self.conv2_r = nn.Conv1d(640, 256, 1)\n",
    "        self.conv2_t = nn.Conv1d(640, 256, 1)\n",
    "        self.conv2_c = nn.Conv1d(640, 256, 1)\n",
    "        self.conv3_r = nn.Conv1d(256, 128, 1)\n",
    "        self.conv3_t = nn.Conv1d(256, 128, 1)\n",
    "        self.conv3_c = nn.Conv1d(256, 128, 1)\n",
    "        self.conv4_r = nn.Conv1d(128, num_obj * 4, 1)  # Outputs quaternions\n",
    "        self.conv4_t = nn.Conv1d(128, num_obj * 3, 1)  # Outputs translations\n",
    "        self.conv4_c = nn.Conv1d(128, num_obj * 1, 1)  # Outputs confidence scores\n",
    "\n",
    "    def forward(self, img, x, choose, obj):\n",
    "        # Generate semantic segmentation-like feature map from the input image\n",
    "        out_img = self.cnn(img)\n",
    "        bs, di, _, _ = out_img.size()  # Batch size, Depth, Height, Width\n",
    "\n",
    "        # Embed image features into the point cloud space\n",
    "        emb = out_img.view(bs, di, -1) # Flatten spatial dimensions\n",
    "        choose = choose.repeat(1, di, 1)  # Repeat indices for sampling\n",
    "        emb = torch.gather(emb, 2, choose).contiguous() # Gather RGB features for selected points\n",
    "\n",
    "        # Process point cloud data\n",
    "        x = x.transpose(2, 1).contiguous()  # Transpose to match Conv1d input\n",
    "        ap_x = self.feat(x, emb) # Fused feature map\n",
    "\n",
    "        # Compute rotation, translation, and confidence\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))\n",
    "        cx = F.relu(self.conv1_c(ap_x))\n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "        cx = F.relu(self.conv2_c(cx))\n",
    "\n",
    "        rx = F.relu(self.conv3_r(rx))\n",
    "        tx = F.relu(self.conv3_t(tx))\n",
    "        cx = F.relu(self.conv3_c(cx))\n",
    "\n",
    "        # Final outputs reshaped for the given number of objects and points\n",
    "        rx = self.conv4_r(rx).view(bs, self.num_obj, 4, self.num_points)\n",
    "        tx = self.conv4_t(tx).view(bs, self.num_obj, 3, self.num_points)\n",
    "        cx = torch.sigmoid(self.conv4_c(cx)).view(bs, self.num_obj, 1, self.num_points)\n",
    "\n",
    "        # Select outputs for the given object indices\n",
    "        b = 0 # 첫 번째 배치\n",
    "        out_rx = torch.index_select(rx[b], 0, obj[b]) # Rotation\n",
    "        out_tx = torch.index_select(tx[b], 0, obj[b]) # Translation\n",
    "        out_cx = torch.index_select(cx[b], 0, obj[b]) # Confidence\n",
    "\n",
    "        # Transpose for consistency\n",
    "        out_rx = out_rx.contiguous().transpose(2, 1).contiguous()\n",
    "        out_cx = out_cx.contiguous().transpose(2, 1).contiguous()\n",
    "        out_tx = out_tx.contiguous().transpose(2, 1).contiguous()\n",
    "\n",
    "        return out_rx, out_tx, out_cx, emb.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fa499",
   "metadata": {},
   "source": [
    "**PoseRefineNet**\n",
    "1. **Input Data**:\n",
    "   - Point cloud (`x`): Shape `(batch_size, 3, num_points)`.\n",
    "   - Embedded RGB features (`emb`): Shape `(batch_size, 32, num_points)`.\n",
    "   - Object indices (`obj`): Indices of objects to refine.\n",
    "\n",
    "2. **Point Cloud and RGB Feature Fusion**:\n",
    "   - Pass the point cloud and embedded RGB features into `PoseRefineNetFeat`.\n",
    "   - Extract a global, fused feature vector of shape `(batch_size, 1024)` using average pooling.\n",
    "\n",
    "3. **Refinement Layers**:\n",
    "   - Pass the fused feature vector through fully connected layers to refine:\n",
    "     - **Rotation (`rx`)**: Shape `(batch_size, num_objects, 4)` (quaternion).\n",
    "     - **Translation (`tx`)**: Shape `(batch_size, num_objects, 3)` (3D vector).\n",
    "\n",
    "4. **Output**:\n",
    "   - Return refined rotation (`rx`) and translation (`tx`) predictions for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51fd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoseRefineNet\n",
    "# Feature Extraction for PoseRefineNet\n",
    "class PoseRefineNetFeat(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        \"\"\"\n",
    "        Extracts global features from fused point cloud and RGB data.\n",
    "\n",
    "        Args:\n",
    "            num_points (int): The number of points in the input point cloud.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        # Convolution layers for point cloud features\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)  # (3 -> 64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)  # (64 -> 128)\n",
    "\n",
    "        # Convolution layers for RGB embedding features\n",
    "        self.e_conv1 = nn.Conv1d(32, 64, 1)  # (32 -> 64)\n",
    "        self.e_conv2 = nn.Conv1d(64, 128, 1)  # (64 -> 128)\n",
    "\n",
    "        # Convolution layers for combined features\n",
    "        self.conv5 = nn.Conv1d(384, 512, 1)  # (128+256 -> 512)\n",
    "        self.conv6 = nn.Conv1d(512, 1024, 1)  # (512 -> 1024)\n",
    "\n",
    "        # Global average pooling\n",
    "        self.ap1 = nn.AvgPool1d(num_points)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        # Process point cloud features\n",
    "        x = F.relu(self.conv1(x))  # Shape: (batch_size, 64, num_points)\n",
    "        # Process RGB embedding features\n",
    "        emb = F.relu(self.e_conv1(emb))  # Shape: (batch_size, 64, num_points)\n",
    "        # Fuse point cloud and RGB features\n",
    "        pointfeat_1 = torch.cat([x, emb], dim=1)  # Shape: (batch_size, 128, num_points)\n",
    "\n",
    "        # Second layer of feature extraction\n",
    "        x = F.relu(self.conv2(x))  # Shape: (batch_size, 128, num_points)\n",
    "        emb = F.relu(self.e_conv2(emb))  # Shape: (batch_size, 128, num_points)\n",
    "        # Fuse again\n",
    "        pointfeat_2 = torch.cat([x, emb], dim=1)  # Shape: (batch_size, 256, num_points)\n",
    "\n",
    "        # Combine features and extract global features\n",
    "        pointfeat_3 = torch.cat([pointfeat_1, pointfeat_2], dim=1)  # Shape: (batch_size, 384, num_points)\n",
    "        x = F.relu(self.conv5(pointfeat_3))  # Shape: (batch_size, 512, num_points)\n",
    "        x = F.relu(self.conv6(x))  # Shape: (batch_size, 1024, num_points)\n",
    "\n",
    "        # Global average pooling\n",
    "        ap_x = self.ap1(x).view(-1, 1024)  # Shape: (batch_size, 1024)\n",
    "\n",
    "        return ap_x\n",
    "\n",
    "# PoseRefineNet: Refinement Network\n",
    "class PoseRefineNet(nn.Module):\n",
    "    def __init__(self, num_points, num_obj):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.num_obj = num_obj\n",
    "        self.feat = PoseRefineNetFeat(num_points)\n",
    "\n",
    "        # Fully connected layers for refining rotation\n",
    "        self.conv1_r = nn.Linear(1024, 512)\n",
    "        self.conv2_r = nn.Linear(512, 128)\n",
    "        self.conv3_r = nn.Linear(128, num_obj * 4)  # Outputs refined quaternions\n",
    "\n",
    "        # Fully connected layers for refining translation\n",
    "        self.conv1_t = nn.Linear(1024, 512)\n",
    "        self.conv2_t = nn.Linear(512, 128)\n",
    "        self.conv3_t = nn.Linear(128, num_obj * 3)  # Outputs refined translations\n",
    "\n",
    "    def forward(self, x, emb, obj):\n",
    "        bs = x.size(0)  # Batch size\n",
    "\n",
    "        # Transpose point cloud for processing\n",
    "        x = x.transpose(2, 1).contiguous()  # Shape: (batch_size, num_points, 3)\n",
    "        # Extract global features (per-pixel fusion)\n",
    "        ap_x = self.feat(x, emb)  # Shape: (batch_size, 1024)\n",
    "\n",
    "        # Refine rotation\n",
    "        rx = F.relu(self.conv1_r(ap_x))  # Shape: (batch_size, 512)\n",
    "        rx = F.relu(self.conv2_r(rx))  # Shape: (batch_size, 128)\n",
    "        rx = self.conv3_r(rx).view(bs, self.num_obj, 4)  # Shape: (batch_size, num_objects, 4)\n",
    "\n",
    "        # Refine translation\n",
    "        tx = F.relu(self.conv1_t(ap_x))  # Shape: (batch_size, 512)\n",
    "        tx = F.relu(self.conv2_t(tx))  # Shape: (batch_size, 128)\n",
    "        tx = self.conv3_t(tx).view(bs, self.num_obj, 3)  # Shape: (batch_size, num_objects, 3)\n",
    "\n",
    "        return rx, tx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff0016",
   "metadata": {},
   "source": [
    "#### 3-3. Optimization (with loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34309fed",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbe5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "opt = parse_arguments()\n",
    "estimator = PoseNet(num_points=opt.num_points, num_obj=opt.num_objects).cuda()\n",
    "refiner = PoseRefineNet(num_points=opt.num_points, num_obj=opt.num_objects).cuda()\n",
    "\n",
    "optimizer = optim.Adam(refiner.parameters() if opt.resume_refinenet else estimator.parameters(), lr=opt.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f826c",
   "metadata": {},
   "source": [
    "**Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1acf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from lib.knn.__init__ import KNearestNeighbor\n",
    "\n",
    "def loss_calculation(pred_r, pred_t, pred_c, target, model_points, idx, points, w, refine, num_point_mesh, sym_list):\n",
    "\n",
    "    knn = KNearestNeighbor(1) # KNN module for finding 1-closest point (symmetric objects)\n",
    "    bs, num_p, _ = pred_c.size() \n",
    "\n",
    "    # Normalize predicted rotation (크기가 1이 되게 함)\n",
    "    pred_r = pred_r / torch.norm(pred_r, dim=2, keepdim=True)\n",
    "\n",
    "    # Convert quaternions to 3x3 rotation matrix\n",
    "    # https://gofo-coding.tistory.com/entry/Orientation-Rotation\n",
    "    # Quaternion: [q0 (scalar), q1, q2, q3 (vector parts)]\n",
    "    # Rotation equation: v' = q * v * q^*\n",
    "    #   - v: Input vector (extended to a pure quaternion, v = [0, x, y, z])\n",
    "    #   - q^*: Conjugate of quaternion q\n",
    "    # Result: v' = R * v, where R is the rotation matrix derived from q\n",
    "    \n",
    "    # Rotation matrix R:\n",
    "    # R = [[1 - 2(q2^2 + q3^2), 2(q1q2 - q0q3),   2(q0q2 + q1q3)],\n",
    "    #      [2(q1q2 + q0q3),     1 - 2(q1^2 + q3^2), 2(q2q3 - q0q1)],\n",
    "    #      [2(q1q3 - q0q2),     2(q0q1 + q2q3),   1 - 2(q1^2 + q2^2)]]\n",
    "    base = torch.cat([\n",
    "        (1.0 - 2.0 * (pred_r[..., 2]**2 + pred_r[..., 3]**2)).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 1] * pred_r[..., 2] - 2.0 * pred_r[..., 0] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 0] * pred_r[..., 2] + 2.0 * pred_r[..., 1] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 1] * pred_r[..., 2] + 2.0 * pred_r[..., 0] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (1.0 - 2.0 * (pred_r[..., 1]**2 + pred_r[..., 3]**2)).unsqueeze(-1),\n",
    "        (-2.0 * pred_r[..., 0] * pred_r[..., 1] + 2.0 * pred_r[..., 2] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (-2.0 * pred_r[..., 0] * pred_r[..., 2] + 2.0 * pred_r[..., 1] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 0] * pred_r[..., 1] + 2.0 * pred_r[..., 2] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (1.0 - 2.0 * (pred_r[..., 1]**2 + pred_r[..., 2]**2)).unsqueeze(-1)\n",
    "    ], dim=-1).view(bs * num_p, 3, 3) # Shape (batch,num_point,4)->(batch,num_point,3,3)\n",
    "\n",
    "    # Prepare inputs for loss calculation (model_points, target을 입력 데이터와 맞추기 위해 변환)\n",
    "    # model_points: cad 모델에서 샘플링된 3d 점\n",
    "    model_points = model_points.view(bs, 1, num_point_mesh, 3).expand(-1, num_p, -1, -1).reshape(bs * num_p, num_point_mesh, 3)\n",
    "    # target: GT로, 물체의 실제 위치와 회전을 반영한 점 집합\n",
    "    target = target.view(bs, 1, num_point_mesh, 3).expand(-1, num_p, -1, -1).reshape(bs * num_p, num_point_mesh, 3)\n",
    "    pred_t = pred_t.view(bs * num_p, 1, 3) # 예측된 translation\n",
    "    points = points.view(bs * num_p, 1, 3) # 입력 point cloud\n",
    "    pred_c = pred_c.view(bs * num_p) # 예측된 confidence\n",
    "\n",
    "    # Apply the rotation and translation to model points (base: 예측 rotation)\n",
    "    # torch.bmm: (bs*num_p,num_point_mesh,3) x (bs*num_p,3,3) = (bs*num_p,num_point_mesh,3)\n",
    "    pred = torch.bmm(model_points, base.transpose(2, 1)) + points + pred_t\n",
    "\n",
    "    # Handle symmetric objects\n",
    "    if not refine and idx[0].item() in sym_list:\n",
    "        # 1. 타겟과 예측 점의 차원 재구성\n",
    "        target = target[0].permute(1, 0).reshape(3, -1) # Shape (bs*num_p,num_point_mesh, 3) -> (3,bs*num_p*num_point_mesh)\n",
    "        pred = pred.permute(2, 0, 1).reshape(3, -1) # Shape (bs*num_p,num_point_mesh, 3) -> (3,bs*num_p*num_point_mesh)\n",
    "        \n",
    "        # 2. KNN을 사용하여 가장 가까운 타겟 점 찾기 \n",
    "        # knn할 때 (batch_size, num_points, dimension) 해야 함 -> unsqueeze\n",
    "        # 출력: (num_points_pred, 1)\n",
    "        inds = knn(target.unsqueeze(0), pred.unsqueeze(0)).squeeze(0) - 1\n",
    "        \n",
    "        # 3. 매칭된 타겟 점을 재구성\n",
    "        target = target[:, inds].reshape(3, bs * num_p, num_point_mesh).permute(1, 2, 0) # Shape (bs*num_p,num_point_mesh,3)\n",
    "        pred = pred.reshape(3, bs * num_p, num_point_mesh).permute(1, 2, 0) # Shape (bs*num_p,num_point_mesh,3)\n",
    "\n",
    "    # Calculate loss\n",
    "    dis = torch.norm(pred - target, dim=2).mean(dim=1) # (batch_size * num_points)\n",
    "    loss = torch.mean(dis * pred_c - w * torch.log(pred_c + 1e-8))\n",
    "\n",
    "    # Select the best matching prediction\n",
    "    pred_c = pred_c.view(bs, num_p)\n",
    "    dis = dis.view(bs, num_p)\n",
    "    _, which_max = torch.max(pred_c, dim=1) # return: value, index 각 배치마다 최고 찾기\n",
    "\n",
    "    t = pred_t[which_max[0]] + points[which_max[0]] # 신뢰도 가장 높은 포인트 기준으로 새로운 변환 벡터\n",
    "    ori_base = base[which_max[0]].unsqueeze(0) # 신뢰도 가장 높은 포인트 회전 행렬(base)를 선택하여 새로운 기준 좌표계 설정\n",
    "\n",
    "    # Transform points and targets for refinement (새로운 좌표계로 정렬)\n",
    "    # 신뢰도가 가장 높은 포인트 위치 원점 설정, 예측된 회전 행렬 기준으로 설정\n",
    "    new_points = torch.bmm(points - t.unsqueeze(1), ori_base.transpose(2, 1))\n",
    "    new_target = torch.bmm(target[0] - t.unsqueeze(1), ori_base.transpose(2, 1))\n",
    "\n",
    "    return loss, dis[0][which_max[0]], new_points.detach(), new_target.detach()\n",
    "\n",
    "class Loss(_Loss):\n",
    "    def __init__(self, num_points_mesh, sym_list):\n",
    "        super().__init__(reduction='mean')\n",
    "        self.num_pt_mesh = num_points_mesh\n",
    "        self.sym_list = sym_list\n",
    "\n",
    "    def forward(self, pred_r, pred_t, pred_c, target, model_points, idx, points, w, refine):\n",
    "        return loss_calculation(pred_r, pred_t, pred_c, target, model_points, idx, points, w, refine, self.num_pt_mesh, self.sym_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c9f0b",
   "metadata": {},
   "source": [
    "**Loss_refine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d44030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "import torch\n",
    "from lib.knn.__init__ import KNearestNeighbor\n",
    "\n",
    "def loss_calculation(pred_r, pred_t, target, model_points, idx, points, num_point_mesh, sym_list):\n",
    "    knn = KNearestNeighbor(1)\n",
    "\n",
    "    # Normalize predicted rotation\n",
    "    pred_r = pred_r / torch.norm(pred_r, dim=2, keepdim=True)\n",
    "\n",
    "    # Construct rotation matrices from quaternions\n",
    "    base = torch.cat([\n",
    "        (1.0 - 2.0 * (pred_r[..., 2]**2 + pred_r[..., 3]**2)).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 1] * pred_r[..., 2] - 2.0 * pred_r[..., 0] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 0] * pred_r[..., 2] + 2.0 * pred_r[..., 1] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 1] * pred_r[..., 2] + 2.0 * pred_r[..., 0] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (1.0 - 2.0 * (pred_r[..., 1]**2 + pred_r[..., 3]**2)).unsqueeze(-1),\n",
    "        (-2.0 * pred_r[..., 0] * pred_r[..., 1] + 2.0 * pred_r[..., 2] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (-2.0 * pred_r[..., 0] * pred_r[..., 2] + 2.0 * pred_r[..., 1] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (2.0 * pred_r[..., 0] * pred_r[..., 1] + 2.0 * pred_r[..., 2] * pred_r[..., 3]).unsqueeze(-1),\n",
    "        (1.0 - 2.0 * (pred_r[..., 1]**2 + pred_r[..., 2]**2)).unsqueeze(-1)\n",
    "    ], dim=-1).view(-1, 3, 3)\n",
    "\n",
    "    # Prepare inputs for loss calculation\n",
    "    model_points = model_points.view(1, 1, num_point_mesh, 3).expand(-1, pred_r.size(1), -1, -1).reshape(-1, num_point_mesh, 3)\n",
    "    target = target.view(1, 1, num_point_mesh, 3).expand(-1, pred_r.size(1), -1, -1).reshape(-1, num_point_mesh, 3)\n",
    "    pred_t = pred_t.view(-1, 1, 3)\n",
    "    pred = torch.bmm(model_points, base.transpose(2, 1)) + pred_t\n",
    "\n",
    "    # Handle symmetric objects\n",
    "    if idx[0].item() in sym_list:\n",
    "        target = target[0].permute(1, 0).reshape(3, -1)\n",
    "        pred = pred.permute(2, 0, 1).reshape(3, -1)\n",
    "        inds = knn(target.unsqueeze(0), pred.unsqueeze(0)).squeeze(0) - 1\n",
    "        target = target[:, inds].reshape(3, pred_r.size(0), num_point_mesh).permute(1, 2, 0)\n",
    "        pred = pred.reshape(3, pred_r.size(0), num_point_mesh).permute(1, 2, 0)\n",
    "\n",
    "    # Calculate distance\n",
    "    dis = torch.mean(torch.norm(pred - target, dim=2), dim=1)\n",
    "\n",
    "    # Transform points and targets for refinement\n",
    "    t = pred_t[0]\n",
    "    ori_base = base[0].unsqueeze(0)\n",
    "    points = points.view(1, -1, 3)\n",
    "    new_points = torch.bmm((points - t.unsqueeze(1)), ori_base.transpose(2, 1))\n",
    "    new_target = torch.bmm((target[0] - t.unsqueeze(1)), ori_base.transpose(2, 1))\n",
    "\n",
    "    del knn\n",
    "    return dis, new_points.detach(), new_target.detach()\n",
    "\n",
    "class Loss_refine(_Loss):\n",
    "    def __init__(self, num_points_mesh, sym_list):\n",
    "        super().__init__(reduction='mean')\n",
    "        self.num_pt_mesh = num_points_mesh\n",
    "        self.sym_list = sym_list\n",
    "\n",
    "    def forward(self, pred_r, pred_t, target, model_points, idx, points):\n",
    "        return loss_calculation(pred_r, pred_t, target, model_points, idx, points, self.num_pt_mesh, self.sym_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07292d",
   "metadata": {},
   "source": [
    "#### 3-4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f129d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets.ycb.dataset import PoseDataset as PoseDataset_ycb\n",
    "from datasets.linemod.dataset import PoseDataset as PoseDataset_linemod\n",
    "from lib.network import PoseNet, PoseRefineNet\n",
    "from lib.loss import Loss\n",
    "from lib.loss_refiner import Loss_refine\n",
    "from lib.utils import setup_logger\n",
    "\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    \"\"\"Parse command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"DenseFusion Training\")\n",
    "    parser.add_argument('--dataset', type=str, default='ycb', help='Dataset to use: ycb or linemod')\n",
    "    parser.add_argument('--dataset_root', type=str, default='data/', help='Root directory of the dataset')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training')\n",
    "    parser.add_argument('--workers', type=int, default=4, help='Number of data loading workers')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='Initial learning rate')\n",
    "    parser.add_argument('--lr_rate', type=float, default=0.3, help='Learning rate decay factor')\n",
    "    parser.add_argument('--w', type=float, default=0.015, help='Loss weight factor')\n",
    "    parser.add_argument('--w_rate', type=float, default=0.3, help='Weight decay factor')\n",
    "    parser.add_argument('--decay_margin', type=float, default=0.016, help='Margin for learning rate decay')\n",
    "    parser.add_argument('--refine_margin', type=float, default=0.013, help='Margin to start refinement training')\n",
    "    parser.add_argument('--noise_trans', type=float, default=0.03, help='Translation noise range for training data')\n",
    "    parser.add_argument('--iteration', type=int, default=2, help='Number of refinement iterations')\n",
    "    parser.add_argument('--epoch', type=int, default=500, help='Number of epochs to train')\n",
    "    parser.add_argument('--resume_posenet', type=str, default='', help='Path to a pre-trained PoseNet model')\n",
    "    parser.add_argument('--resume_refinenet', type=str, default='', help='Path to a pre-trained PoseRefineNet model')\n",
    "    parser.add_argument('--seed', type=int, default=7, help='Random seed Number')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def initialize_model(arg: argparse.Namespace) -> tuple[PoseNet, PoseRefineNet, optim.Optimizer]:\n",
    "    \"\"\"Initialize models and optimizer.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    estimator = PoseNet(num_points=arg.num_points, num_obj=arg.num_objects).to(device)\n",
    "    refiner = PoseRefineNet(num_points=arg.num_points, num_obj=arg.num_objects).to(device)\n",
    "\n",
    "    # Load pre-trained weights if specified\n",
    "    if arg.resume_posenet:\n",
    "        estimator.load_state_dict(torch.load(os.path.join(arg.outf, arg.resume_posenet), map_location=device))\n",
    "    if arg.resume_refinenet:\n",
    "        refiner.load_state_dict(torch.load(os.path.join(arg.outf, arg.resume_refinenet), map_location=device))\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(refiner.parameters() if arg.resume_refinenet else estimator.parameters(), lr=arg.lr)\n",
    "    return estimator, refiner, optimizer\n",
    "\n",
    "def load_dataset(arg: argparse.Namespace, phase: str) -> DataLoader:\n",
    "    \"\"\"Load the appropriate dataset.\"\"\"\n",
    "    match arg.dataset:\n",
    "        case 'ycb':\n",
    "            dataset_class = PoseDataset_ycb\n",
    "        case 'linemod':\n",
    "            dataset_class = PoseDataset_linemod\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported dataset: {arg.dataset}\")\n",
    "\n",
    "    dataset = dataset_class(phase, arg.num_points, phase == 'train', arg.dataset_root, arg.noise_trans, arg.refine_start)\n",
    "    return DataLoader(dataset, batch_size=arg.batch_size if phase == 'train' else 1, shuffle=(phase == 'train'), num_workers=arg.workers)\n",
    "\n",
    "def test_phase(arg: argparse.Namespace, estimator: PoseNet, refiner: PoseRefineNet, test_loader: DataLoader, criterion: Loss, criterion_refine: Loss_refine) -> float:\n",
    "    \"\"\"Perform testing and return the average distance.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    estimator.eval()\n",
    "    refiner.eval()\n",
    "    total_dis = 0.0\n",
    "    test_count = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        points, choose, img, target, model_points, idx = (x.to(device) for x in data)\n",
    "\n",
    "        # Forward pass for PoseNet\n",
    "        pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx)\n",
    "        _, dis, new_points, new_target = criterion(pred_r, pred_t, pred_c, target, model_points, idx, points, arg.w, arg.refine_start)\n",
    "\n",
    "        # Refinement iterations\n",
    "        if arg.refine_start:\n",
    "            for _ in range(arg.iteration):\n",
    "                pred_r, pred_t = refiner(new_points, emb, idx)\n",
    "                dis, new_points, new_target = criterion_refine(pred_r, pred_t, new_target, model_points, idx, new_points)\n",
    "\n",
    "        total_dis += dis.item()\n",
    "        test_count += 1\n",
    "\n",
    "    avg_dis = total_dis / test_count\n",
    "    print(f\"Test complete: Average distance = {avg_dis:.6f}\")\n",
    "    return avg_dis\n",
    "\n",
    "def main():\n",
    "    arg = parse_arguments()\n",
    "\n",
    "    # Seed initialization for reproducibility\n",
    "    random.seed(arg.seed)\n",
    "    torch.manual_seed(arg.seed)\n",
    "\n",
    "    # Dataset-specific parameters\n",
    "    match arg.dataset:\n",
    "        case 'ycb':\n",
    "            arg.num_objects = 21\n",
    "            arg.num_points = 1000\n",
    "            arg.outf = 'trained_models/ycb'\n",
    "            arg.log_dir = 'experiments/logs/ycb'\n",
    "            arg.repeat_epoch = 1\n",
    "        case 'linemod':\n",
    "            arg.num_objects = 13\n",
    "            arg.num_points = 500\n",
    "            arg.outf = 'trained_models/linemod'\n",
    "            arg.log_dir = 'experiments/logs/linemod'\n",
    "            arg.repeat_epoch = 20\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown dataset: {arg.dataset}\")\n",
    "\n",
    "    # Initialize models, datasets, and optimizer\n",
    "    estimator, refiner, optimizer = initialize_model(arg)\n",
    "    train_loader = load_dataset(arg, 'train')\n",
    "    test_loader = load_dataset(arg, 'test')\n",
    "\n",
    "    criterion = Loss(arg.num_points_mesh, arg.sym_list)\n",
    "    criterion_refine = Loss_refine(arg.num_points_mesh, arg.sym_list)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Training loop\n",
    "    best_test = float('inf')\n",
    "    start_time_str = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())  # Start time in readable format\n",
    "\n",
    "    for epoch in range(arg.epoch):\n",
    "        # Training phase\n",
    "        estimator.train()\n",
    "        for data in train_loader:\n",
    "            # Unpack data and send to device\n",
    "            points, choose, img, target, model_points, idx = (x.to(device) for x in data)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx)\n",
    "            loss, dis, new_points, new_target = criterion(pred_r, pred_t, pred_c, target, model_points, idx, points, arg.w, arg.refine_start)\n",
    "            \n",
    "            # Backward pass\n",
    "            if arg.refine_start:\n",
    "                for _ in range(arg.iteration):\n",
    "                    pred_r, pred_t = refiner(new_points, emb, idx)\n",
    "                    dis, new_points, new_target = criterion_refine(pred_r, pred_t, new_target, model_points, idx, new_points)\n",
    "                    dis.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished.\")\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            # Testing phase\n",
    "            avg_dis = test_phase(arg, estimator, refiner, test_loader, criterion, criterion_refine)\n",
    "            if avg_dis < best_test:\n",
    "                best_test = avg_dis\n",
    "                save_path = f\"{arg.outf}/best_model_{start_time_str}_avgdis_{avg_dis:.6f}.pth\"\n",
    "                torch.save(estimator.state_dict(), save_path)\n",
    "                print(f\"New best model saved at {save_path}\")\n",
    "\n",
    "    print(\"Training and testing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57851a",
   "metadata": {},
   "source": [
    "## 4. Evaluation Pipeline\n",
    "The `eval.py` script handles evaluation of the DenseFusion framework. Key steps:\n",
    "\n",
    "1. **Load Pretrained Model**: Initialize and load weights for `PoseNet` and `PoseRefineNet`.\n",
    "2. **Dataset Preparation**: Load the YCB test data.\n",
    "3. **Pose Prediction**: Perform inference to predict 6D object poses.\n",
    "4. **Metric Calculation**: Evaluate performance using ADD and ADD-S metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "import numpy.ma as ma\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from datasets.ycb.dataset import PoseDataset\n",
    "from lib.network import PoseNet, PoseRefineNet\n",
    "from lib.transformations import quaternion_matrix, quaternion_from_matrix\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"\n",
    "    Parse command-line arguments for model paths and dataset settings.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset_root', type=str, default='', help='Dataset root directory')\n",
    "    parser.add_argument('--model', type=str, default='', help='Path to the PoseNet model')\n",
    "    parser.add_argument('--refine_model', type=str, default='', help='Path to the PoseRefineNet model')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_bbox(posecnn_rois, idx, img_width, img_length, border_list):\n",
    "    \"\"\"\n",
    "    Compute bounding box for the given ROI (Region of Interest).\n",
    "    \"\"\"\n",
    "    rmin = int(posecnn_rois[idx][3]) + 1\n",
    "    rmax = int(posecnn_rois[idx][5]) - 1\n",
    "    cmin = int(posecnn_rois[idx][2]) + 1\n",
    "    cmax = int(posecnn_rois[idx][4]) - 1\n",
    "\n",
    "    # Adjust bounding box size based on `border_list`\n",
    "    r_b, c_b = rmax - rmin, cmax - cmin\n",
    "    for tt in range(len(border_list)):\n",
    "        if r_b > border_list[tt] and r_b < border_list[tt + 1]:\n",
    "            r_b = border_list[tt + 1]\n",
    "            break\n",
    "    for tt in range(len(border_list)):\n",
    "        if c_b > border_list[tt] and c_b < border_list[tt + 1]:\n",
    "            c_b = border_list[tt + 1]\n",
    "            break\n",
    "\n",
    "    # Center-based adjustment to bounding box\n",
    "    center = [int((rmin + rmax) / 2), int((cmin + cmax) / 2)]\n",
    "    rmin, rmax = center[0] - int(r_b / 2), center[0] + int(r_b / 2)\n",
    "    cmin, cmax = center[1] - int(c_b / 2), center[1] + int(c_b / 2)\n",
    "\n",
    "    # Ensure bounding box stays within image boundaries\n",
    "    rmin, rmax = max(rmin, 0), min(rmax, img_width)\n",
    "    cmin, cmax = max(cmin, 0), min(cmax, img_length)\n",
    "\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def initialize_models(arg, num_points, num_obj):\n",
    "    \"\"\"\n",
    "    Initialize PoseNet and PoseRefineNet models with pre-trained weights.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    estimator = PoseNet(num_points=num_points, num_obj=num_obj).to(device)\n",
    "    estimator.load_state_dict(torch.load(arg.model, map_location=device))\n",
    "    estimator.eval()\n",
    "\n",
    "    refiner = PoseRefineNet(num_points=num_points, num_obj=num_obj).to(device)\n",
    "    refiner.load_state_dict(torch.load(arg.refine_model, map_location=device))\n",
    "    refiner.eval()\n",
    "\n",
    "    return estimator, refiner\n",
    "\n",
    "def prepare_data(img, depth, label, posecnn_rois, idx, itemid, xmap, ymap, cam_params, num_points, border_list):\n",
    "    \"\"\"\n",
    "    Prepare point cloud and masked RGB image for pose estimation.\n",
    "    \"\"\"\n",
    "    # Camera intrinsics\n",
    "    cam_cx, cam_cy, cam_fx, cam_fy, cam_scale = cam_params['cx'], cam_params['cy'], cam_params['fx'], cam_params['fy'], cam_params['scale']\n",
    "\n",
    "    # Get bounding box coordinates\n",
    "    rmin, rmax, cmin, cmax = get_bbox(posecnn_rois, idx, 480, 640, border_list)\n",
    "\n",
    "    # Mask processing\n",
    "    mask_depth = ma.getmaskarray(ma.masked_not_equal(depth, 0))\n",
    "    mask_label = ma.getmaskarray(ma.masked_equal(label, itemid))\n",
    "    mask = mask_label * mask_depth\n",
    "\n",
    "    # Select valid points\n",
    "    choose = mask[rmin:rmax, cmin:cmax].flatten().nonzero()[0]\n",
    "    if len(choose) > num_points:\n",
    "        np.random.shuffle(choose)\n",
    "        choose = choose[:num_points]\n",
    "    else:\n",
    "        choose = np.pad(choose, (0, num_points - len(choose)), 'wrap')\n",
    "\n",
    "    # Compute 3D points from depth and camera intrinsics\n",
    "    depth_masked = depth[rmin:rmax, cmin:cmax].flatten()[choose][:, None].astype(np.float32)\n",
    "    xmap_masked = xmap[rmin:rmax, cmin:cmax].flatten()[choose][:, None].astype(np.float32)\n",
    "    ymap_masked = ymap[rmin:rmax, cmin:cmax].flatten()[choose][:, None].astype(np.float32)\n",
    "\n",
    "    # make from 2d points to 3d points\n",
    "    pt2 = depth_masked / cam_scale\n",
    "    pt0 = (ymap_masked - cam_cx) * pt2 / cam_fx\n",
    "    pt1 = (xmap_masked - cam_cy) * pt2 / cam_fy\n",
    "    cloud = np.concatenate((pt0, pt1, pt2), axis=1)\n",
    "\n",
    "    # Process RGB image for the ROI\n",
    "    img_masked = np.array(img)[:, :, :3]\n",
    "    img_masked = np.transpose(img_masked, (2, 0, 1))[:, rmin:rmax, cmin:cmax]\n",
    "\n",
    "    # Convert to tensors\n",
    "    cloud = torch.tensor(cloud, dtype=torch.float32).unsqueeze(0).to(cam_params['device'])\n",
    "    choose = torch.tensor(choose, dtype=torch.int64).unsqueeze(0).to(cam_params['device'])\n",
    "    img_masked = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(\n",
    "        torch.tensor(img_masked, dtype=torch.float32).unsqueeze(0)\n",
    "    ).to(cam_params['device'])\n",
    "\n",
    "    return cloud, choose, img_masked\n",
    "\n",
    "def refine_pose(refiner, cloud, emb, index, pred_r, pred_t, num_points, iterations=2):\n",
    "    \"\"\"\n",
    "    Refine pose estimation iteratively using PoseRefineNet.\n",
    "    \"\"\"\n",
    "    device = cloud.device\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Convert quaternion to rotation matrix\n",
    "        base = quaternion_matrix(pred_r.cpu().numpy())[:3, :3] # Return homogeneous rotation matrix from quaternion. & 3x3:rotation만\n",
    "        base = torch.tensor(base, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Transform point cloud using the predicted pose\n",
    "        cloud_transformed = torch.bmm(cloud - pred_t.unsqueeze(1), base.unsqueeze(0))\n",
    "\n",
    "        # Refine pose predictions\n",
    "        pred_r, pred_t = refiner(cloud_transformed, emb, index)\n",
    "        pred_r = pred_r / torch.norm(pred_r, dim=2, keepdim=True)  # Normalize quaternion\n",
    "\n",
    "    return pred_r, pred_t\n",
    "\n",
    "def _load_classes(class_file_path, dataset_root):\n",
    "    \"\"\"\n",
    "    Load object model points from .xyz files.\n",
    "    \"\"\"\n",
    "    cld = {}\n",
    "    with class_file_path.open() as class_file:\n",
    "        for class_id, class_name in enumerate(class_file, start=1):\n",
    "            points_path = Path(dataset_root) / f\"models/{class_name.strip()}/points.xyz\"\n",
    "            points = np.loadtxt(points_path)\n",
    "            cld[class_id] = points\n",
    "    return cld\n",
    "\n",
    "def calculate_add(pred_r, pred_t, model_points, target_points):\n",
    "    \"\"\"\n",
    "    Calculate Average Distance of Model Points (ADD) metric.\n",
    "    \"\"\"\n",
    "    pred_points = np.dot(model_points, pred_r.T) + pred_t\n",
    "    return np.mean(np.linalg.norm(pred_points - target_points, axis=1))\n",
    "\n",
    "def calculate_add_s(pred_r, pred_t, model_points, target_points):\n",
    "    \"\"\"\n",
    "    Calculate ADD-S (symmetric) metric.\n",
    "    \"\"\"\n",
    "    # 예측된 포인트 생성\n",
    "    pred_points = np.dot(model_points, pred_r.T) + pred_t\n",
    "\n",
    "    # PyTorch를 활용한 ADD-S 계산\n",
    "    pred_points = torch.from_numpy(pred_points).to(device)\n",
    "    target_points = torch.from_numpy(target_points).to(device)\n",
    "    distances = torch.cdist(pred_points.unsqueeze(0), target_points.unsqueeze(0), p=2)\n",
    "    min_distances = distances.min(dim=2)[0]\n",
    "    return min_distances.mean().item()\n",
    "\n",
    "def main():\n",
    "    arg = parse_arguments()\n",
    "\n",
    "    # Camera parameters and constants\n",
    "    cam_params = {\n",
    "        'cx': 312.9869,\n",
    "        'cy': 241.3109,\n",
    "        'fx': 1066.778,\n",
    "        'fy': 1067.487,\n",
    "        'scale': 10000.0,\n",
    "        'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    }\n",
    "    num_obj, num_points, iterations = 21, 1000, 2\n",
    "    border_list = [-1, 40, 80, 120, 160, 200, 240, 280, 320, 360, 400, 440, 480, 520, 560, 600, 640, 680]\n",
    "\n",
    "    # Initialize models\n",
    "    estimator, refiner = initialize_models(arg, num_points, num_obj)\n",
    "\n",
    "    # Iterate over test samples\n",
    "    for now in range(2949):\n",
    "        try:\n",
    "            # Load data\n",
    "            img = Image.open(f'{arg.dataset_root}/{now:06d}-color.png')\n",
    "            depth = np.array(Image.open(f'{arg.dataset_root}/{now:06d}-depth.png'))\n",
    "            posecnn_meta = scio.loadmat(f'YCB_Video_toolbox/results_PoseCNN_RSS2018/{now:06d}.mat')\n",
    "            posecnn_rois = np.array(posecnn_meta['rois'])\n",
    "            label = np.array(posecnn_meta['labels'])\n",
    "            lst = posecnn_rois[:, 1:2].flatten()  # List of object IDs\n",
    "\n",
    "            # Store results for each object\n",
    "            my_result_wo_refine = []\n",
    "            my_result = []\n",
    "            add_scores, add_s_scores = [], []\n",
    "\n",
    "            for idx, itemid in enumerate(lst): # 물체마다\n",
    "            \n",
    "                # model point load \n",
    "                data_list_path = Path(f\"datasets/ycb/dataset_config/test_data_list.txt\")\n",
    "                list = data_list_path.read_text().splitlines()\n",
    "                meta_path = f'{arg.dataset_root} / {list[itemid]}-meta.mat'\n",
    "                meta = scio.loadmat(meta_path)\n",
    "                target_r = meta['poses'][:, :, idx][:, :3]\n",
    "                target_t = meta['poses'][:, :, idx][:, 3:4].flatten()\n",
    "                target_points = np.dot(model_points, target_r.T) + target_t\n",
    "                \n",
    "                model_path = Path(arg.dataset_root) / f\"models/{itemid:02d}/points.xyz\"\n",
    "                if not model_path.exists():\n",
    "                  print(f\"Model point file not found for object {itemid}. Skipping...\")\n",
    "                  break\n",
    "                model_points = np.loadtxt(model_path)  \n",
    "                      \n",
    "                cloud, choose, img_masked = prepare_data(\n",
    "                    img, depth, label, posecnn_rois, idx, itemid,\n",
    "                    np.array([[j for i in range(640)] for j in range(480)]),\n",
    "                    np.array([[i for i in range(640)] for j in range(480)]),\n",
    "                    cam_params, num_points, border_list\n",
    "                )\n",
    "\n",
    "                # Initial pose estimation\n",
    "                pred_r, pred_t, pred_c, emb = estimator(\n",
    "                    img_masked, cloud, choose, torch.tensor([itemid - 1], dtype=torch.int64).to(cam_params['device'])\n",
    "                )\n",
    "                pred_r = pred_r / torch.norm(pred_r, dim=2, keepdim=True)\n",
    "\n",
    "                # Save pose without refinement\n",
    "                result_wo_refine = torch.cat((pred_r[0], pred_t[0].unsqueeze(0)), dim=1).cpu().numpy().flatten()\n",
    "                my_result_wo_refine.append(result_wo_refine.tolist())\n",
    "\n",
    "                # Refinement\n",
    "                pred_r, pred_t = refine_pose(\n",
    "                    refiner, cloud, emb, torch.tensor([itemid - 1], dtype=torch.int64).to(cam_params['device']), pred_r, pred_t, num_points, iterations\n",
    "                )\n",
    "                result_refine = torch.cat((pred_r[0], pred_t[0].unsqueeze(0)), dim=1).cpu().numpy().flatten()\n",
    "                my_result.append(result_refine.tolist())\n",
    "                \n",
    "                pred_r_matrix = quaternion_matrix(pred_r.cpu().numpy())[0, :3, :3]\n",
    "                pred_t_matrix = pred_t.cpu().numpy()\n",
    "\n",
    "                add_scores.append(calculate_add(pred_r_matrix, pred_t_matrix, model_points, target_points))\n",
    "                add_s_scores.append(calculate_add_s(pred_r_matrix, pred_t_matrix, model_points, target_points))\n",
    "\n",
    "            # Save results\n",
    "            scio.savemat(f'experiments/results/{now:06d}_wo_refine.mat', {'poses': my_result_wo_refine})\n",
    "            scio.savemat(f'experiments/results/{now:06d}_refine.mat', {'poses': my_result})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {now}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
