{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseFusion\n",
    "\n",
    "Code from https://github.com/j96w/DenseFusion\n",
    "\n",
    "Train 코드를 살펴본 후, Dataset, Model, Loss 순서로 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./experiments/scripts/train_ycb.sh\n",
    "\"\"\"\n",
    "experiments 폴더에는 dataset마다 train과 evaluation을 위한 shell script가 담겨있다. \n",
    "아래는 ycb video dataset을 사용해 train하는 shell script 예시이다.\n",
    "\"\"\"\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "set -x\n",
    "set -e\n",
    "\n",
    "export PYTHONUNBUFFERED=\"True\"\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "# dataset의 종류와 경로를 지정하여 train.py를 실행\n",
    "python3 ./tools/train.py --dataset ycb\\ \n",
    "  --dataset_root ./datasets/ycb/YCB_Video_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./tools/train.py\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# DenseFusion 6D Object Pose Estimation by Iterative Dense Fusion\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Chen\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import _init_paths\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from datasets.ycb.dataset import PoseDataset as PoseDataset_ycb\n",
    "from datasets.linemod.dataset import PoseDataset as PoseDataset_linemod\n",
    "from lib.network import PoseNet, PoseRefineNet\n",
    "from lib.loss import Loss\n",
    "from lib.loss_refiner import Loss_refine\n",
    "from lib.utils import setup_logger\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default = 'ycb', help='ycb or linemod')\n",
    "parser.add_argument('--dataset_root', type=str, default = '', help='dataset root dir (''YCB_Video_Dataset'' or ''Linemod_preprocessed'')')\n",
    "parser.add_argument('--batch_size', type=int, default = 8, help='batch size')\n",
    "parser.add_argument('--workers', type=int, default = 10, help='number of data loading workers')\n",
    "parser.add_argument('--lr', default=0.0001, help='learning rate')\n",
    "parser.add_argument('--lr_rate', default=0.3, help='learning rate decay rate')\n",
    "parser.add_argument('--w', default=0.015, help='learning rate') # confidence regularization을 위해 loss에서 사용하는 balancing hyperparameter\n",
    "parser.add_argument('--w_rate', default=0.3, help='learning rate decay rate')\n",
    "parser.add_argument('--decay_margin', default=0.016, help='margin to decay lr & w')\n",
    "parser.add_argument('--refine_margin', default=0.013, help='margin to start the training of iterative refinement') # ADD가 1.3cm보다 작아지면 refinement model로 학습\n",
    "parser.add_argument('--noise_trans', default=0.03, help='range of the random noise of translation added to the training data') # 최대 3cm의 random translation\n",
    "parser.add_argument('--iteration', type=int, default = 2, help='number of refinement iterations')\n",
    "parser.add_argument('--nepoch', type=int, default=500, help='max number of epochs to train')\n",
    "parser.add_argument('--resume_posenet', type=str, default = '',  help='resume PoseNet model')\n",
    "parser.add_argument('--resume_refinenet', type=str, default = '',  help='resume PoseRefineNet model')\n",
    "parser.add_argument('--start_epoch', type=int, default = 1, help='which epoch to start')\n",
    "opt = parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Random seed 고정\n",
    "    opt.manualSeed = random.randint(1, 10000)\n",
    "    random.seed(opt.manualSeed)\n",
    "    torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "    # 각 dataset(ycb/linemod)에 맞게 설정\n",
    "    if opt.dataset == 'ycb':\n",
    "        opt.num_objects = 21 #number of object classes in the dataset\n",
    "        opt.num_points = 1000 #number of points on the input pointcloud\n",
    "        opt.outf = 'trained_models/ycb' #folder to save trained models\n",
    "        opt.log_dir = 'experiments/logs/ycb' #folder to save logs\n",
    "        opt.repeat_epoch = 1 #number of repeat times for one epoch training\n",
    "    elif opt.dataset == 'linemod':\n",
    "        opt.num_objects = 13\n",
    "        opt.num_points = 500\n",
    "        opt.outf = 'trained_models/linemod'\n",
    "        opt.log_dir = 'experiments/logs/linemod'\n",
    "        opt.repeat_epoch = 20\n",
    "    else:\n",
    "        print('Unknown dataset')\n",
    "        return\n",
    "\n",
    "    # DenseFusion 모델 정의(Refinement 없는 모델 : estimator, Refinement 있는 모델 : refiner)\n",
    "    estimator = PoseNet(num_points = opt.num_points, num_obj = opt.num_objects)\n",
    "    estimator.cuda()\n",
    "    refiner = PoseRefineNet(num_points = opt.num_points, num_obj = opt.num_objects)\n",
    "    refiner.cuda()\n",
    "\n",
    "    \"\"\" 학습에 3가지 경우가 존재\n",
    "    1. 처음부터 학습\n",
    "    2. 저장해둔 estimator에서 다시 시작\n",
    "    3. 저장해둔 refiner에서 다시 시작\n",
    "    # 기존에 학습하던 모델의 학습을 재개하는 경우, 모델 파라미터 load\n",
    "    \"\"\"\n",
    "    if opt.resume_posenet != '': # 2\n",
    "        estimator.load_state_dict(torch.load('{0}/{1}'.format(opt.outf, opt.resume_posenet)))\n",
    "\n",
    "    if opt.resume_refinenet != '': # 3\n",
    "        refiner.load_state_dict(torch.load('{0}/{1}'.format(opt.outf, opt.resume_refinenet)))\n",
    "        opt.refine_start = True\n",
    "        opt.decay_start = True\n",
    "        opt.lr *= opt.lr_rate\n",
    "        opt.w *= opt.w_rate\n",
    "        opt.batch_size = int(opt.batch_size / opt.iteration) \n",
    "        optimizer = optim.Adam(refiner.parameters(), lr=opt.lr)  \n",
    "    else: # 1, 2\n",
    "        opt.refine_start = False\n",
    "        opt.decay_start = False\n",
    "        optimizer = optim.Adam(estimator.parameters(), lr=opt.lr)\n",
    "\n",
    "    # dataset, dataloader 정의\n",
    "    # Train\n",
    "    if opt.dataset == 'ycb':\n",
    "        dataset = PoseDataset_ycb('train', opt.num_points, True, opt.dataset_root, opt.noise_trans, opt.refine_start)\n",
    "    elif opt.dataset == 'linemod':\n",
    "        dataset = PoseDataset_linemod('train', opt.num_points, True, opt.dataset_root, opt.noise_trans, opt.refine_start)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=opt.workers)\n",
    "    # Validation\n",
    "    if opt.dataset == 'ycb':\n",
    "        test_dataset = PoseDataset_ycb('test', opt.num_points, False, opt.dataset_root, 0.0, opt.refine_start)\n",
    "    elif opt.dataset == 'linemod':\n",
    "        test_dataset = PoseDataset_linemod('test', opt.num_points, False, opt.dataset_root, 0.0, opt.refine_start)\n",
    "    testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=opt.workers)\n",
    "    \n",
    "    opt.sym_list = dataset.get_sym_list() # 대칭적인 물체의 index를 반환\n",
    "    opt.num_points_mesh = dataset.get_num_points_mesh() # Loss에 사용할 point_mesh 개수를 반환(refine x -> num_pt_mesh_small, refine o -> num_pt_mesh_large)\n",
    "\n",
    "    print('>>>>>>>>----------Dataset loaded!---------<<<<<<<<\\nlength of the training set: {0}\\nlength of the testing set: {1}\\nnumber of sample points on mesh: {2}\\nsymmetry object list: {3}'.format(len(dataset), len(test_dataset), opt.num_points_mesh, opt.sym_list))\n",
    "\n",
    "    # Loss 정의\n",
    "    criterion = Loss(opt.num_points_mesh, opt.sym_list)\n",
    "    criterion_refine = Loss_refine(opt.num_points_mesh, opt.sym_list)\n",
    "\n",
    "    best_test = np.Inf\n",
    "\n",
    "    # 처음부터 학습한다면 기존 log 파일들을 삭제\n",
    "    if opt.start_epoch == 1:\n",
    "        for log in os.listdir(opt.log_dir):\n",
    "            os.remove(os.path.join(opt.log_dir, log))\n",
    "\n",
    "    # 시작 시간\n",
    "    st_time = time.time()\n",
    "\n",
    "    for epoch in range(opt.start_epoch, opt.nepoch):\n",
    "        # Train\n",
    "        logger = setup_logger('epoch%d' % epoch, os.path.join(opt.log_dir, 'epoch_%d_log.txt' % epoch))\n",
    "        logger.info('Train time {0}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)) + ', ' + 'Training started'))\n",
    "        train_count = 0\n",
    "        train_dis_avg = 0.0 # ADD\n",
    "        if opt.refine_start: # 3\n",
    "            estimator.eval() # estimator 평가모드\n",
    "            refiner.train() # refiner 학습모드\n",
    "        else: # 1, 2\n",
    "            estimator.train() # estimator 학습모드\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for rep in range(opt.repeat_epoch):\n",
    "            for i, data in enumerate(dataloader, 0):\n",
    "                points, choose, img, target, model_points, idx = data\n",
    "                points, choose, img, target, model_points, idx = Variable(points).cuda(), \\\n",
    "                                                                 Variable(choose).cuda(), \\\n",
    "                                                                 Variable(img).cuda(), \\\n",
    "                                                                 Variable(target).cuda(), \\\n",
    "                                                                 Variable(model_points).cuda(), \\\n",
    "                                                                 Variable(idx).cuda()\n",
    "                ##### estimator input #####\n",
    "                # img : image crop\n",
    "                # points : masked point cloud\n",
    "                # choose : point cloud의 각 point가 crop한 이미지의 어느 위치에 해당하는지를 나타내는 index(crop image를 1D로 flatten했을 때를 기준으로)\n",
    "                # idx : 예측하려는 객체의 class index\n",
    "                pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx) # estimator forward pass -> 각 point에서의 quaternion, translation, confidence와 color embedding 반환\n",
    "                \n",
    "                ##### estimator loss input #####\n",
    "                # pred_r : rotation prediction per pixel\n",
    "                # pred_t : translation prediction per pixel\n",
    "                # pred_c : confidence prediction per pixel\n",
    "                # target : target pose로 변환한 3D model_points\n",
    "                # model_points : 변환하기 전 3D model_points\n",
    "                loss, dis, new_points, new_target = criterion(pred_r, pred_t, pred_c, target, model_points, idx, points, opt.w, opt.refine_start)\n",
    "                \n",
    "                if opt.refine_start: # 3\n",
    "                    for ite in range(0, opt.iteration): # refinement 반복\n",
    "                        pred_r, pred_t = refiner(new_points, emb, idx) # refiner forward pass\n",
    "                        dis, new_points, new_target = criterion_refine(pred_r, pred_t, new_target, model_points, idx, new_points)\n",
    "                        dis.backward() # refiner backward pass\n",
    "                else: # 1, 2\n",
    "                    loss.backward() # estimator backward pass\n",
    "\n",
    "                train_dis_avg += dis.item()\n",
    "                train_count += 1\n",
    "\n",
    "                if train_count % opt.batch_size == 0:\n",
    "                    logger.info('Train time {0} Epoch {1} Batch {2} Frame {3} Avg_dis:{4}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)), epoch, int(train_count / opt.batch_size), train_count, train_dis_avg / opt.batch_size))\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    train_dis_avg = 0\n",
    "\n",
    "                if train_count != 0 and train_count % 1000 == 0:\n",
    "                    if opt.refine_start:\n",
    "                        torch.save(refiner.state_dict(), '{0}/pose_refine_model_current.pth'.format(opt.outf))\n",
    "                    else:\n",
    "                        torch.save(estimator.state_dict(), '{0}/pose_model_current.pth'.format(opt.outf))\n",
    "\n",
    "        print('>>>>>>>>----------epoch {0} train finish---------<<<<<<<<'.format(epoch))\n",
    "\n",
    "        # Validation\n",
    "        logger = setup_logger('epoch%d_test' % epoch, os.path.join(opt.log_dir, 'epoch_%d_test_log.txt' % epoch))\n",
    "        logger.info('Test time {0}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)) + ', ' + 'Testing started'))\n",
    "        test_dis = 0.0\n",
    "        test_count = 0\n",
    "        estimator.eval() # estimator 평가모드\n",
    "        refiner.eval() # refiner 평가모드\n",
    "\n",
    "        for j, data in enumerate(testdataloader, 0):\n",
    "            points, choose, img, target, model_points, idx = data\n",
    "            points, choose, img, target, model_points, idx = Variable(points).cuda(), \\\n",
    "                                                             Variable(choose).cuda(), \\\n",
    "                                                             Variable(img).cuda(), \\\n",
    "                                                             Variable(target).cuda(), \\\n",
    "                                                             Variable(model_points).cuda(), \\\n",
    "                                                             Variable(idx).cuda()\n",
    "            pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx) \n",
    "            _, dis, new_points, new_target = criterion(pred_r, pred_t, pred_c, target, model_points, idx, points, opt.w, opt.refine_start)\n",
    "\n",
    "            if opt.refine_start: # 3\n",
    "                for ite in range(0, opt.iteration):\n",
    "                    pred_r, pred_t = refiner(new_points, emb, idx)\n",
    "                    dis, new_points, new_target = criterion_refine(pred_r, pred_t, new_target, model_points, idx, new_points)\n",
    "\n",
    "            test_dis += dis.item()\n",
    "            logger.info('Test time {0} Test Frame No.{1} dis:{2}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)), test_count, dis))\n",
    "\n",
    "            test_count += 1\n",
    "\n",
    "        test_dis = test_dis / test_count\n",
    "        logger.info('Test time {0} Epoch {1} TEST FINISH Avg dis: {2}'.format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - st_time)), epoch, test_dis))\n",
    "        if test_dis <= best_test:\n",
    "            best_test = test_dis\n",
    "            if opt.refine_start: # 3\n",
    "                torch.save(refiner.state_dict(), '{0}/pose_refine_model_{1}_{2}.pth'.format(opt.outf, epoch, test_dis))\n",
    "            else: # 1, 2\n",
    "                torch.save(estimator.state_dict(), '{0}/pose_model_{1}_{2}.pth'.format(opt.outf, epoch, test_dis))\n",
    "            print(epoch, '>>>>>>>>----------BEST TEST MODEL SAVED---------<<<<<<<<')\n",
    "\n",
    "        # decay margin에 도달하면 learning rate 감소(scheduler 역할)\n",
    "        if best_test < opt.decay_margin and not opt.decay_start:\n",
    "            opt.decay_start = True\n",
    "            opt.lr *= opt.lr_rate\n",
    "            opt.w *= opt.w_rate\n",
    "            optimizer = optim.Adam(estimator.parameters(), lr=opt.lr)\n",
    "\n",
    "        # refine margin에 도달하면 refiner로 학습 모델 전환\n",
    "        if best_test < opt.refine_margin and not opt.refine_start:\n",
    "            opt.refine_start = True\n",
    "            opt.batch_size = int(opt.batch_size / opt.iteration)\n",
    "            optimizer = optim.Adam(refiner.parameters(), lr=opt.lr)\n",
    "\n",
    "            # refiner에서 dataset, dataloader 새로 정의(3D model point 개수가 달라짐)\n",
    "            # Train\n",
    "            if opt.dataset == 'ycb':\n",
    "                dataset = PoseDataset_ycb('train', opt.num_points, True, opt.dataset_root, opt.noise_trans, opt.refine_start)\n",
    "            elif opt.dataset == 'linemod':\n",
    "                dataset = PoseDataset_linemod('train', opt.num_points, True, opt.dataset_root, opt.noise_trans, opt.refine_start)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=opt.workers)\n",
    "            # Validation\n",
    "            if opt.dataset == 'ycb':\n",
    "                test_dataset = PoseDataset_ycb('test', opt.num_points, False, opt.dataset_root, 0.0, opt.refine_start)\n",
    "            elif opt.dataset == 'linemod':\n",
    "                test_dataset = PoseDataset_linemod('test', opt.num_points, False, opt.dataset_root, 0.0, opt.refine_start)\n",
    "            testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=opt.workers)\n",
    "            \n",
    "            opt.sym_list = dataset.get_sym_list()\n",
    "            opt.num_points_mesh = dataset.get_num_points_mesh()\n",
    "\n",
    "            print('>>>>>>>>----------Dataset loaded!---------<<<<<<<<\\nlength of the training set: {0}\\nlength of the testing set: {1}\\nnumber of sample points on mesh: {2}\\nsymmetry object list: {3}'.format(len(dataset), len(test_dataset), opt.num_points_mesh, opt.sym_list))\n",
    "\n",
    "            criterion = Loss(opt.num_points_mesh, opt.sym_list)\n",
    "            criterion_refine = Loss_refine(opt.num_points_mesh, opt.sym_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./datasets/ycb/dataset.py\n",
    "\"\"\"\n",
    "학습과 평가에 사용할 dataset class를 정의한다.\n",
    "아래는 ycb video dataset의 예시이다.\n",
    "\"\"\"\n",
    "\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "from lib.transformations import quaternion_from_euler, euler_matrix, random_quaternion, quaternion_matrix\n",
    "import numpy.ma as ma\n",
    "import copy\n",
    "import scipy.misc\n",
    "import scipy.io as scio\n",
    "\n",
    "\n",
    "class PoseDataset(data.Dataset):\n",
    "    def __init__(self, mode, num_pt, add_noise, root, noise_trans, refine):\n",
    "        if mode == 'train':\n",
    "            self.path = 'datasets/ycb/dataset_config/train_data_list.txt' # ex) data/0000/000000 or data_syn/000000\n",
    "        elif mode == 'test':\n",
    "            self.path = 'datasets/ycb/dataset_config/test_data_list.txt'\n",
    "        self.num_pt = num_pt # 1000\n",
    "        self.root = root # ./datasets/ycb/YCB_Video_Dataset\n",
    "        self.add_noise = add_noise # True\n",
    "        self.noise_trans = noise_trans # 0.03(=3cm)\n",
    "\n",
    "        # Data list를 만드는 과정\n",
    "        self.list = []\n",
    "        self.real = [] \n",
    "        self.syn = [] \n",
    "        input_file = open(self.path)\n",
    "        while 1:\n",
    "            input_line = input_file.readline()\n",
    "            if not input_line:\n",
    "                break\n",
    "            if input_line[-1:] == '\\n': # \\n 제거\n",
    "                input_line = input_line[:-1]\n",
    "\n",
    "            # 실제 or 합성 데이터\n",
    "            if input_line[:5] == 'data/':\n",
    "                self.real.append(input_line)\n",
    "            else: \n",
    "                self.syn.append(input_line)\n",
    "            # 전체 데이터\n",
    "            self.list.append(input_line)\n",
    "        input_file.close()\n",
    "\n",
    "        self.length = len(self.list)\n",
    "        self.len_real = len(self.real)\n",
    "        self.len_syn = len(self.syn)\n",
    "\n",
    "        class_file = open('datasets/ycb/dataset_config/classes.txt') # ex) 002_master_chef_can\n",
    "        class_id = 1\n",
    "        self.cld = {}\n",
    "\n",
    "        # 클래스 별로 3D model points를 읽고 dict( ex) 1 : [[x1, y1, z1], [x2, y2, z2] ...] )로 저장\n",
    "        while 1:\n",
    "            class_input = class_file.readline()\n",
    "            if not class_input:\n",
    "                break\n",
    "\n",
    "            input_file = open('{0}/models/{1}/points.xyz'.format(self.root, class_input[:-1]))\n",
    "            self.cld[class_id] = []\n",
    "            while 1:\n",
    "                input_line = input_file.readline()\n",
    "                if not input_line:\n",
    "                    break\n",
    "                input_line = input_line[:-1].split(' ')\n",
    "                self.cld[class_id].append([float(input_line[0]), float(input_line[1]), float(input_line[2])])\n",
    "            self.cld[class_id] = np.array(self.cld[class_id])\n",
    "            input_file.close()\n",
    "            \n",
    "            class_id += 1\n",
    "\n",
    "        self.cam_cx_1 = 312.9869\n",
    "        self.cam_cy_1 = 241.3109\n",
    "        self.cam_fx_1 = 1066.778\n",
    "        self.cam_fy_1 = 1067.487\n",
    "\n",
    "        self.cam_cx_2 = 323.7872\n",
    "        self.cam_cy_2 = 279.6921\n",
    "        self.cam_fx_2 = 1077.836\n",
    "        self.cam_fy_2 = 1078.189\n",
    "\n",
    "        self.xmap = np.array([[j for i in range(640)] for j in range(480)])\n",
    "        self.ymap = np.array([[i for i in range(640)] for j in range(480)])\n",
    "        \n",
    "        self.trancolor = transforms.ColorJitter(0.2, 0.2, 0.2, 0.05) # brightness[0.8, 1.2], contrast[0.8, 1.2], saturation[0.8, 1.2], hue[-0.05, 0.05]\n",
    "        self.noise_img_loc = 0.0\n",
    "        self.noise_img_scale = 7.0\n",
    "        self.minimum_num_pt = 50\n",
    "        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.symmetry_obj_idx = [12, 15, 18, 19, 20]\n",
    "        self.num_pt_mesh_small = 500 \n",
    "        self.num_pt_mesh_large = 2600\n",
    "        self.refine = refine\n",
    "        self.front_num = 2\n",
    "\n",
    "        print(len(self.list))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 현재 index에 해당하는 color, depth, label 및 meta data load\n",
    "        img = Image.open('{0}/{1}-color.png'.format(self.root, self.list[index]))\n",
    "        depth = np.array(Image.open('{0}/{1}-depth.png'.format(self.root, self.list[index])))\n",
    "        label = np.array(Image.open('{0}/{1}-label.png'.format(self.root, self.list[index])))\n",
    "        meta = scio.loadmat('{0}/{1}-meta.mat'.format(self.root, self.list[index]))\n",
    "\n",
    "        # 각 이미지에 맞는 camera parameter 지정\n",
    "        if self.list[index][:8] != 'data_syn' and int(self.list[index][5:9]) >= 60: # 실제 데이터 중 60번 이상은 cam2\n",
    "            cam_cx = self.cam_cx_2\n",
    "            cam_cy = self.cam_cy_2\n",
    "            cam_fx = self.cam_fx_2\n",
    "            cam_fy = self.cam_fy_2\n",
    "        else:\n",
    "            cam_cx = self.cam_cx_1\n",
    "            cam_cy = self.cam_cy_1\n",
    "            cam_fx = self.cam_fx_1\n",
    "            cam_fy = self.cam_fy_1\n",
    "\n",
    "        # Preprocessing\n",
    "\n",
    "        \"\"\" Data Augmentation\n",
    "        add_noise가 True인 경우, 다음 세 가지 증강을 실시\n",
    "        (1) ColorJitter\n",
    "        (2) Random pose translation\n",
    "        (3) 데이터 합성 \n",
    "            (3-1) 배경이 존재하지 않는 합성 데이터 뒤에 랜덤으로 선택한 실제 데이터를 배경으로 추가\n",
    "            (3-2) 데이터 앞에 합성 데이터의 객체를 추가\n",
    "        \"\"\"\n",
    "\n",
    "        # (3-1) mask -> label에서 값이 0인 부분을 True, 나머지를 False로 masking한 array\n",
    "        mask_back = ma.getmaskarray(ma.masked_equal(label, 0))\n",
    "\n",
    "        # (3-2) mask와 뒤쪽에 들어갈 label 준비\n",
    "        add_front = False\n",
    "        if self.add_noise:\n",
    "            for k in range(5): # 최대 5번 시도\n",
    "                seed = random.choice(self.syn) # 랜덤으로 합성 데이터 하나를 선택\n",
    "                front = np.array(self.trancolor(Image.open('{0}/{1}-color.png'.format(self.root, seed)).convert(\"RGB\"))) # (1)\n",
    "                front = np.transpose(front, (2, 0, 1)) # 채널 순서 (H, W, C) -> (C, H, W)\n",
    "                f_label = np.array(Image.open('{0}/{1}-label.png'.format(self.root, seed))) # Label 이미지\n",
    "                front_label = np.unique(f_label).tolist()[1:] # np.unique를 통해 해당 합성 데이터의 label list 저장\n",
    "                if len(front_label) < self.front_num: # 랜덤으로 선택한 합성 데이터의 label 개수가 front_num보다 작을 경우 다시\n",
    "                   continue\n",
    "                front_label = random.sample(front_label, self.front_num) # label 중에서 front_num 개수만큼 랜덤 선택\n",
    "                # 합성 데이터의 front_label에 속하지 않는 배경 부분을 추출하는 과정\n",
    "                for f_i in front_label:\n",
    "                    mk = ma.getmaskarray(ma.masked_not_equal(f_label, f_i)) # Label 이미지에서 f_i에 해당하면 False, 나머지는 True\n",
    "                    # mk들을 곱해 공통 배경 부분만 True로 남겨둠\n",
    "                    if f_i == front_label[0]: \n",
    "                        mask_front = mk\n",
    "                    else:  \n",
    "                        mask_front = mask_front * mk\n",
    "\n",
    "                t_label = label * mask_front # label에 합성 데이터의 배경 array를 곱해 합성 데이터의 객체에 의해 가려진 실제 데이터를 표현\n",
    "                if len(t_label.nonzero()[0]) > 1000: # 기존 데이터가 충분히 포함되어 있으면 채택\n",
    "                    label = t_label\n",
    "                    add_front = True\n",
    "                    break\n",
    "        \n",
    "        # Class indexes\n",
    "        obj = meta['cls_indexes'].flatten().astype(np.int32)\n",
    "\n",
    "        # 특정 객체 영역만 crop하여 모델에 넣기 위한 과정\n",
    "        while 1:\n",
    "            idx = np.random.randint(0, len(obj)) # 랜덤으로 객체 선택\n",
    "            mask_depth = ma.getmaskarray(ma.masked_not_equal(depth, 0)) # Depth 이미지에서 값이 0인 부분을 False, 나머지는 True로 하여 깊이가 유효한 영역 선택\n",
    "            mask_label = ma.getmaskarray(ma.masked_equal(label, obj[idx])) # 객체에 해당하면 True, 나머지는 False\n",
    "            mask = mask_label * mask_depth # 선택한 객체에 해당하는 영역과 유효한 depth 영역을 곱\n",
    "            if len(mask.nonzero()[0]) > self.minimum_num_pt: # 객체 point가 최소 개수를 충족하면 break\n",
    "                break\n",
    "        \n",
    "        # (1)\n",
    "        if self.add_noise:\n",
    "            img = self.trancolor(img)\n",
    "\n",
    "        # mask_label에서 bbox 추출\n",
    "        rmin, rmax, cmin, cmax = get_bbox(mask_label)\n",
    "        img = np.transpose(np.array(img)[:, :, :3], (2, 0, 1))[:, rmin:rmax, cmin:cmax] # Color 이미지에서 RGB 부분 선택 -> (C, H, W)로 변경 -> bbox 영역만 crop\n",
    "\n",
    "\n",
    "        \"\"\" 변수 되짚어보기\n",
    "        mask_back : 전체 이미지에서 배경만 True인 mask \n",
    "        mask_label : 전체 이미지에서 객체 영역만 True인 mask\n",
    "        mask : mask_label에서 depth도 유효한 영역만 True인 mask\n",
    "        \n",
    "        if add_noise is True ...\n",
    "        mask_front : 전체 이미지에서 합성 데이터가 들어갈 영역만 제외하고 전부 True인 mask\n",
    "        \"\"\"\n",
    "\n",
    "        # (3-1)\n",
    "        if self.list[index][:8] == 'data_syn':\n",
    "            seed = random.choice(self.real) # 랜덤으로 실제 데이터 중 하나를 선택\n",
    "            back = np.array(self.trancolor(Image.open('{0}/{1}-color.png'.format(self.root, seed)).convert(\"RGB\")))\n",
    "            back = np.transpose(back, (2, 0, 1))[:, rmin:rmax, cmin:cmax] # 실제 데이터에서 bbox영역만 crop\n",
    "            img_masked = back * mask_back[rmin:rmax, cmin:cmax] + img # 합성 데이터의 bbox부분에서 배경 부분은 실제 데이터의 값으로 채움\n",
    "        else: # 실제 데이터는 그대로\n",
    "            img_masked = img\n",
    "\n",
    "        # (3-2) bbox 영역에 합성 데이터 추가 (합성 데이터에 합성 데이터를 추가하는 경우, 배경이 채워지지 않으므로 실제 데이터로 배경을 채우는 과정을 우선하고 더해줌)\n",
    "        if self.add_noise and add_front:\n",
    "            img_masked = img_masked * mask_front[rmin:rmax, cmin:cmax] + front[:, rmin:rmax, cmin:cmax] * ~(mask_front[rmin:rmax, cmin:cmax])\n",
    "\n",
    "        if self.list[index][:8] == 'data_syn':\n",
    "            img_masked = img_masked + np.random.normal(loc=0.0, scale=7.0, size=img_masked.shape)\n",
    "        \n",
    "        # => 여기까지가 crop한 color image인 img_masked를 뽑는 과정\n",
    "        \n",
    "        # img_masked 저장\n",
    "        # p_img = np.transpose(img_masked, (1, 2, 0))\n",
    "        # scipy.misc.imsave('temp/{0}_input.png'.format(index), p_img)\n",
    "        # scipy.misc.imsave('temp/{0}_label.png'.format(index), mask[rmin:rmax, cmin:cmax].astype(np.int32))\n",
    "\n",
    "        # Target pose의 transformation matrix\n",
    "        target_r = meta['poses'][:, :, idx][:, 0:3] # target rotation\n",
    "        target_t = np.array([meta['poses'][:, :, idx][:, 3:4].flatten()]) # target translation\n",
    "        add_t = np.array([random.uniform(-self.noise_trans, self.noise_trans) for i in range(3)]) # (2)\n",
    "\n",
    "        # num_pt 개수만큼 point를 추출하는 과정\n",
    "        choose = mask[rmin:rmax, cmin:cmax].flatten().nonzero() # bbox 영역에 있는 유효한 객체 point \n",
    "        if len(choose) > self.num_pt: # 객체 point 개수가 num_pt보다 많으면 num_pt만큼 랜덤 선택\n",
    "            c_mask = np.zeros(len(choose), dtype=int)\n",
    "            c_mask[:self.num_pt] = 1\n",
    "            np.random.shuffle(c_mask)\n",
    "            choose = choose[c_mask.nonzero()]\n",
    "        else: # 객체 point 개수가 num_pt 이하면 부족한 개수만큼 padding(중복 추출)\n",
    "            choose = np.pad(choose, (0, self.num_pt - len(choose)), 'wrap')\n",
    "        \n",
    "        # Depth 이미지에서 depth 값을, xmap, ymap에서 location을 추출하여 point cloud 생성 (ex) depth:10, xmap:2, ymap:3 -> Image coordinate의 [2, 3] 위치에서 깊이 10)\n",
    "        depth_masked = depth[rmin:rmax, cmin:cmax].flatten()[choose][:, np.newaxis].astype(np.float32)\n",
    "        xmap_masked = self.xmap[rmin:rmax, cmin:cmax].flatten()[choose][:, np.newaxis].astype(np.float32)\n",
    "        ymap_masked = self.ymap[rmin:rmax, cmin:cmax].flatten()[choose][:, np.newaxis].astype(np.float32)\n",
    "        choose = np.array([choose])\n",
    "\n",
    "        # Image coordinate을 camera coordinate으로 변환\n",
    "        cam_scale = meta['factor_depth'][0][0]\n",
    "        pt2 = depth_masked / cam_scale\n",
    "        pt0 = (ymap_masked - cam_cx) * pt2 / cam_fx\n",
    "        pt1 = (xmap_masked - cam_cy) * pt2 / cam_fy\n",
    "        cloud = np.concatenate((pt0, pt1, pt2), axis=1)\n",
    "        if self.add_noise: \n",
    "            cloud = np.add(cloud, add_t) # (2)\n",
    "\n",
    "        # => 여기까지가 masked point cloud인 cloud를 뽑는 과정\n",
    "\n",
    "        # cloud 저장\n",
    "        # fw = open('temp/{0}_cld.xyz'.format(index), 'w')\n",
    "        # for it in cloud:\n",
    "        #    fw.write('{0} {1} {2}\\n'.format(it[0], it[1], it[2]))\n",
    "        # fw.close()\n",
    "\n",
    "        # 3D model points에서 필요한 개수(num_points_mesh)만 랜덤 선택\n",
    "        dellist = [j for j in range(0, len(self.cld[obj[idx]]))] # 해당 객체의 3D model points 개수로 list 생성\n",
    "        if self.refine:\n",
    "            dellist = random.sample(dellist, len(self.cld[obj[idx]]) - self.num_pt_mesh_large)\n",
    "        else:\n",
    "            dellist = random.sample(dellist, len(self.cld[obj[idx]]) - self.num_pt_mesh_small)\n",
    "        model_points = np.delete(self.cld[obj[idx]], dellist, axis=0) # 필요없는 개수만큼 삭제\n",
    "\n",
    "        # => 여기까지가 3D model point cloud인 model_points를 뽑는 과정\n",
    "\n",
    "        # model_points 저장\n",
    "        # fw = open('temp/{0}_model_points.xyz'.format(index), 'w')\n",
    "        # for it in model_points:\n",
    "        #    fw.write('{0} {1} {2}\\n'.format(it[0], it[1], it[2]))\n",
    "        # fw.close()\n",
    "\n",
    "        # 3D model points에 target rotation, target translation 적용\n",
    "        target = np.dot(model_points, target_r.T)\n",
    "        if self.add_noise:\n",
    "            target = np.add(target, target_t + add_t) # (2)\n",
    "        else:\n",
    "            target = np.add(target, target_t)\n",
    "        \n",
    "        # => 여기까지가 3D model point cloud를 target pose로 변환한 target을 뽑는 과정\n",
    "        \n",
    "        # target 저장\n",
    "        # fw = open('temp/{0}_tar.xyz'.format(index), 'w')\n",
    "        # for it in target:\n",
    "        #    fw.write('{0} {1} {2}\\n'.format(it[0], it[1], it[2]))\n",
    "        # fw.close()\n",
    "        \n",
    "        return torch.from_numpy(cloud.astype(np.float32)), \\\n",
    "               torch.LongTensor(choose.astype(np.int32)), \\\n",
    "               self.norm(torch.from_numpy(img_masked.astype(np.float32))), \\\n",
    "               torch.from_numpy(target.astype(np.float32)), \\\n",
    "               torch.from_numpy(model_points.astype(np.float32)), \\\n",
    "               torch.LongTensor([int(obj[idx]) - 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    # 대칭적인 물체의 index를 반환하는 함수\n",
    "    def get_sym_list(self):\n",
    "        return self.symmetry_obj_idx\n",
    "\n",
    "    # Loss 계산에 사용할 point 개수를 반환하는 함수\n",
    "    def get_num_points_mesh(self):\n",
    "        if self.refine:\n",
    "            return self.num_pt_mesh_large # refine o -> 2600\n",
    "        else:\n",
    "            return self.num_pt_mesh_small # refine x -> 500\n",
    "\n",
    "\n",
    "border_list = [-1, 40, 80, 120, 160, 200, 240, 280, 320, 360, 400, 440, 480, 520, 560, 600, 640, 680]\n",
    "img_width = 480 \n",
    "img_length = 640\n",
    "\n",
    "def get_bbox(label):\n",
    "    rows = np.any(label, axis=1)\n",
    "    cols = np.any(label, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    rmax += 1\n",
    "    cmax += 1\n",
    "    r_b = rmax - rmin\n",
    "    for tt in range(len(border_list)):\n",
    "        if r_b > border_list[tt] and r_b < border_list[tt + 1]:\n",
    "            r_b = border_list[tt + 1]\n",
    "            break\n",
    "    c_b = cmax - cmin\n",
    "    for tt in range(len(border_list)):\n",
    "        if c_b > border_list[tt] and c_b < border_list[tt + 1]:\n",
    "            c_b = border_list[tt + 1]\n",
    "            break\n",
    "    center = [int((rmin + rmax) / 2), int((cmin + cmax) / 2)]\n",
    "    rmin = center[0] - int(r_b / 2)\n",
    "    rmax = center[0] + int(r_b / 2)\n",
    "    cmin = center[1] - int(c_b / 2)\n",
    "    cmax = center[1] + int(c_b / 2)\n",
    "    if rmin < 0:\n",
    "        delt = -rmin\n",
    "        rmin = 0\n",
    "        rmax += delt\n",
    "    if cmin < 0:\n",
    "        delt = -cmin\n",
    "        cmin = 0\n",
    "        cmax += delt\n",
    "    if rmax > img_width:\n",
    "        delt = rmax - img_width\n",
    "        rmax = img_width\n",
    "        rmin -= delt\n",
    "    if cmax > img_length:\n",
    "        delt = cmax - img_length\n",
    "        cmax = img_length\n",
    "        cmin -= delt\n",
    "    return rmin, rmax, cmin, cmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640)\n",
      "[[  0   0   0 ...   0   0   0]\n",
      " [  1   1   1 ...   1   1   1]\n",
      " [  2   2   2 ...   2   2   2]\n",
      " ...\n",
      " [477 477 477 ... 477 477 477]\n",
      " [478 478 478 ... 478 478 478]\n",
      " [479 479 479 ... 479 479 479]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xmap = np.array([[j for i in range(640)] for j in range(480)])\n",
    "print(xmap.shape)\n",
    "print(xmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./lib/network.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch.nn.functional as F\n",
    "from lib.pspnet import PSPNet\n",
    "\n",
    "# Color embedding은 PSPNet을 사용하여 추출\n",
    "\"\"\" PSPNet\n",
    "- Semantic segmentation 모델로 기존 FCN 모델이 비슷하거나 작은 객체들의 class를 잘 구분하지 못하는 한계점을 극복하고자\n",
    "  Pyramid Pooling Module을 통해 global contextual information을 활용\n",
    "\"\"\"\n",
    "# Feature를 추출하는 backbone은 다양한 깊이의 resnet 중에 선택\n",
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "    'resnet101': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),\n",
    "    'resnet152': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152')\n",
    "}\n",
    "\n",
    "# Color embedding을 추출하는 Modified PSPNet\n",
    "class ModifiedResnet(nn.Module):\n",
    "\n",
    "    def __init__(self, usegpu=True):\n",
    "        super(ModifiedResnet, self).__init__()\n",
    "\n",
    "        self.model = psp_models['resnet18'.lower()]()\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# Embedding으로부터 pixel-wise feature를 만들어내는 모듈\n",
    "class PoseNetFeat(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(PoseNetFeat, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "        self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "        self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "        self.conv5 = torch.nn.Conv1d(256, 512, 1)\n",
    "        self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "        self.ap1 = torch.nn.AvgPool1d(num_points)\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        # 1. Pixel-wise feature 생성\n",
    "        x = F.relu(self.conv1(x)) # Geometry embedding 1 (3 -> 64)\n",
    "        emb = F.relu(self.e_conv1(emb)) # Color embedding 1 (32 -> 64)\n",
    "        pointfeat_1 = torch.cat((x, emb), dim=1) # Pixel wise dense fusion 1 (Dim: 128)\n",
    "\n",
    "        x = F.relu(self.conv2(x)) # Geometry embedding 2 (64 -> 128)\n",
    "        emb = F.relu(self.e_conv2(emb)) # Color embedding 2 (64 -> 128)\n",
    "        pointfeat_2 = torch.cat((x, emb), dim=1) # Pixel wise dense fusion 2 (Dim: 256)\n",
    "\n",
    "        # 2. Global feature 생성\n",
    "        x = F.relu(self.conv5(pointfeat_2)) # (256 -> 512)\n",
    "        x = F.relu(self.conv6(x)) # (512 -> 1024)\n",
    "\n",
    "        ap_x = self.ap1(x) # 전체 픽셀에 대하여 average pooling (Dim: 1024)\n",
    "\n",
    "        # 3. 1과 2를 결합해 pixel-wise feature 생성\n",
    "        ap_x = ap_x.view(-1, 1024, 1).repeat(1, 1, self.num_points) # Global feature를 num_points만큼 확장\n",
    "        # 2개의 pixel-wise feature와 global feature 결합 (Dim: 128 + 256 + 1024)\n",
    "        return torch.cat([pointfeat_1, pointfeat_2, ap_x], 1)\n",
    "\n",
    "# Densefusion 모델 -> estimator\n",
    "class PoseNet(nn.Module):\n",
    "    def __init__(self, num_points, num_obj):\n",
    "        # num_points : number of points on the input pointcloud\n",
    "        # num_obj : number of object classes in the dataset\n",
    "\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.num_points = num_points \n",
    "        self.cnn = ModifiedResnet()\n",
    "        self.feat = PoseNetFeat(num_points)\n",
    "        \n",
    "        self.conv1_r = torch.nn.Conv1d(1408, 640, 1)\n",
    "        self.conv1_t = torch.nn.Conv1d(1408, 640, 1)\n",
    "        self.conv1_c = torch.nn.Conv1d(1408, 640, 1)\n",
    "\n",
    "        self.conv2_r = torch.nn.Conv1d(640, 256, 1)\n",
    "        self.conv2_t = torch.nn.Conv1d(640, 256, 1)\n",
    "        self.conv2_c = torch.nn.Conv1d(640, 256, 1)\n",
    "\n",
    "        self.conv3_r = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv3_t = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv3_c = torch.nn.Conv1d(256, 128, 1)\n",
    "\n",
    "        self.conv4_r = torch.nn.Conv1d(128, num_obj*4, 1) #quaternion\n",
    "        self.conv4_t = torch.nn.Conv1d(128, num_obj*3, 1) #translation\n",
    "        self.conv4_c = torch.nn.Conv1d(128, num_obj*1, 1) #confidence\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, img, x, choose, obj):\n",
    "        # 1. Color 이미지에서 PSPNet으로 color Embedding 추출\n",
    "        out_img = self.cnn(img) # crop한 img size와 동일\n",
    "        \n",
    "        # 2. 선택된 point cloud index를 기반으로 color Embedding 추출\n",
    "        bs, di, _, _ = out_img.size() # Embedding 크기: (batch, dim, H, W)\n",
    "\n",
    "        emb = out_img.view(bs, di, -1) # 각 픽셀의 embedding을 1D로 펼침\n",
    "        choose = choose.repeat(1, di, 1) # 각 픽셀의 embedding 전체를 선택하기 위한 확장\n",
    "        emb = torch.gather(emb, 2, choose).contiguous() # point cloud index에 해당하는 color embedding만 추출 -> (batch, dim, num_points)\n",
    "\n",
    "        # 3. Point cloud와 color embedding을 결합하여 feature 생성\n",
    "        x = x.transpose(2, 1).contiguous() # Point cloud를 (batch, 3, num_points)로 변환\n",
    "        ap_x = self.feat(x, emb) # PoseNetFeat에서 feature 생성\n",
    "\n",
    "        # 4. Feature로부터 pose 추정(quaternion, translation, confidence)\n",
    "        # Dim: 1408 -> 640\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))\n",
    "        cx = F.relu(self.conv1_c(ap_x))      \n",
    "        # Dim: 640 -> 256\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "        cx = F.relu(self.conv2_c(cx))\n",
    "        # Dim: 256 -> 128\n",
    "        rx = F.relu(self.conv3_r(rx))\n",
    "        tx = F.relu(self.conv3_t(tx))\n",
    "        cx = F.relu(self.conv3_c(cx))\n",
    "        # Dim: 128 -> (num_obj * n) : prediction per pixel\n",
    "        rx = self.conv4_r(rx).view(bs, self.num_obj, 4, self.num_points) # (batch, num_obj*4, num_points) -> (batch, num_obj, 4, num_points) : 각 클래스와 매칭되는 21개의 채널\n",
    "        tx = self.conv4_t(tx).view(bs, self.num_obj, 3, self.num_points) # (batch, num_obj*3, num_points) -> (batch, num_obj, 3, num_points)\n",
    "        cx = torch.sigmoid(self.conv4_c(cx)).view(bs, self.num_obj, 1, self.num_points) # sigmoid -> (batch, num_obj*1, num_points) -> (batch, num_obj, 1, num_points)\n",
    "        \n",
    "        # 5. Batch 차원을 없애고 21개의 클래스 중 해당 객체의 값만 남김(Dataloader의 batch_size가 항상 1이라 가능)\n",
    "        b = 0\n",
    "        out_rx = torch.index_select(rx[b], 0, obj[b]) # (batch, num_obj, 4, num_points) -> (1, 4, num_points)\n",
    "        out_tx = torch.index_select(tx[b], 0, obj[b])\n",
    "        out_cx = torch.index_select(cx[b], 0, obj[b])\n",
    "        \n",
    "        # 6. output 차원 변경\n",
    "        out_rx = out_rx.contiguous().transpose(2, 1).contiguous() # (1, 4, num_points) -> (1, num_points, 4)\n",
    "        out_tx = out_tx.contiguous().transpose(2, 1).contiguous() # (1, 3, num_points) -> (1, num_points, 3)\n",
    "        out_cx = out_cx.contiguous().transpose(2, 1).contiguous() # (1, 1, num_points) -> (1, num_points, 1)\n",
    "        \n",
    "        return out_rx, out_tx, out_cx, emb.detach()\n",
    " \n",
    "\n",
    "\n",
    "class PoseRefineNetFeat(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(PoseRefineNetFeat, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "        self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "        self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "        self.conv5 = torch.nn.Conv1d(384, 512, 1)\n",
    "        self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "        self.ap1 = torch.nn.AvgPool1d(num_points)\n",
    "        self.num_points = num_points\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        emb = F.relu(self.e_conv1(emb))\n",
    "        pointfeat_1 = torch.cat([x, emb], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        emb = F.relu(self.e_conv2(emb))\n",
    "        pointfeat_2 = torch.cat([x, emb], dim=1)\n",
    "\n",
    "        pointfeat_3 = torch.cat([pointfeat_1, pointfeat_2], dim=1) # 1과 2를 결합 (Dim: 64 + 128)\n",
    "\n",
    "        # Global feature 생성\n",
    "        x = F.relu(self.conv5(pointfeat_3)) # (384 -> 512)\n",
    "        x = F.relu(self.conv6(x)) # (512 -> 1024)\n",
    "\n",
    "        ap_x = self.ap1(x) # 전체 픽셀에 대하여 average pooling (Dim: 1024)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024)\n",
    "        return ap_x # Global feature만 반환\n",
    "\n",
    "# Refinement를 추가한 Densefusion 모델 -> refiner\n",
    "class PoseRefineNet(nn.Module):\n",
    "    def __init__(self, num_points, num_obj):\n",
    "        super(PoseRefineNet, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.feat = PoseRefineNetFeat(num_points)\n",
    "        \n",
    "        self.conv1_r = torch.nn.Linear(1024, 512)\n",
    "        self.conv1_t = torch.nn.Linear(1024, 512)\n",
    "\n",
    "        self.conv2_r = torch.nn.Linear(512, 128)\n",
    "        self.conv2_t = torch.nn.Linear(512, 128)\n",
    "\n",
    "        self.conv3_r = torch.nn.Linear(128, num_obj*4) #quaternion\n",
    "        self.conv3_t = torch.nn.Linear(128, num_obj*3) #translation\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, x, emb, obj): # new point cloud, color embedding, class index\n",
    "        bs = x.size()[0]\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        # Feature로부터 quaternion, translation 추정\n",
    "        # Dim: 1024 -> 512\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))   \n",
    "        # Dim: 512 -> 128\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "        # Dim: 128 -> (num_obj*n)\n",
    "        rx = self.conv3_r(rx).view(bs, self.num_obj, 4)\n",
    "        tx = self.conv3_t(tx).view(bs, self.num_obj, 3)\n",
    "        \n",
    "        # Batch 차원을 없애고 21개의 클래스 중 해당 객체의 값만 남김\n",
    "        b = 0\n",
    "        out_rx = torch.index_select(rx[b], 0, obj[b])\n",
    "        out_tx = torch.index_select(tx[b], 0, obj[b])\n",
    "\n",
    "        return out_rx, out_tx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./lib/loss.py\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from lib.knn.__init__ import KNearestNeighbor\n",
    "\n",
    "\n",
    "def loss_calculation(pred_r, pred_t, pred_c, target, model_points, idx, points, w, refine, num_point_mesh, sym_list):\n",
    "    knn = KNearestNeighbor(1)\n",
    "    bs, num_p, _ = pred_c.size() # (1, num_points, 1)\n",
    "\n",
    "    # quaternion 정규화\n",
    "    pred_r = pred_r / (torch.norm(pred_r, dim=2).view(bs, num_p, 1))\n",
    "    \n",
    "    # 3x3 rotation matrix로 변환 : quaternion(4) -> rotation(3, 3)\n",
    "    base = torch.cat(((1.0 - 2.0*(pred_r[:, :, 2]**2 + pred_r[:, :, 3]**2)).view(bs, num_p, 1),\\\n",
    "                      (2.0*pred_r[:, :, 1]*pred_r[:, :, 2] - 2.0*pred_r[:, :, 0]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (2.0*pred_r[:, :, 0]*pred_r[:, :, 2] + 2.0*pred_r[:, :, 1]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (2.0*pred_r[:, :, 1]*pred_r[:, :, 2] + 2.0*pred_r[:, :, 3]*pred_r[:, :, 0]).view(bs, num_p, 1), \\\n",
    "                      (1.0 - 2.0*(pred_r[:, :, 1]**2 + pred_r[:, :, 3]**2)).view(bs, num_p, 1), \\\n",
    "                      (-2.0*pred_r[:, :, 0]*pred_r[:, :, 1] + 2.0*pred_r[:, :, 2]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (-2.0*pred_r[:, :, 0]*pred_r[:, :, 2] + 2.0*pred_r[:, :, 1]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (2.0*pred_r[:, :, 0]*pred_r[:, :, 1] + 2.0*pred_r[:, :, 2]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (1.0 - 2.0*(pred_r[:, :, 1]**2 + pred_r[:, :, 2]**2)).view(bs, num_p, 1)), dim=2).contiguous().view(bs * num_p, 3, 3)\n",
    "\n",
    "    ori_base = base\n",
    "    base = base.contiguous().transpose(2, 1).contiguous() # 행렬곱을 위한 transpose\n",
    "\n",
    "    # 3D model_points, target을 각각의 num_points에서 예측한 rotation, translation으로 변환하기 위해 확장\n",
    "    model_points = model_points.view(bs, 1, num_point_mesh, 3).repeat(1, num_p, 1, 1).view(bs * num_p, num_point_mesh, 3)\n",
    "    target = target.view(bs, 1, num_point_mesh, 3).repeat(1, num_p, 1, 1).view(bs * num_p, num_point_mesh, 3)\n",
    "    ori_target = target\n",
    "\n",
    "    pred_t = pred_t.contiguous().view(bs * num_p, 1, 3)\n",
    "    ori_t = pred_t\n",
    "    points = points.contiguous().view(bs * num_p, 1, 3)\n",
    "    pred_c = pred_c.contiguous().view(bs * num_p) \n",
    "\n",
    "    # 예측한 rotaion과 translation으로 3D model_points 변환\n",
    "    pred = torch.add(torch.bmm(model_points, base), points + pred_t) # bmm = batch matrix multiplication\n",
    "    \n",
    "    if not refine:\n",
    "        if idx[0].item() in sym_list: # 대칭적인 물체인 경우\n",
    "            target = target[0].transpose(1, 0).contiguous().view(3, -1)\n",
    "            pred = pred.permute(2, 0, 1).contiguous().view(3, -1)\n",
    "            # target에서 가장 가까운 point와 매칭\n",
    "            inds = knn(target.unsqueeze(0), pred.unsqueeze(0))\n",
    "            target = torch.index_select(target, 1, inds.view(-1) - 1)\n",
    "            target = target.view(3, bs * num_p, num_point_mesh).permute(1, 2, 0).contiguous()\n",
    "            pred = pred.view(3, bs * num_p, num_point_mesh).permute(1, 2, 0).contiguous()\n",
    "\n",
    "    # pred, target 간 유클리드 거리를 계산하고 평균내어 각 픽셀에서 평균 거리를 계산\n",
    "    dis = torch.mean(torch.norm((pred - target), dim=2), dim=1)\n",
    "\n",
    "    # 거리 기반 loss 계산(confidence로 가중치)\n",
    "    loss = torch.mean((dis * pred_c - w * torch.log(pred_c)), dim=0)\n",
    "    \n",
    "    # 가장 confidence가 높은 예측 선택\n",
    "    pred_c = pred_c.view(bs, num_p)\n",
    "    how_max, which_max = torch.max(pred_c, 1)\n",
    "    dis = dis.view(bs, num_p)\n",
    "\n",
    "    t = ori_t[which_max[0]] + points[which_max[0]]\n",
    "    points = points.view(1, bs * num_p, 3)\n",
    "\n",
    "    ori_base = ori_base[which_max[0]].view(1, 3, 3).contiguous()\n",
    "    ori_t = t.repeat(bs * num_p, 1).contiguous().view(1, bs * num_p, 3)\n",
    "    new_points = torch.bmm((points - ori_t), ori_base).contiguous()\n",
    "\n",
    "    new_target = ori_target[0].view(1, num_point_mesh, 3).contiguous()\n",
    "    ori_t = t.repeat(num_point_mesh, 1).contiguous().view(1, num_point_mesh, 3)\n",
    "    new_target = torch.bmm((new_target - ori_t), ori_base).contiguous()\n",
    "\n",
    "    # print('------------> ', dis[0][which_max[0]].item(), pred_c[0][which_max[0]].item(), idx[0].item())\n",
    "    del knn\n",
    "    return loss, dis[0][which_max[0]], new_points.detach(), new_target.detach() # loss, max distance, new_points, new_target\n",
    "\n",
    "\n",
    "class Loss(_Loss):\n",
    "\n",
    "    def __init__(self, num_points_mesh, sym_list):\n",
    "        super(Loss, self).__init__(True)\n",
    "        self.num_pt_mesh = num_points_mesh\n",
    "        self.sym_list = sym_list\n",
    "\n",
    "    def forward(self, pred_r, pred_t, pred_c, target, model_points, idx, points, w, refine):\n",
    "\n",
    "        return loss_calculation(pred_r, pred_t, pred_c, target, model_points, idx, points, w, refine, self.num_pt_mesh, self.sym_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./lib/loss_refiner.py\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from lib.knn.__init__ import KNearestNeighbor\n",
    "\n",
    "\n",
    "def loss_calculation(pred_r, pred_t, target, model_points, idx, points, num_point_mesh, sym_list):\n",
    "    knn = KNearestNeighbor(1)\n",
    "    pred_r = pred_r.view(1, 1, -1)\n",
    "    pred_t = pred_t.view(1, 1, -1)\n",
    "    bs, num_p, _ = pred_r.size()\n",
    "    num_input_points = len(points[0])\n",
    "\n",
    "    pred_r = pred_r / (torch.norm(pred_r, dim=2).view(bs, num_p, 1))\n",
    "    \n",
    "    base = torch.cat(((1.0 - 2.0*(pred_r[:, :, 2]**2 + pred_r[:, :, 3]**2)).view(bs, num_p, 1),\\\n",
    "                      (2.0*pred_r[:, :, 1]*pred_r[:, :, 2] - 2.0*pred_r[:, :, 0]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (2.0*pred_r[:, :, 0]*pred_r[:, :, 2] + 2.0*pred_r[:, :, 1]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (2.0*pred_r[:, :, 1]*pred_r[:, :, 2] + 2.0*pred_r[:, :, 3]*pred_r[:, :, 0]).view(bs, num_p, 1), \\\n",
    "                      (1.0 - 2.0*(pred_r[:, :, 1]**2 + pred_r[:, :, 3]**2)).view(bs, num_p, 1), \\\n",
    "                      (-2.0*pred_r[:, :, 0]*pred_r[:, :, 1] + 2.0*pred_r[:, :, 2]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (-2.0*pred_r[:, :, 0]*pred_r[:, :, 2] + 2.0*pred_r[:, :, 1]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (2.0*pred_r[:, :, 0]*pred_r[:, :, 1] + 2.0*pred_r[:, :, 2]*pred_r[:, :, 3]).view(bs, num_p, 1), \\\n",
    "                      (1.0 - 2.0*(pred_r[:, :, 1]**2 + pred_r[:, :, 2]**2)).view(bs, num_p, 1)), dim=2).contiguous().view(bs * num_p, 3, 3)\n",
    "    \n",
    "    ori_base = base\n",
    "    base = base.contiguous().transpose(2, 1).contiguous()\n",
    "    model_points = model_points.view(bs, 1, num_point_mesh, 3).repeat(1, num_p, 1, 1).view(bs * num_p, num_point_mesh, 3)\n",
    "    target = target.view(bs, 1, num_point_mesh, 3).repeat(1, num_p, 1, 1).view(bs * num_p, num_point_mesh, 3)\n",
    "    ori_target = target\n",
    "    pred_t = pred_t.contiguous().view(bs * num_p, 1, 3)\n",
    "    ori_t = pred_t\n",
    "\n",
    "    pred = torch.add(torch.bmm(model_points, base), pred_t)\n",
    "\n",
    "    if idx[0].item() in sym_list:\n",
    "        target = target[0].transpose(1, 0).contiguous().view(3, -1)\n",
    "        pred = pred.permute(2, 0, 1).contiguous().view(3, -1)\n",
    "        inds = knn(target.unsqueeze(0), pred.unsqueeze(0))\n",
    "        target = torch.index_select(target, 1, inds.view(-1) - 1)\n",
    "        target = target.view(3, bs * num_p, num_point_mesh).permute(1, 2, 0).contiguous()\n",
    "        pred = pred.view(3, bs * num_p, num_point_mesh).permute(1, 2, 0).contiguous()\n",
    "\n",
    "    dis = torch.mean(torch.norm((pred - target), dim=2), dim=1)\n",
    "\n",
    "    t = ori_t[0]\n",
    "    points = points.view(1, num_input_points, 3)\n",
    "\n",
    "    ori_base = ori_base[0].view(1, 3, 3).contiguous()\n",
    "    ori_t = t.repeat(bs * num_input_points, 1).contiguous().view(1, bs * num_input_points, 3)\n",
    "    new_points = torch.bmm((points - ori_t), ori_base).contiguous()\n",
    "\n",
    "    new_target = ori_target[0].view(1, num_point_mesh, 3).contiguous()\n",
    "    ori_t = t.repeat(num_point_mesh, 1).contiguous().view(1, num_point_mesh, 3)\n",
    "    new_target = torch.bmm((new_target - ori_t), ori_base).contiguous()\n",
    "\n",
    "    # print('------------> ', dis.item(), idx[0].item())\n",
    "    del knn\n",
    "    return dis, new_points.detach(), new_target.detach()\n",
    "\n",
    "\n",
    "class Loss_refine(_Loss):\n",
    "\n",
    "    def __init__(self, num_points_mesh, sym_list):\n",
    "        super(Loss_refine, self).__init__(True)\n",
    "        self.num_pt_mesh = num_points_mesh\n",
    "        self.sym_list = sym_list\n",
    "\n",
    "\n",
    "    def forward(self, pred_r, pred_t, target, model_points, idx, points):\n",
    "        return loss_calculation(pred_r, pred_t, target, model_points, idx, points, self.num_pt_mesh, self.sym_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
