{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdad09b",
   "metadata": {},
   "source": [
    "# PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation\n",
    "---\n",
    "This notebook provides a comprehensive explanation of **PoseCNN**, an architecture designed for **6D object pose estimation** in cluttered scenes. PoseCNN tackles the challenges of occlusion and object symmetry effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6b366",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "PoseCNN is a convolutional neural network developed to estimate the **6D pose** of known objects. It decouples the problem into **semantic labeling**, **3D translation estimation**, and **3D rotation regression**. Its main contributions include:\n",
    "\n",
    "- A robust framework for 6D pose estimation in occluded and cluttered scenes.\n",
    "- Introduction of **ShapeMatch Loss** to handle symmetric objects.\n",
    "- Creation of the **YCB-Video dataset** with 133,827 frames for 6D pose estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0a967",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "### YCB-Video Dataset\n",
    "This dataset contains **133,827 RGB-D frames** across 92 videos. It provides:\n",
    "- 21 objects with 6D pose annotations.\n",
    "- Severe occlusions and symmetric objects.\n",
    "\n",
    "### OccludedLINEMOD Dataset\n",
    "- A benchmark for 6D pose estimation with significant occlusions.\n",
    "- 1,214 frames with 6D poses for 8 objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8daab",
   "metadata": {},
   "source": [
    "## 3. PoseCNN Architecture (Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71997c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(scores, labels):\n",
    "    \"\"\"\n",
    "    scores: a tensor [batch_size, num_classes, height, width]\n",
    "    labels: a tensor [batch_size, num_classes, height, width]\n",
    "    \"\"\"\n",
    "\n",
    "    cross_entropy = -torch.sum(labels * scores, dim=1)\n",
    "    loss = torch.div(torch.sum(cross_entropy), torch.sum(labels)+1e-10)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def smooth_l1_loss(vertex_pred, vertex_targets, vertex_weights, sigma=1.0):\n",
    "    sigma_2 = sigma ** 2\n",
    "    vertex_diff = vertex_pred - vertex_targets\n",
    "    diff = torch.mul(vertex_weights, vertex_diff)\n",
    "    abs_diff = torch.abs(diff)\n",
    "    smoothL1_sign = torch.lt(abs_diff, 1. / sigma_2).float().detach()\n",
    "    in_loss = torch.pow(diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "            + (abs_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "    loss = torch.div( torch.sum(in_loss), torch.sum(vertex_weights) + 1e-10 )\n",
    "    return loss\n",
    "\n",
    "# compute output\n",
    "if cfg.TRAIN.VERTEX_REG:\n",
    "    if cfg.TRAIN.POSE_REG:\n",
    "        out_logsoftmax, out_weight, out_vertex, out_logsoftmax_box, \\\n",
    "            bbox_labels, bbox_pred, bbox_targets, bbox_inside_weights, loss_pose_tensor, poses_weight \\\n",
    "            = network(inputs, labels, meta_data, extents, gt_boxes, poses, points, symmetry)\n",
    "\n",
    "        loss_label = loss_cross_entropy(out_logsoftmax, out_weight) # 예측 확률과 실제 라벨 간 차이(semantic segmentation)\n",
    "        loss_vertex = cfg.TRAIN.VERTEX_W * smooth_l1_loss(out_vertex, vertex_targets, vertex_weights) # 물체 vertex 위치 regression 손실\n",
    "        loss_box = loss_cross_entropy(out_logsoftmax_box, bbox_labels) # bbox 클래스 예측과 실제 bbox 클레스 간 차이\n",
    "        loss_location = smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights) # bbox 위치 regression 손실\n",
    "        loss_pose = torch.mean(loss_pose_tensor) # 물체의 Point matching loss(아래 PLOSS와 SLOSS)\n",
    "        loss = loss_label + loss_vertex + loss_box + loss_location + loss_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75317841",
   "metadata": {},
   "source": [
    "### Pose Loss (PLOSS)\n",
    "Pose Loss is used for **asymmetric objects**. It minimizes the average squared distance between the predicted and ground-truth 3D points transformed by the respective rotations.\n",
    "\n",
    "#### Pose Loss Formula:\n",
    "$$ PLOSS(q, \\tilde{q}) = \\frac{1}{2m} \\sum_{x \\in M} ||R(q)x - R(\\tilde{q})x||^2 $$\n",
    "Where:\n",
    "- $q$ is the ground-truth quaternion.\n",
    "- $\\tilde{q}$ is the predicted quaternion.\n",
    "- $R(q)$ is the rotation matrix derived from $q$.\n",
    "- $M$ is the set of 3D model points.\n",
    "\n",
    "### ShapeMatch Loss (SLOSS)\n",
    "ShapeMatch Loss is used for **symmetric objects**. Instead of comparing fixed points, it measures the closest point distance to handle symmetry effectively.\n",
    "\n",
    "#### ShapeMatch Loss Formula:\n",
    "$$ SLOSS(q, \\tilde{q}) = \\frac{1}{2m} \\sum_{x_1 \\in M} \\min_{x_2 \\in M} ||R(q)x_1 - R(\\tilde{q})x_2||^2 $$\n",
    "Where:\n",
    "- $x_1$ is a point on the estimated model.\n",
    "- $x_2$ is the closest point on the ground-truth model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20719717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose Loss and ShapeMatch Loss Implementation\n",
    "import torch\n",
    "import math\n",
    "\n",
    "POSE_CHANNELS = 4\n",
    "\n",
    "def pml_forward(bottom_prediction: torch.Tensor,\n",
    "                bottom_target: torch.Tensor,\n",
    "                bottom_weight: torch.Tensor,\n",
    "                points: torch.Tensor,\n",
    "                symmetry: torch.Tensor,\n",
    "                hard_angle: float):\n",
    "    \"\"\"\n",
    "    Python/PyTorch 구현의 pml_cuda_forward 개념적 복사본.\n",
    "    원본 .cu 코드와 동일한 인덱싱, 로직, 수학 연산을 재현.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bottom_prediction : torch.Tensor\n",
    "        [batch, num_classes*4] 예측 퀘터니언\n",
    "    bottom_target : torch.Tensor\n",
    "        [batch, num_classes*4] GT 퀘터니언\n",
    "    bottom_weight : torch.Tensor\n",
    "        [batch, num_classes*4] 해당 클래스 사용 여부\n",
    "    points : torch.Tensor\n",
    "        [num_classes, num_points, 3] 물체 포인트\n",
    "    symmetry : torch.Tensor\n",
    "        [num_classes], 대칭 물체 여부\n",
    "    hard_angle : float\n",
    "        hard angle 기준 값\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_data: shape [1], 전체 손실\n",
    "    bottom_diff: [batch, num_classes*POSE_CHANNELS], 회전에 대한 그래디언트\n",
    "    \"\"\"\n",
    "\n",
    "    device = bottom_prediction.device\n",
    "    batch_size = bottom_prediction.size(0)\n",
    "    num_classes = points.size(0)\n",
    "    num_points = points.size(1)\n",
    "\n",
    "    # rotations: [batch, num_points, 54], 54 = 6*9\n",
    "    # 각 포인트 마다 \n",
    "    # 0-8: GT rotation matrix\n",
    "    # 9-17: predicted rotation matrix\n",
    "    # 18-26, 27-35, 36-44, 45-53: 4개의 미분 행렬(각 9원소, 총 36원소)\n",
    "    rotations = torch.zeros(batch_size, num_points, 6*9, device=device)\n",
    "    losses = torch.zeros(batch_size, num_points, device=device)\n",
    "    diffs = torch.zeros(batch_size, num_points, POSE_CHANNELS * num_classes, device=device)\n",
    "    angles_batch = torch.zeros(batch_size, device=device)\n",
    "\n",
    "    # points를 원래 코드처럼 flatten 하여 인덱싱하기 (C++에서 point[index+k]와 유사)\n",
    "    # index = index_cls * num_points * 3 + p * 3 로 접근\n",
    "    points_flat = points.view(-1, 3)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        # find class\n",
    "        index_cls = -1\n",
    "        for c in range(num_classes):\n",
    "            if bottom_weight[n, c*POSE_CHANNELS].item() > 0:\n",
    "                index_cls = c\n",
    "                break\n",
    "        if index_cls == -1:\n",
    "            continue\n",
    "\n",
    "        # GT quaternion\n",
    "        s_gt = bottom_target[n, index_cls*POSE_CHANNELS+0].item()\n",
    "        u_gt = bottom_target[n, index_cls*POSE_CHANNELS+1].item()\n",
    "        v_gt = bottom_target[n, index_cls*POSE_CHANNELS+2].item()\n",
    "        w_gt = bottom_target[n, index_cls*POSE_CHANNELS+3].item()\n",
    "\n",
    "        # Pred quaternion\n",
    "        s_pr = bottom_prediction[n, index_cls*POSE_CHANNELS+0].item()\n",
    "        u_pr = bottom_prediction[n, index_cls*POSE_CHANNELS+1].item()\n",
    "        v_pr = bottom_prediction[n, index_cls*POSE_CHANNELS+2].item()\n",
    "        w_pr = bottom_prediction[n, index_cls*POSE_CHANNELS+3].item()\n",
    "\n",
    "        # Compute GT rotation matrix\n",
    "        # indices for rotations\n",
    "        # ind = n * num_points * 6 * 9 + p * 6 * 9; -> Python: rotations[n, p, ...]\n",
    "        # 여기서는 p 루프 후에 할당\n",
    "        # 각 p마다 GT/Pred/derivatives를 rotations에 저장\n",
    "\n",
    "        # 각 포인트별 반복\n",
    "        for p in range(num_points):\n",
    "            # set rotations for GT quaternion\n",
    "            ind_base = p * 6 * 9\n",
    "            # GT rotation\n",
    "            rotations[n, p, 0] = s_gt*s_gt + u_gt*u_gt - v_gt*v_gt - w_gt*w_gt\n",
    "            rotations[n, p, 1] = 2*(u_gt*v_gt - s_gt*w_gt)\n",
    "            rotations[n, p, 2] = 2*(u_gt*w_gt + s_gt*v_gt)\n",
    "            rotations[n, p, 3] = 2*(u_gt*v_gt + s_gt*w_gt)\n",
    "            rotations[n, p, 4] = s_gt*s_gt - u_gt*u_gt + v_gt*v_gt - w_gt*w_gt\n",
    "            rotations[n, p, 5] = 2*(v_gt*w_gt - s_gt*u_gt)\n",
    "            rotations[n, p, 6] = 2*(u_gt*w_gt - s_gt*v_gt)\n",
    "            rotations[n, p, 7] = 2*(v_gt*w_gt + s_gt*u_gt)\n",
    "            rotations[n, p, 8] = s_gt*s_gt - u_gt*u_gt - v_gt*v_gt + w_gt*w_gt\n",
    "\n",
    "            # predicted rotation\n",
    "            rotations[n, p, 9] = s_pr*s_pr + u_pr*u_pr - v_pr*v_pr - w_pr*w_pr\n",
    "            rotations[n, p,10] = 2*(u_pr*v_pr - s_pr*w_pr)\n",
    "            rotations[n, p,11] = 2*(u_pr*w_pr + s_pr*v_pr)\n",
    "            rotations[n, p,12] = 2*(u_pr*v_pr + s_pr*w_pr)\n",
    "            rotations[n, p,13] = s_pr*s_pr - u_pr*u_pr + v_pr*v_pr - w_pr*w_pr\n",
    "            rotations[n, p,14] = 2*(v_pr*w_pr - s_pr*u_pr)\n",
    "            rotations[n, p,15] = 2*(u_pr*w_pr - s_pr*v_pr)\n",
    "            rotations[n, p,16] = 2*(v_pr*w_pr + s_pr*u_pr)\n",
    "            rotations[n, p,17] = s_pr*s_pr - u_pr*u_pr - v_pr*v_pr + w_pr*w_pr\n",
    "\n",
    "            # 각도 계산(p == 0일때)\n",
    "            if p == 0:\n",
    "                d = s_gt*s_pr + u_gt*u_pr + v_gt*v_pr + w_gt*w_pr # 내적을 통한 유사도 표현\n",
    "                angle = math.acos(max(min(2*d*d - 1,1),-1))*180.0/math.pi # θ/2 -> θ \n",
    "                if angle > hard_angle:\n",
    "                    angles_batch[n] = 1.0\n",
    "\n",
    "            # Derivatives of Ru to quaternion (from original code)\n",
    "            # ind + 18 sets\n",
    "            # For simplicity, reuse s_pr,u_pr,v_pr,w_pr from predicted quaternion\n",
    "            # This is from original code\n",
    "            # 18-26\n",
    "            idx_deriv = 18\n",
    "            rotations[n, p, idx_deriv+0] = 2 * s_pr\n",
    "            rotations[n, p, idx_deriv+1] = -2 * w_pr\n",
    "            rotations[n, p, idx_deriv+2] = 2 * v_pr\n",
    "            rotations[n, p, idx_deriv+3] = 2 * w_pr\n",
    "            rotations[n, p, idx_deriv+4] = 2 * s_pr\n",
    "            rotations[n, p, idx_deriv+5] = -2 * u_pr\n",
    "            rotations[n, p, idx_deriv+6] = -2 * v_pr\n",
    "            rotations[n, p, idx_deriv+7] = 2 * u_pr\n",
    "            rotations[n, p, idx_deriv+8] = 2 * s_pr\n",
    "\n",
    "            # 27-35\n",
    "            idx_deriv = 27\n",
    "            rotations[n, p, idx_deriv+0] = 2 * u_pr\n",
    "            rotations[n, p, idx_deriv+1] = 2 * v_pr\n",
    "            rotations[n, p, idx_deriv+2] = 2 * w_pr\n",
    "            rotations[n, p, idx_deriv+3] = 2 * v_pr\n",
    "            rotations[n, p, idx_deriv+4] = -2 * u_pr\n",
    "            rotations[n, p, idx_deriv+5] = -2 * s_pr\n",
    "            rotations[n, p, idx_deriv+6] = 2 * w_pr\n",
    "            rotations[n, p, idx_deriv+7] = 2 * s_pr\n",
    "            rotations[n, p, idx_deriv+8] = -2 * u_pr\n",
    "\n",
    "            # 36-44\n",
    "            idx_deriv = 36\n",
    "            rotations[n, p, idx_deriv+0] = -2 * v_pr\n",
    "            rotations[n, p, idx_deriv+1] = 2 * u_pr\n",
    "            rotations[n, p, idx_deriv+2] = 2 * s_pr\n",
    "            rotations[n, p, idx_deriv+3] = 2 * u_pr\n",
    "            rotations[n, p, idx_deriv+4] = 2 * v_pr\n",
    "            rotations[n, p, idx_deriv+5] = 2 * w_pr\n",
    "            rotations[n, p, idx_deriv+6] = -2 * s_pr\n",
    "            rotations[n, p, idx_deriv+7] = 2 * w_pr\n",
    "            rotations[n, p, idx_deriv+8] = -2 * v_pr\n",
    "\n",
    "            # 45-53\n",
    "            idx_deriv = 45\n",
    "            rotations[n, p, idx_deriv+0] = -2 * w_pr\n",
    "            rotations[n, p, idx_deriv+1] = -2 * s_pr\n",
    "            rotations[n, p, idx_deriv+2] = 2 * u_pr\n",
    "            rotations[n, p, idx_deriv+3] = 2 * s_pr\n",
    "            rotations[n, p, idx_deriv+4] = -2 * w_pr\n",
    "            rotations[n, p, idx_deriv+5] = 2 * v_pr\n",
    "            rotations[n, p, idx_deriv+6] = 2 * u_pr\n",
    "            rotations[n, p, idx_deriv+7] = 2 * v_pr\n",
    "            rotations[n, p, idx_deriv+8] = 2 * w_pr\n",
    "\n",
    "            index = index_cls * num_points * 3 + p * 3\n",
    "            # rotate the first point with predicted rotation:\n",
    "            x1 = rotations[n, p, 9+0]*points_flat[index][0]+rotations[n, p, 9+1]*points_flat[index][1]+rotations[n, p, 9+2]*points_flat[index][2]\n",
    "            y1 = rotations[n, p, 9+3]*points_flat[index][0]+rotations[n, p, 9+4]*points_flat[index][1]+rotations[n, p, 9+5]*points_flat[index][2]\n",
    "            z1 = rotations[n, p, 9+6]*points_flat[index][0]+rotations[n, p, 9+7]*points_flat[index][1]+rotations[n, p, 9+8]*points_flat[index][2]\n",
    "\n",
    "            ## symmetry ##\n",
    "            if symmetry[index_cls].item() > 0:\n",
    "                dmin = float('inf')\n",
    "                index_min = 0\n",
    "                for i in range(num_points):\n",
    "                    index2 = index_cls * num_points * 3 + i * 3\n",
    "                    x2 = rotations[n, p, 0]*points_flat[index2][0]+rotations[n, p, 1]*points_flat[index2][1]+rotations[n, p, 2]*points_flat[index2][2]\n",
    "                    y2 = rotations[n, p, 3]*points_flat[index2][0]+rotations[n, p, 4]*points_flat[index2][1]+rotations[n, p, 5]*points_flat[index2][2]\n",
    "                    z2 = rotations[n, p, 6]*points_flat[index2][0]+rotations[n, p, 7]*points_flat[index2][1]+rotations[n, p, 8]*points_flat[index2][2]\n",
    "                    dist = (x1 - x2)**2+(y1 - y2)**2+(z1 - z2)**2\n",
    "                    if dist < dmin:\n",
    "                        dmin = dist\n",
    "                        index_min = index2\n",
    "            else:\n",
    "                index_min = index\n",
    "\n",
    "            x2 = rotations[n, p, 0]*points_flat[index_min][0]+rotations[n, p, 1]*points_flat[index_min][1]+rotations[n, p, 2]*points_flat[index_min][2]\n",
    "            y2 = rotations[n, p, 3]*points_flat[index_min][0]+rotations[n, p, 4]*points_flat[index_min][1]+rotations[n, p, 5]*points_flat[index_min][2]\n",
    "            z2 = rotations[n, p, 6]*points_flat[index_min][0]+rotations[n, p, 7]*points_flat[index_min][1]+rotations[n, p, 8]*points_flat[index_min][2]\n",
    "\n",
    "            # smooth l1 loss\n",
    "            distance = 0.0\n",
    "            index_diff = n * num_points * POSE_CHANNELS * num_classes + p * POSE_CHANNELS * num_classes + POSE_CHANNELS * index_cls\n",
    "            coords_diff = [(x1 - x2), (y1 - y2), (z1 - z2)]\n",
    "            for j in range(3):\n",
    "                diff_val = coords_diff[j]\n",
    "                abs_diff = abs(diff_val)\n",
    "                if abs_diff < 1:\n",
    "                    distance += 0.5 * diff_val*diff_val\n",
    "                    df = diff_val\n",
    "                else:\n",
    "                    distance += abs_diff - 0.5\n",
    "                    df = 1.0 if diff_val > 0 else -1.0\n",
    "\n",
    "                # diffs 계산\n",
    "                # k 루프\n",
    "                # rotations 미분 인덱싱\n",
    "                # 첫 미분행렬: ind + 18\n",
    "                # ind = n * num_points * 6 * 9 + p * 6 * 9\n",
    "                ind_base_rot = n * num_points * 6 * 9 + p * 6 * 9\n",
    "                # 각 파셜 파트\n",
    "                # diffs[index_diff + 0 ... 3]\n",
    "                # point[index + k] * rotations[ind + ...] / num_points\n",
    "                # k in {0,1,2}\n",
    "                for k_ in range(3):\n",
    "                    # 4 세트의 미분 행렬\n",
    "                    # 0 set: ind+18\n",
    "                    # 1 set: ind+27\n",
    "                    # 2 set: ind+36\n",
    "                    # 3 set: ind+45\n",
    "                    # j * 3 + k_ 로 접근\n",
    "\n",
    "                    # channel별 diffs\n",
    "                    # diffs for quaternion channels 0..3\n",
    "                    # original code:\n",
    "                    # diffs[index_diff + 0] += df * point[index + k_] * rotations[ind + 18 + j*3 + k_] / num_points;\n",
    "                    # diff[channel 0]\n",
    "                    r18 = rotations.view(-1)[ind_base_rot + 18 + j*3 + k_].item()\n",
    "                    r27 = rotations.view(-1)[ind_base_rot + 27 + j*3 + k_].item()\n",
    "                    r36 = rotations.view(-1)[ind_base_rot + 36 + j*3 + k_].item()\n",
    "                    r45 = rotations.view(-1)[ind_base_rot + 45 + j*3 + k_].item()\n",
    "\n",
    "                    val_point = points_flat[index + k_][0] if k_ == 0 else (points_flat[index + k_][1] if k_==1 else points_flat[index + k_][2])\n",
    "                    # 하지만 points_flat[index+k_][dim]은 index+k_에서 이미 3D point: points_flat는 (..,3)\n",
    "                    # 실제로 index+k_는 point 인덱스 자체임. index는 3D점 시작점\n",
    "                    # points_flat[index + k_]은 불가. index+k_는 3단위 증가\n",
    "                    # 수정: index는 이미 point의 시작 index. k_를 좌표축으로 착각했음\n",
    "                    # original code: point[index + k] 은 (index: start of point) + k (0=x,1=y,2=z)\n",
    "                    val_point = points_flat[index][k_]\n",
    "\n",
    "                    # num_points로 나누기\n",
    "                    scale = df * val_point.item() / num_points\n",
    "\n",
    "                    diffs[n, p, index_cls*POSE_CHANNELS + 0] += scale * r18\n",
    "                    diffs[n, p, index_cls*POSE_CHANNELS + 1] += scale * r27\n",
    "                    diffs[n, p, index_cls*POSE_CHANNELS + 2] += scale * r36\n",
    "                    diffs[n, p, index_cls*POSE_CHANNELS + 3] += scale * r45\n",
    "\n",
    "            losses[n, p] = distance / num_points\n",
    "\n",
    "    # angles sum\n",
    "    batch_hard = angles_batch.sum().item()\n",
    "\n",
    "    # sum diffs and losses for bottom_diff, losses_batch\n",
    "    bottom_diff = torch.zeros(batch_size, POSE_CHANNELS * num_classes, device=device)\n",
    "    losses_batch = torch.zeros(batch_size, device=device)\n",
    "    if batch_hard > 0:\n",
    "        for n in range(batch_size):\n",
    "            if angles_batch[n].item() > 0:\n",
    "                # diffs sum\n",
    "                # bottom_diff[index] = sum over p of diffs / batch_hard\n",
    "                # losses_batch[n] = sum(losses[n,:]) / batch_hard\n",
    "                bottom_diff[n, :] = diffs[n, :, :].sum(dim=0) / batch_hard\n",
    "                losses_batch[n] = losses[n, :].sum() / batch_hard\n",
    "\n",
    "    top_data = losses_batch.sum().unsqueeze(0)\n",
    "\n",
    "    return top_data, bottom_diff\n",
    "\n",
    "\n",
    "def pml_backward(grad_loss: torch.Tensor, bottom_diff: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Python/PyTorch로 pml_cuda_backward 개념 구현.\n",
    "    grad_loss: [1]\n",
    "    bottom_diff: [batch, num_classes*POSE_CHANNELS]\n",
    "\n",
    "    output:\n",
    "    grad_rotation: [batch, num_classes*POSE_CHANNELS]\n",
    "    grad_rotation = grad_loss[0]*bottom_diff\n",
    "    \"\"\"\n",
    "    grad_rotation = grad_loss[0] * bottom_diff\n",
    "    return grad_rotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250c6bb",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics (Extended)\n",
    "PoseCNN uses two primary evaluation metrics for 6D object pose estimation:\n",
    "\n",
    "### 4.1. ADD (Average Distance)\n",
    "ADD measures the mean distance between corresponding points of the predicted and ground-truth 3D model poses.\n",
    "\n",
    "$$ ADD = \\frac{1}{m} \\sum_{x \\in M} \\| (R\\tilde{x} + T) - (Rx + T) \\| $$\n",
    "- $R$ and $T$ are the ground-truth rotation and translation.\n",
    "- $\\tilde{R}$ and $\\tilde{T}$ are the predicted rotation and translation.\n",
    "- $x$ is a point on the 3D model with $m$ total points.\n",
    "\n",
    "ADD is effective for **asymmetric objects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d39ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD Metric Implementation\n",
    "def transform_pts_Rt(pts, R, t):\n",
    "    \"\"\"\n",
    "    Applies a rigid transformation to 3D points.\n",
    "\n",
    "    :param pts: nx3 ndarray with 3D points.\n",
    "    :param R: 3x3 rotation matrix.\n",
    "    :param t: 3x1 translation vector.\n",
    "    :return: nx3 ndarray with transformed 3D points.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1] == 3)\n",
    "    pts_t = R.dot(pts.T) + t.reshape((3, 1))\n",
    "    return pts_t.T\n",
    "\n",
    "def add(R_est, t_est, R_gt, t_gt, pts):\n",
    "    \"\"\"\n",
    "    Average Distance of Model Points for objects with no indistinguishable views\n",
    "    - by Hinterstoisser et al. (ACCV 2012).\n",
    "\n",
    "    :param R_est, t_est: Estimated pose (3x3 rot. matrix and 3x1 trans. vector).\n",
    "    :param R_gt, t_gt: GT pose (3x3 rot. matrix and 3x1 trans. vector).\n",
    "    :param model: Object model given by a dictionary where item 'pts'\n",
    "    is nx3 ndarray with 3D model points.\n",
    "    :return: Error of pose_est w.r.t. pose_gt.\n",
    "    \"\"\"\n",
    "    pts_est = transform_pts_Rt(pts, R_est, t_est)\n",
    "    pts_gt = transform_pts_Rt(pts, R_gt, t_gt)\n",
    "    e = np.linalg.norm(pts_est - pts_gt, axis=1).mean()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d502a",
   "metadata": {},
   "source": [
    "### 4.2. ADD-S (ADD-Symmetric)\n",
    "ADD-S extends ADD to handle symmetric objects by taking the closest point distance:\n",
    "\n",
    "$$ ADD-S = \\frac{1}{m} \\sum_{x_1 \\in M} \\min_{x_2 \\in M} \\| (R\\tilde{x}_1 + T) - (Rx_2 + T) \\| $$\n",
    "- $x_1$ is a point on the predicted model.\n",
    "- $x_2$ is the closest point on the ground-truth model.\n",
    "\n",
    "ADD-S ensures robustness for symmetric objects, where exact point correspondence is ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD-S Metric Implementation\n",
    "from scipy import spatial\n",
    "\n",
    "def transform_pts_Rt(pts, R, t):\n",
    "    \"\"\"\n",
    "    Applies a rigid transformation to 3D points.\n",
    "\n",
    "    :param pts: nx3 ndarray with 3D points.\n",
    "    :param R: 3x3 rotation matrix.\n",
    "    :param t: 3x1 translation vector.\n",
    "    :return: nx3 ndarray with transformed 3D points.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1] == 3)\n",
    "    pts_t = R.dot(pts.T) + t.reshape((3, 1))\n",
    "    return pts_t.T\n",
    "\n",
    "def adi(R_est, t_est, R_gt, t_gt, pts):\n",
    "    \"\"\"\n",
    "    Average Distance of Model Points for objects with indistinguishable views\n",
    "    - by Hinterstoisser et al. (ACCV 2012).\n",
    "\n",
    "    :param R_est, t_est: Estimated pose (3x3 rot. matrix and 3x1 trans. vector).\n",
    "    :param R_gt, t_gt: GT pose (3x3 rot. matrix and 3x1 trans. vector).\n",
    "    :param model: Object model given by a dictionary where item 'pts'\n",
    "    is nx3 ndarray with 3D model points.\n",
    "    :return: Error of pose_est w.r.t. pose_gt.\n",
    "    \"\"\"\n",
    "    pts_est = transform_pts_Rt(pts, R_est, t_est)\n",
    "    pts_gt = transform_pts_Rt(pts, R_gt, t_gt)\n",
    "\n",
    "    # Calculate distances to the nearest neighbors from pts_gt to pts_est\n",
    "    nn_index = spatial.cKDTree(pts_est) # KD-Tree\n",
    "    nn_dists, _ = nn_index.query(pts_gt, k=1)\n",
    "\n",
    "    e = nn_dists.mean()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663b66ea",
   "metadata": {},
   "source": [
    "## 5. PoseCNN Architecture (Overview)\n",
    "PoseCNN consists of three main branches:\n",
    "\n",
    "1. **Semantic Labeling:** Identifies object pixels in the input image.\n",
    "2. **3D Translation Estimation:** Localizes the object center and estimates the distance from the camera.\n",
    "3. **3D Rotation Regression:** Estimates the orientation of the object using a quaternion representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb259f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi_target_layer (학습에 사용할 classification label과 Box regression Target 값 생성)\n",
    "\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from fcn.config import cfg\n",
    "from utils.bbox_transform import bbox_transform\n",
    "from utils.cython_bbox import bbox_overlaps\n",
    "\n",
    "def roi_target_layer(rpn_rois, gt_boxes):\n",
    "    \"\"\"\n",
    "    Assign RoIs to ground truth boxes and compute classification labels and\n",
    "    bounding box regression targets.\n",
    "    \n",
    "    Args:\n",
    "        rpn_rois: Tensor of shape (N, 6) - [batch_id, class, x1, y1, x2, y2].\n",
    "        gt_boxes: Tensor of shape (batch, num_classes, 5) - [x1, y1, x2, y2, class].\n",
    "\n",
    "    Returns:\n",
    "        label_blob: One-hot encoded classification labels.\n",
    "        bbox_targets: Bounding box regression targets.\n",
    "        bbox_inside_weights: Weights for inside boxes.\n",
    "        bbox_outside_weights: Weights for outside boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tensors to NumPy arrays for processing\n",
    "    rpn_rois = rpn_rois.detach().cpu().numpy()\n",
    "    gt_boxes = gt_boxes.detach().cpu().numpy()\n",
    "    num_classes = gt_boxes.shape[1]\n",
    "\n",
    "    # Prepare RoI and GT blobs for processing\n",
    "    roi_blob = rpn_rois[:, (0, 2, 3, 4, 5, 1)]  # Rearrange columns\n",
    "    gt_box_blob = np.vstack([\n",
    "        np.hstack([\n",
    "            np.full((gt_boxes.shape[1], 1), i),  # Batch index\n",
    "            gt_boxes[i, :, :4],\n",
    "            gt_boxes[i, :, 4:5]  # Class\n",
    "        ])\n",
    "        for i in range(gt_boxes.shape[0])\n",
    "        if np.any(gt_boxes[i, :, -1] > 0)\n",
    "    ])\n",
    "\n",
    "    # Sample RoIs to create classification labels and regression targets\n",
    "    labels, bbox_targets, bbox_inside_weights = _sample_rois(roi_blob, gt_box_blob, num_classes)\n",
    "    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)  # Binary outside weights\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    label_blob = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
    "    valid_indices = labels > 0\n",
    "    label_blob[valid_indices, labels[valid_indices].astype(int)] = 1.0\n",
    "\n",
    "    # Convert outputs back to PyTorch tensors\n",
    "    return (\n",
    "        torch.tensor(label_blob, device=rpn_rois.device, dtype=torch.float32),\n",
    "        torch.tensor(bbox_targets, device=rpn_rois.device, dtype=torch.float32),\n",
    "        torch.tensor(bbox_inside_weights, device=rpn_rois.device, dtype=torch.float32),\n",
    "        torch.tensor(bbox_outside_weights, device=rpn_rois.device, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "def _get_bbox_regression_labels(bbox_target_data, num_classes):\n",
    "    \"\"\"\n",
    "    Expand compact bounding-box regression targets into a per-class format.\n",
    "\n",
    "    Args:\n",
    "        bbox_target_data: (N, 5) np.array - [class, tx, ty, tw, th].\n",
    "        num_classes: Total number of classes. (int)\n",
    "\n",
    "    Returns:\n",
    "        bbox_targets: (N, 4 * num_classes) regression targets.\n",
    "        bbox_inside_weights: (N, 4 * num_classes) inside weights.\n",
    "    \"\"\"\n",
    "\n",
    "    clss = bbox_target_data[:, 0].astype(int)\n",
    "    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n",
    "    bbox_inside_weights = np.zeros_like(bbox_targets, dtype=np.float32)\n",
    "\n",
    "    # Fill in regression targets for foreground classes\n",
    "    for i, cls in enumerate(clss):\n",
    "        if cls > 0:\n",
    "            start = 4 * cls\n",
    "            bbox_targets[i, start:start+4] = bbox_target_data[i, 1:]\n",
    "            bbox_inside_weights[i, start:start+4] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n",
    "\n",
    "    return bbox_targets, bbox_inside_weights\n",
    "\n",
    "\n",
    "\n",
    "def _compute_targets(ex_rois, gt_rois, labels):\n",
    "    \"\"\"\n",
    "    Compute bounding-box regression targets for RoIs.\n",
    "    \n",
    "    Args:\n",
    "        ex_rois: (N, 4) array of proposed RoIs.\n",
    "        gt_rois: (N, 4) array of ground truth boxes.\n",
    "        labels: (N,) array of class labels.\n",
    "\n",
    "    Returns:\n",
    "        targets: (N, 5) array of regression targets [class, tx, ty, tw, th].\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure inputs have consistent dimensions\n",
    "    assert ex_rois.shape[0] == gt_rois.shape[0]\n",
    "    assert ex_rois.shape[1] == 4\n",
    "    assert gt_rois.shape[1] == 4\n",
    "\n",
    "    # Compute the bounding-box transformation\n",
    "    targets = bbox_transform(ex_rois, gt_rois)\n",
    "\n",
    "    # Normalize the targets if configured\n",
    "    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "        targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n",
    "                   / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n",
    "\n",
    "    return np.hstack((labels[:, None], targets)).astype(np.float32)\n",
    "\n",
    "\n",
    "def _sample_rois(all_rois, gt_boxes, num_classes):\n",
    "    \"\"\"\n",
    "    Generate a random sample of RoIs for training.\n",
    "    \n",
    "    Args:\n",
    "        all_rois: (N, 6) array of proposed RoIs.\n",
    "        gt_boxes: (M, 6) array of ground truth boxes.\n",
    "\n",
    "    Returns:\n",
    "        labels: (N,) classification labels.\n",
    "        bbox_targets: (N, 4 * num_classes) regression targets.\n",
    "        bbox_inside_weights: (N, 4 * num_classes) inside weights.\n",
    "    \"\"\"\n",
    "\n",
    "    if gt_boxes.shape[0] == 0:  # If no ground truth boxes exist\n",
    "        num = all_rois.shape[0]\n",
    "        return (\n",
    "            np.zeros(num, dtype=np.float32),\n",
    "            np.zeros((num, 4 * num_classes), dtype=np.float32),\n",
    "            np.zeros((num, 4 * num_classes), dtype=np.float32),\n",
    "        )\n",
    "        \n",
    "    # Compute IoU overlaps between RoIs and GT boxes\n",
    "    overlaps = bbox_overlaps(all_rois[:, 1:5], gt_boxes[:, 1:5])\n",
    "\n",
    "    # Assign the best matching GT box to each RoI\n",
    "    gt_assignment = overlaps.argmax(axis=1)\n",
    "    max_overlaps = overlaps.max(axis=1)\n",
    "    labels = gt_boxes[gt_assignment, -1]\n",
    "\n",
    "    # Mark RoIs with low IoU as background\n",
    "    bg_inds = max_overlaps < cfg.TRAIN.FG_THRESH\n",
    "    labels[bg_inds] = 0\n",
    "\n",
    "    # Compute regression targets\n",
    "    bbox_target_data = _compute_targets(all_rois[:, 1:5], gt_boxes[gt_assignment, 1:5], labels)\n",
    "    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(bbox_target_data, num_classes)\n",
    "\n",
    "    return labels, bbox_targets, bbox_inside_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48633073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose_target_layer (학습에 사용할 Pose label과 Target 값 생성)\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from fcn.config import cfg\n",
    "from utils.bbox_transform import bbox_transform_inv\n",
    "from utils.cython_bbox import bbox_overlaps\n",
    "\n",
    "def pose_target_layer(\n",
    "    rois: torch.Tensor,\n",
    "    bbox_prob: torch.Tensor,\n",
    "    bbox_pred: torch.Tensor,\n",
    "    gt_boxes: torch.Tensor,\n",
    "    poses: torch.Tensor,\n",
    "    is_training: bool\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate pose targets for training.\n",
    "\n",
    "    Args:\n",
    "        rois: Proposed Regions of Interest (RoIs) [batch_id, class, x1, y1, x2, y2].\n",
    "        bbox_prob: Predicted class probabilities for each RoI.\n",
    "        bbox_pred: Predicted bounding box coordinates.\n",
    "        gt_boxes: Ground truth bounding boxes.\n",
    "        poses: Ground truth pose data.\n",
    "        is_training: Whether the model is in training mode.\n",
    "\n",
    "    Returns:\n",
    "        Updated RoIs, pose targets, and pose weights.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch tensors to NumPy arrays for processing\n",
    "    rois = rois.detach().cpu().numpy()\n",
    "    bbox_prob = bbox_prob.detach().cpu().numpy()\n",
    "    bbox_pred = bbox_pred.detach().cpu().numpy()\n",
    "    gt_boxes = gt_boxes.detach().cpu().numpy()\n",
    "    num_classes = bbox_prob.shape[1]  # Number of classes\n",
    "\n",
    "    # Apply normalization to bounding box predictions if configured\n",
    "    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "        stds = np.tile(cfg.TRAIN.BBOX_NORMALIZE_STDS, num_classes)\n",
    "        means = np.tile(cfg.TRAIN.BBOX_NORMALIZE_MEANS, num_classes)\n",
    "        bbox_pred_np *= stds\n",
    "        bbox_pred_np += means\n",
    "\n",
    "    # Decode bounding box predictions into actual coordinates\n",
    "    boxes = rois[:, 2:6].copy()  # Extract current box coordinates\n",
    "    pred_boxes = bbox_transform_inv(boxes, bbox_pred)  # Transform predictions into coordinates\n",
    "\n",
    "    # Update RoIs with predicted boxes and probabilities\n",
    "    for i in range(rois.shape[0]):\n",
    "        cls = int(rois[i, 1])  # Get the predicted class for this RoI\n",
    "        rois[i, 2:6] = pred_boxes[i, cls*4:cls*4+4]  # Assign predicted box\n",
    "        rois[i, 6] = bbox_prob[i, cls]  # Assign predicted class probability\n",
    "\n",
    "    # Prepare blobs for RoIs and ground truth\n",
    "    roi_blob = rois[:, [0, 2, 3, 4, 5, 1]]  # Format: [batch_id, x1, y1, x2, y2, class]\n",
    "    gt_box_blob = np.zeros((0, 6), dtype=np.float32)  # Initialize ground truth blob\n",
    "    pose_blob = np.zeros((0, 9), dtype=np.float32)  # Initialize pose blob\n",
    "\n",
    "    # Loop through ground truth boxes and prepare blobs\n",
    "    for i in range(gt_boxes.shape[0]):  # Batch-wise\n",
    "        for j in range(gt_boxes.shape[1]):  # Per-object\n",
    "            if gt_boxes[i, j, -1] > 0:  # Check if the object exists (class > 0)\n",
    "                gt_box = np.zeros((1, 6), dtype=np.float32)\n",
    "                gt_box[0, 0] = i  # Batch index\n",
    "                gt_box[0, 1:5] = gt_boxes[i, j, :4]  # Box coordinates\n",
    "                gt_box[0, 5] = gt_boxes[i, j, 4]  # Class label\n",
    "                gt_box_blob = np.vstack([gt_box_blob, gt_box])  # Add to blob\n",
    "                poses[i, j, 0] = i  # Assign batch index for poses\n",
    "                pose_blob = np.vstack([pose_blob, poses[i, j, :].cpu().reshape(1, 9)])  # Add pose data\n",
    "\n",
    "    # If no ground truth boxes exist, create empty targets and weights\n",
    "    if gt_box_blob.shape[0] == 0:\n",
    "        num = rois.shape[0]\n",
    "        poses_target = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "        poses_weight = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "    else:\n",
    "        # Compute overlaps between RoIs and ground truth boxes\n",
    "        overlaps = bbox_overlaps(\n",
    "            roi_blob[:, :5].astype(np.float32),\n",
    "            gt_box_blob[:, :5].astype(np.float32)\n",
    "        )\n",
    "        # Match each RoI to the best ground truth box\n",
    "        gt_assignment = overlaps.argmax(axis=1)\n",
    "        max_overlaps = overlaps.max(axis=1)\n",
    "        labels = gt_box_blob[gt_assignment, 5]  # Assign labels\n",
    "        quaternions = pose_blob[gt_assignment, 2:6]  # Assign poses (quaternions)\n",
    "\n",
    "        # Mark RoIs with low overlap as background\n",
    "        bg_inds = np.where(max_overlaps < cfg.TRAIN.FG_THRESH_POSE)[0]\n",
    "        labels[bg_inds] = 0\n",
    "\n",
    "        # Further filter based on mismatched class predictions\n",
    "        bg_inds = np.where(roi_blob[:, -1] != labels)[0]\n",
    "        labels[bg_inds] = 0\n",
    "\n",
    "        # In training, only keep positive samples for pose regression\n",
    "        if is_training:\n",
    "            fg_inds = np.where(labels > 0)[0]  # Positive RoIs\n",
    "            if len(fg_inds) > 0:\n",
    "                rois = rois[fg_inds]  # Filter RoIs\n",
    "                quaternions = quaternions[fg_inds]  # Filter quaternions\n",
    "                labels = labels[fg_inds]  # Filter labels\n",
    "\n",
    "        # Compute pose targets and weights\n",
    "        poses_target, poses_weight = _compute_pose_targets(quaternions, labels, num_classes)\n",
    "\n",
    "    # Convert NumPy arrays back to PyTorch tensors\n",
    "    return (\n",
    "        torch.tensor(rois_np, device=rois.device, dtype=torch.float32),\n",
    "        torch.tensor(poses_target, device=rois.device, dtype=torch.float32),\n",
    "        torch.tensor(poses_weight, device=rois.device, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "def _compute_pose_targets(quaternions, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Compute pose regression targets for an image.\n",
    "\n",
    "    quaternions: Ground truth quaternions (rotation information).\n",
    "    labels: Class labels for each RoI.\n",
    "    num_classes: Number of classes.\n",
    "    \"\"\"\n",
    "    num = quaternions.shape[0]  # Number of RoIs\n",
    "    poses_target = np.zeros((num, 4 * num_classes), dtype=np.float32)  # Initialize targets\n",
    "    poses_weight = np.zeros((num, 4 * num_classes), dtype=np.float32)  # Initialize weights\n",
    "\n",
    "    for i in range(num):\n",
    "        cls = labels[i]  # Class of this RoI\n",
    "        if cls > 0 and np.linalg.norm(quaternions[i, :]) > 0:  # Skip invalid quaternions\n",
    "            start = int(4 * cls)  # Start index for this class\n",
    "            poses_target[i, start:start+4] = quaternions[i]  # Assign quaternion\n",
    "            poses_weight[i, start:start+4] = 1.0  # Assign weight for this class\n",
    "\n",
    "    return poses_target, poses_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoseCNN Architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "from torch.nn.init import kaiming_normal_\n",
    "from layers.hard_label import HardLabel\n",
    "from layers.hough_voting import HoughVoting\n",
    "from layers.roi_pooling import RoIPool\n",
    "from layers.point_matching_loss import PMLoss\n",
    "from layers.roi_target_layer import roi_target_layer\n",
    "from layers.pose_target_layer import pose_target_layer\n",
    "from fcn.config import cfg\n",
    " \n",
    "def log_softmax_high_dimension(input):\n",
    "    \"\"\"\n",
    "    Compute the log softmax over a high-dimensional input tensor for numerical stability.\n",
    "    \n",
    "    This function calculates:\n",
    "    log_softmax(x) = x_i - log(Σ_j exp(x_j))\n",
    "    \n",
    "    To ensure numerical stability, the maximum value `m` is subtracted from `input` before\n",
    "    computing the exponential to prevent overflow issues.\n",
    "    \n",
    "    Args:\n",
    "        input (torch.Tensor): Input tensor of shape (N, C, H, W) or (N, C), where N is the batch size,\n",
    "                              C is the number of classes, and H, W are the height and width of the input.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Log softmax of the input tensor, same shape as input.\n",
    "    \"\"\"\n",
    "       \n",
    "    num_classes = input.size()[1] # 입력 텐서의 클래스 수 가져오기.\n",
    "    m = torch.max(input, dim=1, keepdim=True)[0] # 클래스 차원에서 최대값 추출 (수치 안전성 확보) + 차원 유지 -> 결과 (N,1,H,W)\n",
    "    \n",
    "    if input.dim() == 4: # (channel, class, height, width)\n",
    "        d = input - m.repeat(1, num_classes, 1, 1) \n",
    "    else: # (channel, class)\n",
    "        d = input - m.repeat(1, num_classes)\n",
    "    e = torch.exp(d)\n",
    "    s = torch.sum(e, dim=1, keepdim=True) # 지수 값의 합을 구함(class로의 합산)\n",
    "    if input.dim() == 4:\n",
    "        output = d - torch.log(s.repeat(1, num_classes, 1, 1)) # 로그 소프트맥스 계산 \n",
    "    else:\n",
    "        output = d - torch.log(s.repeat(1, num_classes))\n",
    "    return output\n",
    "\n",
    "\n",
    "def softmax_high_dimension(input):\n",
    "    \"\"\"\n",
    "    Compute the softmax over a high-dimensional input tensor for numerical stability.\n",
    "    \n",
    "    This function calculates:\n",
    "    softmax(x) = exp(x_i - m) / Σ_j exp(x_j - m)\n",
    "    \n",
    "    Similar to `log_softmax_high_dimension`, the maximum value `m` is subtracted from `input`\n",
    "    for numerical stability during the exponential calculation.\n",
    "    \n",
    "    Args:\n",
    "        input (torch.Tensor): Input tensor of shape (N, C, H, W) or (N, C), where N is the batch size,\n",
    "                              C is the number of classes, and H, W are the height and width of the input.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Softmax of the input tensor, same shape as input.\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = input.size()[1]\n",
    "    m = torch.max(input, dim=1, keepdim=True)[0]\n",
    "    if input.dim() == 4:\n",
    "        e = torch.exp(input - m.repeat(1, num_classes, 1, 1))\n",
    "    else:\n",
    "        e = torch.exp(input - m.repeat(1, num_classes))\n",
    "    s = torch.sum(e, dim=1, keepdim=True)\n",
    "    if input.dim() == 4:\n",
    "        output = torch.div(e, s.repeat(1, num_classes, 1, 1))\n",
    "    else:\n",
    "        output = torch.div(e, s.repeat(1, num_classes))\n",
    "    return output\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, relu=True):\n",
    "    if relu:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True)\n",
    "\n",
    "\n",
    "def fc(in_planes, out_planes, relu=True):\n",
    "    if relu:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_planes, out_planes),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "    else:\n",
    "        return nn.Linear(in_planes, out_planes)\n",
    "\n",
    "\n",
    "def upsample(scale_factor):\n",
    "    return nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "\n",
    "\n",
    "class PoseCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    PoseCNN network class for 6D pose estimation.\n",
    "    This model is based on a VGG16 backbone and includes branches for semantic labeling, vertex regression,\n",
    "    and pose estimation through RoI pooling and Hough voting.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_units):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes the PoseCNN model with feature extraction and specific branches for pose estimation.\n",
    "        The model adapts a VGG16 backbone and adds custom layers for semantic labeling and vertex regression.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes for classification and pose estimation.\n",
    "            num_units (int): Number of feature embedding units in the intermediate layers.\n",
    "        \"\"\"\n",
    "        super(PoseCNN, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "\n",
    "        # conv features\n",
    "        features = list(vgg16.features)[:30]\n",
    "        \n",
    "        # change the first conv layer for RGBD\n",
    "        if cfg.INPUT == 'RGBD': # RGBD: 4-channels\n",
    "            # Copy weights from the original RGB input to extend to RGBD -> first feature input channel 3 -> 6\n",
    "            conv0 = conv(6, 64, kernel_size=3, relu=False)\n",
    "            conv0.weight.data[:, :3, :, :] = features[0].weight.data # RGB\n",
    "            conv0.weight.data[:, 3:, :, :] = features[0].weight.data # Depth\n",
    "            conv0.bias.data = features[0].bias.data\n",
    "            features[0] = conv0\n",
    "\n",
    "        self.features = nn.ModuleList(features) # store the modified feature extraction layers to allow iteration during forward\n",
    "        self.classifier = vgg16.classifier[:-1] # using vgg16 classifier \n",
    "        if cfg.TRAIN.SLIM:\n",
    "            dim_fc = 256\n",
    "            self.classifier[0] = nn.Linear(512*7*7, 256)\n",
    "            self.classifier[3] = nn.Linear(256, 256)\n",
    "        else:\n",
    "            dim_fc = 4096\n",
    "            \n",
    "        print(self.features)\n",
    "        print(self.classifier)\n",
    "\n",
    "        # freeze some layers\n",
    "        if cfg.TRAIN.FREEZE_LAYERS:\n",
    "            for i in [0, 2, 5, 7, 10, 12, 14]: # for transfer-learning \n",
    "                self.features[i].weight.requires_grad = False\n",
    "                self.features[i].bias.requires_grad = False\n",
    "\n",
    "        # semantic labeling branch\n",
    "        self.conv4_embed = conv(512, num_units, kernel_size=1)\n",
    "        self.conv5_embed = conv(512, num_units, kernel_size=1)\n",
    "        self.upsample_conv5_embed = upsample(2.0) # 2배 해서 conn 4 feature와 합칠려고\n",
    "        self.upsample_embed = upsample(8.0) # 8배 해서 원래 이미지 크기 맞추기 위해\n",
    "        self.conv_score = conv(num_units, num_classes, kernel_size=1) # 각 픽셀의 class 점수 계산 (1x1 convolution)\n",
    "        self.hard_label = HardLabel(threshold=cfg.TRAIN.HARD_LABEL_THRESHOLD, sample_percentage=cfg.TRAIN.HARD_LABEL_SAMPLING) # hard label 가중치 부여 일반화 능력 ↑  \n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        if cfg.TRAIN.VERTEX_REG:\n",
    "            # center regression branch\n",
    "            self.conv4_vertex_embed = conv(512, 2*num_units, kernel_size=1, relu=False)\n",
    "            self.conv5_vertex_embed = conv(512, 2*num_units, kernel_size=1, relu=False)\n",
    "            self.upsample_conv5_vertex_embed = upsample(2.0)\n",
    "            self.upsample_vertex_embed = upsample(8.0)\n",
    "            self.conv_vertex_score = conv(2*num_units, 3*num_classes, kernel_size=1, relu=False) # 3D translation Estimation (x,y,distance)\n",
    "            # hough voting\n",
    "            self.hough_voting = HoughVoting(is_train=0, skip_pixels=10, label_threshold=100, \\\n",
    "                                            inlier_threshold=0.9, voting_threshold=-1, per_threshold=0.01)\n",
    "\n",
    "            self.roi_pool_conv4 = RoIPool(pool_height=7, pool_width=7, spatial_scale=1.0 / 8.0)\n",
    "            self.roi_pool_conv5 = RoIPool(pool_height=7, pool_width=7, spatial_scale=1.0 / 16.0)\n",
    "            self.fc8 = fc(dim_fc, num_classes)\n",
    "            self.fc9 = fc(dim_fc, 4 * num_classes, relu=False) # 쿼터니언 4차원\n",
    "\n",
    "            if cfg.TRAIN.POSE_REG:\n",
    "                self.fc10 = fc(dim_fc, 4 * num_classes, relu=False) \n",
    "                self.pml = PMLoss(hard_angle=cfg.TRAIN.HARD_ANGLE)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x, label_gt, meta_data, extents, gt_boxes, poses, points, symmetry):\n",
    "        \n",
    "        \"\"\"\n",
    "        Defines the forward pass of the PoseCNN network.\n",
    "        This method processes input through the feature extractor, semantic labeling branch, vertex regression branch,\n",
    "        and optionally through the bounding box and rotation regression branches.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor.\n",
    "            label_gt (torch.Tensor): Ground truth labels for semantic segmentation.\n",
    "            meta_data (torch.Tensor): Metadata for each input.\n",
    "            extents (torch.Tensor): 3D extents of the object.\n",
    "            gt_boxes (torch.Tensor): Ground truth bounding boxes.\n",
    "            poses (torch.Tensor): Ground truth object poses.\n",
    "            points (torch.Tensor): Points representing the object's 3D structure.\n",
    "            symmetry (torch.Tensor): Symmetry information for objects.\n",
    "\n",
    "        Returns:\n",
    "            Depending on training/testing mode and enabled branches, returns various outputs including\n",
    "            semantic segmentation logits, vertex predictions, bounding box outputs, and rotation outputs.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract convolutional features from the input using VGG16 layers\n",
    "        for i, model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if i == 22:\n",
    "                out_conv4_3 = x # Store the output after the 4th block (used for semantic labeling and vertex regression)\n",
    "            if i == 29:\n",
    "                out_conv5_3 = x # Store the output after the 5th block (used for semantic labeling and vertex regression)\n",
    "\n",
    "\n",
    "        ##### semantic labeling branch #####\n",
    "        out_conv4_embed = self.conv4_embed(out_conv4_3)\n",
    "        out_conv5_embed = self.conv5_embed(out_conv5_3)\n",
    "        out_conv5_embed_up = self.upsample_conv5_embed(out_conv5_embed)\n",
    "        out_embed = self.dropout(out_conv4_embed + out_conv5_embed_up) # point-wise add\n",
    "        out_embed_up = self.upsample_embed(out_embed) # 원본 이미지 크기로 변경\n",
    "        out_score = self.conv_score(out_embed_up) # 픽셀을 클래스로 분류 \n",
    "        out_logsoftmax = log_softmax_high_dimension(out_score) # log-softmax\n",
    "        out_prob = softmax_high_dimension(out_score) # 각 클래스 확률 \n",
    "        out_label = torch.max(out_prob, dim=1)[1].type(torch.IntTensor).cuda() # out_prob에서 클래스 확률이 가장 높은 인덱스 => 클래스 추출\n",
    "        out_weight = self.hard_label(out_prob, label_gt, torch.rand(out_prob.size()).cuda()) # Hard Labeling 확률 적은 쪽에 가중치 높게 줌\n",
    "\n",
    "        if cfg.TRAIN.VERTEX_REG:\n",
    "            ##### center regression branch #####\n",
    "            # Extract features for vertex prediction from the conv4 and conv5 feature maps\n",
    "            out_conv4_vertex_embed = self.conv4_vertex_embed(out_conv4_3)\n",
    "            out_conv5_vertex_embed = self.conv5_vertex_embed(out_conv5_3)\n",
    "            out_conv5_vertex_embed_up = self.upsample_conv5_vertex_embed(out_conv5_vertex_embed)\n",
    "            out_vertex_embed = self.dropout(out_conv4_vertex_embed + out_conv5_vertex_embed_up)\n",
    "            out_vertex_embed_up = self.upsample_vertex_embed(out_vertex_embed)\n",
    "            out_vertex = self.conv_vertex_score(out_vertex_embed_up)\n",
    "\n",
    "            # hough voting\n",
    "            if self.training:\n",
    "                self.hough_voting.is_train = 1\n",
    "                self.hough_voting.label_threshold = cfg.TRAIN.HOUGH_LABEL_THRESHOLD\n",
    "                self.hough_voting.voting_threshold = cfg.TRAIN.HOUGH_VOTING_THRESHOLD\n",
    "                self.hough_voting.skip_pixels = cfg.TRAIN.HOUGH_SKIP_PIXELS\n",
    "                self.hough_voting.inlier_threshold = cfg.TRAIN.HOUGH_INLIER_THRESHOLD\n",
    "            else:\n",
    "                self.hough_voting.is_train = 0\n",
    "                self.hough_voting.label_threshold = cfg.TEST.HOUGH_LABEL_THRESHOLD\n",
    "                self.hough_voting.voting_threshold = cfg.TEST.HOUGH_VOTING_THRESHOLD\n",
    "                self.hough_voting.skip_pixels = cfg.TEST.HOUGH_SKIP_PIXELS\n",
    "                self.hough_voting.inlier_threshold = cfg.TEST.HOUGH_INLIER_THRESHOLD\n",
    "            out_box, out_pose = self.hough_voting(out_label, out_vertex, meta_data, extents) \n",
    "\n",
    "            ##### bounding box classification and regression branch #####\n",
    "            bbox_labels, bbox_targets, bbox_inside_weights, bbox_outside_weights = roi_target_layer(out_box, gt_boxes)\n",
    "            # Perform RoI pooling on conv4 and conv5 feature maps using the predicted bounding boxes\n",
    "            out_roi_conv4 = self.roi_pool_conv4(out_conv4_3, out_box)\n",
    "            out_roi_conv5 = self.roi_pool_conv5(out_conv5_3, out_box)\n",
    "            # Combine the pooled features from conv4 and conv5\n",
    "            out_roi = out_roi_conv4 + out_roi_conv5\n",
    "            # Flatten the combined RoI features for fully connected layer input\n",
    "            out_roi_flatten = out_roi.view(out_roi.size(0), -1)\n",
    "            # Pass the flattened features through the classifier\n",
    "            out_fc7 = self.classifier(out_roi_flatten)\n",
    "            out_fc8 = self.fc8(out_fc7)\n",
    "            out_logsoftmax_box = log_softmax_high_dimension(out_fc8)\n",
    "            bbox_prob = softmax_high_dimension(out_fc8)\n",
    "            bbox_label_weights = self.hard_label(bbox_prob, bbox_labels, torch.rand(bbox_prob.size()).cuda())\n",
    "            bbox_pred = self.fc9(out_fc7)\n",
    "\n",
    "            ##### rotation regression branch #####\n",
    "            rois, poses_target, poses_weight = pose_target_layer(out_box, bbox_prob, bbox_pred, gt_boxes, poses, self.training)\n",
    "            if cfg.TRAIN.POSE_REG:    \n",
    "                out_qt_conv4 = self.roi_pool_conv4(out_conv4_3, rois)\n",
    "                out_qt_conv5 = self.roi_pool_conv5(out_conv5_3, rois)\n",
    "                out_qt = out_qt_conv4 + out_qt_conv5\n",
    "                out_qt_flatten = out_qt.view(out_qt.size(0), -1)\n",
    "                out_qt_fc7 = self.classifier(out_qt_flatten)\n",
    "                out_quaternion = self.fc10(out_qt_fc7)\n",
    "                # point matching loss\n",
    "                poses_pred = nn.functional.normalize(torch.mul(out_quaternion, poses_weight))\n",
    "                if self.training:\n",
    "                    loss_pose = self.pml(poses_pred, poses_target, poses_weight, points, symmetry)\n",
    "\n",
    "        if self.training:\n",
    "            if cfg.TRAIN.VERTEX_REG:\n",
    "                if cfg.TRAIN.POSE_REG:\n",
    "                    return out_logsoftmax, out_weight, out_vertex, out_logsoftmax_box, bbox_label_weights, \\\n",
    "                           bbox_pred, bbox_targets, bbox_inside_weights, loss_pose, poses_weight\n",
    "                else:\n",
    "                    return out_logsoftmax, out_weight, out_vertex, out_logsoftmax_box, bbox_label_weights, \\\n",
    "                           bbox_pred, bbox_targets, bbox_inside_weights\n",
    "            else:\n",
    "                return out_logsoftmax, out_weight\n",
    "        else:\n",
    "            if cfg.TRAIN.VERTEX_REG:\n",
    "                if cfg.TRAIN.POSE_REG:\n",
    "                    return out_label, out_vertex, rois, out_pose, out_quaternion\n",
    "                else:\n",
    "                    return out_label, out_vertex, rois, out_pose\n",
    "            else:\n",
    "                return out_label\n",
    "\n",
    "    def weight_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
    "\n",
    "    def bias_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'bias' in name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dd9fc",
   "metadata": {},
   "source": [
    "### Hough Voting\n",
    "Hough Voting is used to localize the 2D center of objects in an image. Each pixel votes for the object center using a predicted unit vector direction.\n",
    "\n",
    "### Key Steps:\n",
    "1. **Direction Prediction:** Each pixel regresses to a unit vector pointing towards the object center.\n",
    "2. **Voting Process:** Each pixel casts votes for potential object center locations.\n",
    "3. **Center Selection:** The object center is chosen as the location with the highest accumulated votes.\n",
    "\n",
    "#### Illustration:\n",
    "- A pixel at $(x, y)$ votes for points along the ray towards the predicted object center.\n",
    "- Non-maximum suppression (NMS) is applied to handle multiple object instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hough voting\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def hough_voting_cuda_forward(\n",
    "    bottom_label, bottom_vertex, bottom_meta_data, extents,\n",
    "    is_train, skip_pixels, label_threshold, inlier_threshold,\n",
    "    voting_threshold, per_threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Hough Voting in Python using PyTorch tensors with CUDA support.\n",
    "\n",
    "    Args:\n",
    "        bottom_label: Tensor of shape (batch_size, height, width).\n",
    "        bottom_vertex: Tensor of shape (batch_size, num_classes * 3, height, width).\n",
    "        bottom_meta_data: Tensor containing metadata.\n",
    "        extents: Tensor containing 3D extents of object classes.\n",
    "        is_train: Boolean indicating training mode.\n",
    "        skip_pixels: Step size for pixel sampling.\n",
    "        label_threshold: Minimum number of pixels for a class to be considered.\n",
    "        inlier_threshold: Threshold for inlier votes.\n",
    "        voting_threshold: Minimum number of votes for a valid hypothesis.\n",
    "        per_threshold: Percentage threshold for voting area.\n",
    "\n",
    "    Returns:\n",
    "        top_box_final: Final bounding box tensor.\n",
    "        top_pose_final: Final pose tensor.\n",
    "    \"\"\"\n",
    "    batch_size = bottom_vertex.size(0)\n",
    "    num_classes = bottom_vertex.size(1) // 3  # Vertex channels divided by 3\n",
    "    height, width = bottom_vertex.size(2), bottom_vertex.size(3)\n",
    "    num_meta_data = bottom_meta_data.size(1)\n",
    "\n",
    "    # Constants\n",
    "    max_rois = 128\n",
    "    index_size = max_rois // batch_size\n",
    "\n",
    "    top_box = torch.zeros((max_rois * 9, 7), device='cuda')\n",
    "    top_pose = torch.zeros((max_rois * 9, 7), device='cuda')\n",
    "    num_rois = torch.zeros(1, dtype=torch.int32, device='cuda')\n",
    "\n",
    "    for batch_index in range(batch_size):\n",
    "        labelmap = bottom_label[batch_index].to('cuda')\n",
    "        vertmap = bottom_vertex[batch_index].to('cuda')\n",
    "        meta_data = bottom_meta_data[batch_index].to('cuda')\n",
    "\n",
    "        # Step 1: Compute a label index array for each class\n",
    "        arrays = torch.zeros((num_classes, height * width), dtype=torch.int32, device='cuda')\n",
    "        array_sizes = torch.zeros((num_classes,), dtype=torch.int32, device='cuda')\n",
    "\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                cls = labelmap[y, x]\n",
    "                if cls > 0:\n",
    "                    index = y * width + x\n",
    "                    array_sizes[cls] += 1\n",
    "                    arrays[cls, array_sizes[cls] - 1] = index\n",
    "\n",
    "        # Step 2: Compute valid class indexes\n",
    "        class_indexes = []\n",
    "        for c in range(1, num_classes):\n",
    "            if array_sizes[c] > label_threshold:\n",
    "                class_indexes.append(c)\n",
    "\n",
    "        if not class_indexes:\n",
    "            continue\n",
    "\n",
    "        class_indexes = torch.tensor(class_indexes, dtype=torch.int32, device='cuda')\n",
    "\n",
    "        # Step 3: Compute Hough space and data\n",
    "        hough_space = torch.zeros((len(class_indexes), height, width), device='cuda')\n",
    "        hough_data = torch.zeros((len(class_indexes), height, width, 3), device='cuda')\n",
    "\n",
    "        for i, cls in enumerate(class_indexes):\n",
    "            for y in range(height):\n",
    "                for x in range(width):\n",
    "                    for j in range(0, array_sizes[cls], skip_pixels):\n",
    "                        location = arrays[cls, j]\n",
    "                        px, py = location % width, location // width\n",
    "\n",
    "                        # Read direction and distance\n",
    "                        u = vertmap[cls * 3, py, px]\n",
    "                        v = vertmap[cls * 3 + 1, py, px]\n",
    "                        d = torch.exp(vertmap[cls * 3 + 2, py, px])\n",
    "\n",
    "                        # Voting\n",
    "                        dx, dy = x - px, y - py\n",
    "                        norm1 = torch.sqrt(u ** 2 + v ** 2)\n",
    "                        norm2 = torch.sqrt(dx ** 2 + dy ** 2)\n",
    "                        dot = u * dx + v * dy\n",
    "                        angle_dist = dot / (norm1 * norm2)\n",
    "\n",
    "                        if angle_dist > inlier_threshold:\n",
    "                            hough_space[i, y, x] += 1\n",
    "                            hough_data[i, y, x, 0] += d\n",
    "\n",
    "        # Normalize distances\n",
    "        non_zero_votes = hough_space > 0\n",
    "        hough_data[..., 0][non_zero_votes] /= hough_space[non_zero_votes]\n",
    "\n",
    "        # Step 4: Find maxima in Hough space\n",
    "        maxima = (hough_space > voting_threshold).nonzero(as_tuple=False)\n",
    "\n",
    "        for m in maxima:\n",
    "            cls_idx, cy, cx = m\n",
    "            cls = class_indexes[cls_idx]\n",
    "\n",
    "            bb_distance = hough_data[cls_idx, cy, cx, 0]\n",
    "            bb_width = hough_data[cls_idx, cy, cx, 2]\n",
    "            bb_height = hough_data[cls_idx, cy, cx, 1]\n",
    "\n",
    "            # Add box\n",
    "            roi_index = num_rois.item()\n",
    "            top_box[roi_index, :] = torch.tensor([\n",
    "                batch_index, cls, cx - bb_width / 2, cy - bb_height / 2,\n",
    "                cx + bb_width / 2, cy + bb_height / 2, hough_space[cls_idx, cy, cx]\n",
    "            ], device='cuda')\n",
    "\n",
    "            num_rois += 1\n",
    "\n",
    "    # Prepare final outputs\n",
    "    num_rois = num_rois.item()\n",
    "    if num_rois == 0:\n",
    "        num_rois = 1\n",
    "\n",
    "    top_box_final = top_box[:num_rois]\n",
    "    top_pose_final = top_pose[:num_rois]\n",
    "\n",
    "    return top_box_final, top_pose_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ccad6",
   "metadata": {},
   "source": [
    "## 6. Refinement with ICP\n",
    "PoseCNN employs **Iterative Closest Point (ICP)** to refine the initial 6D pose predictions using depth data. This process significantly improves accuracy, particularly for challenging cases with occlusions or symmetric objects.\n",
    "\n",
    "### Key Steps in Refinement:\n",
    "1. **Initial Pose Prediction:** The network predicts an initial 6D pose (translation and rotation).\n",
    "2. **ICP Refinement:**\n",
    "   - Matches observed depth points to the rendered 3D model points.\n",
    "   - Minimizes the point-to-plane residual between the observed and predicted depth points.\n",
    "3. **Final Pose Selection:**\n",
    "   - Multiple refined poses are evaluated.\n",
    "   - The pose with the best alignment metric is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de44be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refinement (ycb_object.py)\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PoseRefiner:\n",
    "    def __init__(self, extents, intrinsic_matrix):\n",
    "        self._extents = extents\n",
    "        self._intrinsic_matrix = intrinsic_matrix\n",
    "\n",
    "    def refine_pose_with_depth(self, im_label, im_depth, cls_indexes, center, poses, classes, num_classes):\n",
    "        \"\"\"\n",
    "        Refines pose using depth information by computing vertex deltas.\n",
    "\n",
    "        Args:\n",
    "            im_label: 2D label image (height x width).\n",
    "            im_depth: Depth image with shape (height x width x 3).\n",
    "            cls_indexes: List of object class indices.\n",
    "            center: Center positions of objects.\n",
    "            poses: List or array of object poses.\n",
    "            classes: List of all classes.\n",
    "            num_classes: Number of classes.\n",
    "\n",
    "        Returns:\n",
    "            vertex_targets: Vertex deltas for regression.\n",
    "            vertex_weights: Weights for regression loss.\n",
    "        \"\"\"\n",
    "        # Extract X, Y, Z depth channels\n",
    "        x_image = im_depth[:, :, 0]\n",
    "        y_image = im_depth[:, :, 1]\n",
    "        z_image = im_depth[:, :, 2]\n",
    "\n",
    "        # Image dimensions\n",
    "        height, width = im_label.shape\n",
    "\n",
    "        # Initialize outputs for vertex deltas and weights\n",
    "        vertex_targets = np.zeros((3 * num_classes, height, width), dtype=np.float32)\n",
    "        vertex_weights = np.zeros((3 * num_classes, height, width), dtype=np.float32)\n",
    "\n",
    "        # Iterate through each class to calculate vertex deltas\n",
    "        for i in range(1, num_classes):\n",
    "            # Mask for valid depth and corresponding class label\n",
    "            valid_mask = (z_image != 0.0)  # Ensure depth is valid\n",
    "            label_mask = (im_label == classes[i])  # Match specific class label\n",
    "            combined_mask = valid_mask & label_mask\n",
    "\n",
    "            # Get pixel indices where both masks are true\n",
    "            y, x = np.where(combined_mask)\n",
    "\n",
    "            # Check if valid pixels exist for the current class\n",
    "            ind = np.where(cls_indexes == classes[i])[0]\n",
    "            if len(x) > 0 and len(ind) > 0:\n",
    "\n",
    "                # Get object extent and compute half diameter for normalization\n",
    "                extents_here = self._extents[i, :]\n",
    "                largest_dim = np.sqrt(np.sum(extents_here**2))\n",
    "                half_diameter = largest_dim / 2.0\n",
    "\n",
    "                # Retrieve object center coordinates from poses or center array\n",
    "                c_x, c_y = center[ind, 0], center[ind, 1]\n",
    "                if isinstance(poses, list):\n",
    "                    x_center_coord = poses[int(ind)][0]\n",
    "                    y_center_coord = poses[int(ind)][1]\n",
    "                    z_center_coord = poses[int(ind)][2]\n",
    "                else:\n",
    "                    x_center_coord = poses[ind, -3]\n",
    "                    y_center_coord = poses[ind, -2]\n",
    "                    z_center_coord = poses[ind, -1]\n",
    "\n",
    "                # Compute vertex deltas normalized by object size\n",
    "                targets_x = (x_image[y, x] - x_center_coord) / half_diameter\n",
    "                targets_y = (y_image[y, x] - y_center_coord) / half_diameter\n",
    "                targets_z = (z_image[y, x] - z_center_coord) / half_diameter\n",
    "\n",
    "                # Assign vertex deltas to the target array\n",
    "                vertex_targets[3 * i + 0, y, x] = targets_x\n",
    "                vertex_targets[3 * i + 1, y, x] = targets_y\n",
    "                vertex_targets[3 * i + 2, y, x] = targets_z\n",
    "\n",
    "                # Assign uniform weights for valid regions\n",
    "                vertex_weights[3 * i + 0, y, x] = 1.0  # Weight for X\n",
    "                vertex_weights[3 * i + 1, y, x] = 1.0  # Weight for Y\n",
    "                vertex_weights[3 * i + 2, y, x] = 1.0  # Weight for Z\n",
    "\n",
    "        # Return the computed vertex targets and weights\n",
    "        return vertex_targets, vertex_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407eecc",
   "metadata": {},
   "source": [
    "## 7. Results and Visualization\n",
    "### Quantitative Results\n",
    "- PoseCNN achieves **state-of-the-art performance** on YCB-Video and OccludedLINEMOD datasets.\n",
    "\n",
    "### Visualization\n",
    "Below is an example of semantic labeling and pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2854a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def _vis_test(inputs, labels, out_label, out_vertex, rois, poses, poses_refined, sample, points, classes, class_colors):\n",
    "    \"\"\"\n",
    "    Visualize a mini-batch for debugging purposes.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Input image tensor of shape (N, C, H, W).\n",
    "        labels (torch.Tensor): Ground truth labels tensor of shape (N, C, H, W).\n",
    "        out_label (torch.Tensor): Predicted labels tensor of shape (N, H, W).\n",
    "        out_vertex (torch.Tensor): Predicted vertex regression tensor.\n",
    "        rois (torch.Tensor): Region of interest tensor.\n",
    "        poses (torch.Tensor): Predicted poses tensor.\n",
    "        poses_refined (torch.Tensor): Refined predicted poses tensor.\n",
    "        sample (dict): Dictionary containing ground truth poses, metadata, and other information.\n",
    "        points (numpy.ndarray): 3D points for object models.\n",
    "        classes (list): List of class names.\n",
    "        class_colors (list): List of RGB colors for each class.\n",
    "\n",
    "    Visualizes the following:\n",
    "        - Input image.\n",
    "        - Ground truth and predicted labels.\n",
    "        - Predicted bounding boxes and poses.\n",
    "        - Refined poses if enabled.\n",
    "        - Ground truth and predicted vertex targets.\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    im_blob = inputs.cpu().numpy()\n",
    "    label_blob = labels.cpu().numpy()\n",
    "    label_pred = out_label.cpu().numpy()\n",
    "    gt_poses = sample['poses'].numpy()\n",
    "    meta_data_blob = sample['meta_data'].numpy()\n",
    "    metadata = meta_data_blob[0, :]\n",
    "    intrinsic_matrix = metadata[:9].reshape((3, 3))\n",
    "    gt_boxes = sample['gt_boxes'].numpy()\n",
    "    extents = sample['extents'][0, :, :].numpy()\n",
    "\n",
    "    if cfg.TRAIN.VERTEX_REG or cfg.TRAIN.VERTEX_REG_DELTA:\n",
    "        vertex_targets = sample['vertex_targets'].numpy()\n",
    "        vertex_pred = out_vertex.detach().cpu().numpy()\n",
    "\n",
    "    m = 4\n",
    "    n = 4\n",
    "    for i in range(im_blob.shape[0]):\n",
    "        fig = plt.figure()\n",
    "        start = 1\n",
    "\n",
    "        # show image\n",
    "        im = im_blob[i, :, :, :].copy()\n",
    "        im = im.transpose((1, 2, 0)) * 255.0\n",
    "        im += cfg.PIXEL_MEANS\n",
    "        im = im[:, :, (2, 1, 0)]\n",
    "        im = im.astype(np.uint8)\n",
    "        ax = fig.add_subplot(m, n, 1)\n",
    "        plt.imshow(im)\n",
    "        ax.set_title('color')\n",
    "        start += 1\n",
    "\n",
    "        # show gt boxes\n",
    "        boxes = gt_boxes[i]\n",
    "        for j in range(boxes.shape[0]):\n",
    "            if boxes[j, 4] == 0:\n",
    "                continue\n",
    "            x1 = boxes[j, 0]\n",
    "            y1 = boxes[j, 1]\n",
    "            x2 = boxes[j, 2]\n",
    "            y2 = boxes[j, 3]\n",
    "            plt.gca().add_patch(\n",
    "                plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='g', linewidth=3))\n",
    "\n",
    "        # show gt label\n",
    "        label_gt = label_blob[i, :, :, :]\n",
    "        label_gt = label_gt.transpose((1, 2, 0))\n",
    "        height = label_gt.shape[0]\n",
    "        width = label_gt.shape[1]\n",
    "        num_classes = label_gt.shape[2]\n",
    "        im_label_gt = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        for j in range(num_classes):\n",
    "            I = np.where(label_gt[:, :, j] > 0)\n",
    "            im_label_gt[I[0], I[1], :] = class_colors[j]\n",
    "\n",
    "        ax = fig.add_subplot(m, n, start)\n",
    "        start += 1\n",
    "        plt.imshow(im_label_gt)\n",
    "        ax.set_title('gt labels')\n",
    "\n",
    "        # show predicted label\n",
    "        label = label_pred[i, :, :]\n",
    "        height = label.shape[0]\n",
    "        width = label.shape[1]\n",
    "        im_label = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        for j in range(num_classes):\n",
    "            I = np.where(label == j)\n",
    "            im_label[I[0], I[1], :] = class_colors[j]\n",
    "\n",
    "        ax = fig.add_subplot(m, n, start)\n",
    "        start += 1\n",
    "        plt.imshow(im_label)\n",
    "        ax.set_title('predicted labels')\n",
    "\n",
    "        if cfg.TRAIN.VERTEX_REG or cfg.TRAIN.VERTEX_REG_DELTA:\n",
    "\n",
    "            # show predicted boxes\n",
    "            ax = fig.add_subplot(m, n, start)\n",
    "            start += 1\n",
    "            plt.imshow(im)\n",
    "\n",
    "            ax.set_title('predicted boxes')\n",
    "            for j in range(rois.shape[0]):\n",
    "                if rois[j, 0] != i or rois[j, -1] < cfg.TEST.DET_THRESHOLD:\n",
    "                    continue\n",
    "                cls = rois[j, 1]\n",
    "                x1 = rois[j, 2]\n",
    "                y1 = rois[j, 3]\n",
    "                x2 = rois[j, 4]\n",
    "                y2 = rois[j, 5]\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=np.array(class_colors[int(cls)]) / 255.0, linewidth=3))\n",
    "\n",
    "                cx = (x1 + x2) / 2\n",
    "                cy = (y1 + y2) / 2\n",
    "                plt.plot(cx, cy, 'yo')\n",
    "\n",
    "            # Additional visualizations for gt poses, predicted poses, refined poses, and vertex targets omitted for brevity.\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f6aff",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Limitations\n",
    "PoseCNN is a robust framework for 6D pose estimation in challenging conditions such as occlusions and symmetry. Its contributions include:\n",
    "- Introduction of ShapeMatch Loss for symmetric objects.\n",
    "- Development of the YCB-Video dataset for pose estimation tasks.\n",
    "- State-of-the-art results on benchmark datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456ca07",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46eb4a7",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5489a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
