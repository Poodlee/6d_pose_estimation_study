{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseCNN-Pytorch 코드 정리 본\n",
    "\n",
    "### Running the demo\n",
    "1. Download 3D models and our pre-trained checkpoints first.\n",
    "\n",
    "2. run the following script\n",
    "- ./experiments/scripts/demo.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_images.py 옵션 정리\n",
    "__--gpu : 사용할 gpu ID 선택 0__\n",
    "\n",
    "\\-\n",
    "\n",
    "__--imgdir : 테스트 이미지 저장 경로 설정__\n",
    "\\-\n",
    "\n",
    "__--meta : meta 데이터 파일 불러오기__\n",
    "\n",
    "\\-  INTRINSICS: [618.0172729492188, 0.0, 312.376953125, 0.0, 618.0033569335938, 232.37530517578125,0.0, 0.0, 1.0]\n",
    "\n",
    "__--color : imgdir 내에 있는 객체들이 있는 이미지({number}-color.png) 경로 설정__\n",
    "\n",
    "\\-\n",
    "\n",
    "__--network : 사용할 CNN 네트워크 선택 (PoseCNN-PyTorch/lib/networks/PoseCNN.py)__\n",
    "\n",
    "\\-  \\_\\_all__' = ['posecnn',]\n",
    "\n",
    "\n",
    "__--pretrained : 사전 훈련된 Encoder Checkpoint 경로 설정__\n",
    "\n",
    "\\-\n",
    "  \n",
    "__--dataset : Train 데이터셋 경로 설정__\n",
    "\n",
    "\n",
    "\\-\n",
    "\n",
    "__--cfg : Config 파일 경로 설정__\n",
    "\n",
    "\\-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parse input arguments\n",
    "    \"\"\"\n",
    "    # demo.sh 에서 입력한 값 불러오기\n",
    "    parser = argparse.ArgumentParser(description='Test a PoseCNN network')\n",
    "    parser.add_argument('--gpu', dest='gpu_id', help='GPU id to use',\n",
    "                        default=0, type=int)\n",
    "    parser.add_argument('--pretrained', dest='pretrained',\n",
    "                        help='initialize with pretrained checkpoint',\n",
    "                        default=None, type=str)\n",
    "    parser.add_argument('--pretrained_encoder', dest='pretrained_encoder',\n",
    "                        help='initialize with pretrained encoder checkpoint',\n",
    "                        default=None, type=str)\n",
    "    parser.add_argument('--codebook', dest='codebook',\n",
    "                        help='codebook',\n",
    "                        default=None, type=str)\n",
    "    parser.add_argument('--cfg', dest='cfg_file',\n",
    "                        help='optional config file', default=None, type=str)\n",
    "    parser.add_argument('--meta', dest='meta_file',\n",
    "                        help='optional metadata file', default=None, type=str)\n",
    "    parser.add_argument('--dataset', dest='dataset_name',\n",
    "                        help='dataset to train on',\n",
    "                        default='shapenet_scene_train', type=str)\n",
    "    parser.add_argument('--depth', dest='depth_name',\n",
    "                        help='depth image pattern',\n",
    "                        default='*depth.png', type=str)\n",
    "    parser.add_argument('--color', dest='color_name',\n",
    "                        help='color image pattern',\n",
    "                        default='*color.png', type=str)\n",
    "    parser.add_argument('--imgdir', dest='imgdir',\n",
    "                        help='path of the directory with the test images',\n",
    "                        default='data/Images', type=str)\n",
    "    parser.add_argument('--rand', dest='randomize',\n",
    "                        help='randomize (do not use a fixed seed)',\n",
    "                        action='store_true')\n",
    "    parser.add_argument('--network', dest='network_name',\n",
    "                        help='name of the network',\n",
    "                        default=None, type=str)\n",
    "    parser.add_argument('--background', dest='background_name',\n",
    "                        help='name of the background file',\n",
    "                        default=None, type=str)\n",
    "    \n",
    "    if len(sys.argv) == 1:\n",
    "        parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoseCNN.py 정리\n",
    "\n",
    "![image.png](https://ar5iv.labs.arxiv.org/html/1711.00199/assets/x2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "# 라이브러리 불러오기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "from torch.nn.init import kaiming_normal_\n",
    "\n",
    "from layers.hard_label import HardLabel\n",
    "from layers.hough_voting import HoughVoting\n",
    "from layers.roi_pooling import RoIPool\n",
    "from layers.point_matching_loss import PMLoss\n",
    "from layers.roi_target_layer import roi_target_layer\n",
    "from layers.pose_target_layer import pose_target_layer\n",
    "from fcn.config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네트워크 종류\n",
    "__all__ = [\n",
    "    'posecnn',\n",
    "]\n",
    "\n",
    "# VGG16 모델 초기화\n",
    "vgg16 = models.vgg16(pretrained=False)\n",
    "\n",
    "# 고차원 Log-Softmax 함수 구현\n",
    "def log_softmax_high_dimension(input):\n",
    "    num_classes = input.size()[1]\n",
    "    m = torch.max(input, dim=1, keepdim=True)[0]\n",
    "    if input.dim() == 4:\n",
    "        d = input - m.repeat(1, num_classes, 1, 1)\n",
    "    else:\n",
    "        d = input - m.repeat(1, num_classes)\n",
    "    e = torch.exp(d)\n",
    "    s = torch.sum(e, dim=1, keepdim=True)\n",
    "    if input.dim() == 4:\n",
    "        output = d - torch.log(s.repeat(1, num_classes, 1, 1))\n",
    "    else:\n",
    "        output = d - torch.log(s.repeat(1, num_classes))\n",
    "    return output\n",
    "\n",
    "# 고차원 Softmax 함수 구현\n",
    "def softmax_high_dimension(input):\n",
    "    num_classes = input.size()[1]\n",
    "    m = torch.max(input, dim=1, keepdim=True)[0]\n",
    "    if input.dim() == 4:\n",
    "        e = torch.exp(input - m.repeat(1, num_classes, 1, 1))\n",
    "    else:\n",
    "        e = torch.exp(input - m.repeat(1, num_classes))\n",
    "    s = torch.sum(e, dim=1, keepdim=True)\n",
    "    if input.dim() == 4:\n",
    "        output = torch.div(e, s.repeat(1, num_classes, 1, 1))\n",
    "    else:\n",
    "        output = torch.div(e, s.repeat(1, num_classes))\n",
    "    return output\n",
    "\n",
    "\n",
    "# Convolutional Layer 함수 구현\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, relu=True):\n",
    "    if relu:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True)\n",
    "\n",
    "# Fully Connected Layer 함수 구현\n",
    "def fc(in_planes, out_planes, relu=True):\n",
    "    if relu:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_planes, out_planes),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "    else:\n",
    "        return nn.Linear(in_planes, out_planes)\n",
    "\n",
    "# Upsampling 함수 구현\n",
    "def upsample(scale_factor):\n",
    "    return nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoseCNN 클래스 구성\n",
    "class PoseCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, num_units):\n",
    "        super(PoseCNN, self).__init__()\n",
    "        self.num_classes = num_classes # 분류할 클래스 수 지정\n",
    "\n",
    "        # conv features\n",
    "        features = list(vgg16.features)[:30] # VGG16 모델의 30개 Conv Features 지정\n",
    "        \n",
    "        # 첫 번째 RGBD Conv Layer 수정\n",
    "        if cfg.INPUT == 'RGBD': \n",
    "            conv0 = conv(6, 64, kernel_size=3, relu=False) # 새로운 Conv Layer 생성\n",
    "            conv0.weight.data[:, :3, :, :] = features[0].weight.data # RGB Channel 가중치\n",
    "            conv0.weight.data[:, 3:, :, :] = features[0].weight.data # Depth Channel 가중치\n",
    "            conv0.bias.data = features[0].bias.data # Bias\n",
    "            features[0] = conv0 # 수정된 Layer 적용\n",
    "\n",
    "        self.features = nn.ModuleList(features) # ModuleList로 Features Layer 저장\n",
    "        self.classifier = vgg16.classifier[:-1] # VGG16 분류기 Last Layer 제거\n",
    "        if cfg.TRAIN.SLIM: # Config 파일 내 Slim 아키텍쳐 사용시 아래 차원 설정값 적용\n",
    "            dim_fc = 256\n",
    "            self.classifier[0] = nn.Linear(512*7*7, 256)\n",
    "            self.classifier[3] = nn.Linear(256, 256)\n",
    "        else:\n",
    "            dim_fc = 4096 # 기본 분류기 차원 설정 \n",
    "            \n",
    "        print(self.features)\n",
    "        print(self.classifier)\n",
    "\n",
    "        # 특정 Layer 고정\n",
    "        if cfg.TRAIN.FREEZE_LAYERS:\n",
    "            for i in [0, 2, 5, 7, 10, 12, 14]:\n",
    "                self.features[i].weight.requires_grad = False # 가중치 업데이트 X\n",
    "                self.features[i].bias.requires_grad = False # Bias 업데이트 X\n",
    "\n",
    "        # semantic labeling branch 설정\n",
    "        self.conv4_embed = conv(512, num_units, kernel_size=1) # conv4 Embedding Layer 설정\n",
    "        self.conv5_embed = conv(512, num_units, kernel_size=1) # conv5 Embedding Layer 설정\n",
    "        self.upsample_conv5_embed = upsample(2.0) # conv5 Embedding Upsampling 설정\n",
    "        self.upsample_embed = upsample(8.0) # 최종 Embedding Upsampling 설정\n",
    "        self.conv_score = conv(num_units, num_classes, kernel_size=1) # Class Score 계산 Layer 설정\n",
    "        self.hard_label = HardLabel(threshold=cfg.TRAIN.HARD_LABEL_THRESHOLD, sample_percentage=cfg.TRAIN.HARD_LABEL_SAMPLING) # Hard Labeling 설정\n",
    "        self.dropout = nn.Dropout() # Dropout Layer\n",
    "\n",
    "        # Vertex Regression 활성화 시 아래 값 설정\n",
    "        if cfg.TRAIN.VERTEX_REG:\n",
    "            # center regression branch 설정\n",
    "            self.conv4_vertex_embed = conv(512, 2*num_units, kernel_size=1, relu=False)\n",
    "            self.conv5_vertex_embed = conv(512, 2*num_units, kernel_size=1, relu=False)\n",
    "            self.upsample_conv5_vertex_embed = upsample(2.0)\n",
    "            self.upsample_vertex_embed = upsample(8.0)\n",
    "            self.conv_vertex_score = conv(2*num_units, 3*num_classes, kernel_size=1, relu=False)\n",
    "            \n",
    "            \n",
    "            # hough voting 설정\n",
    "            self.hough_voting = HoughVoting(is_train=0, skip_pixels=10, label_threshold=100, \\\n",
    "                                            inlier_threshold=0.9, voting_threshold=-1, per_threshold=0.01)\n",
    "\n",
    "            self.roi_pool_conv4 = RoIPool(pool_height=7, pool_width=7, spatial_scale=1.0 / 8.0) # conv4 RoI Pooling Layer 설정\n",
    "            self.roi_pool_conv5 = RoIPool(pool_height=7, pool_width=7, spatial_scale=1.0 / 16.0) # conv5 RoI Pooling Layer 설정\n",
    "            self.fc8 = fc(dim_fc, num_classes) # Bounding Box 분류 Layer 설정\n",
    "            self.fc9 = fc(dim_fc, 4 * num_classes, relu=False) # Bounding Box Regrssion Layer 설정\n",
    "\n",
    "            # Pose Regression 활성화 여부에 따른 값 설정\n",
    "            if cfg.TRAIN.POSE_REG: \n",
    "                self.fc10 = fc(dim_fc, 4 * num_classes, relu=False)\n",
    "                self.pml = PMLoss(hard_angle=cfg.TRAIN.HARD_ANGLE) # Point Matching Loss 설정\n",
    "\n",
    "        # 가중치 초기화\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                kaiming_normal_(m.weight.data) # Kaiming 초기화\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_() # Bias 초기화\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1) # Batch Nomalization Weight 초기화\n",
    "                m.bias.data.zero_() # Batch Nomalization Bias 초기화\n",
    "\n",
    "\n",
    "    def forward(self, x, label_gt, meta_data, extents, gt_boxes, poses, points, symmetry):\n",
    "\n",
    "        # conv features\n",
    "        for i, model in enumerate(self.features):\n",
    "            x = model(x) # 각 Layer에 Model 적용\n",
    "            if i == 22:\n",
    "                out_conv4_3 = x \n",
    "            if i == 29:\n",
    "                out_conv5_3 = x\n",
    "\n",
    "        # semantic labeling branch\n",
    "        out_conv4_embed = self.conv4_embed(out_conv4_3) # conv4 Embedding\n",
    "        out_conv5_embed = self.conv5_embed(out_conv5_3) # conv5 Embedding\n",
    "        out_conv5_embed_up = self.upsample_conv5_embed(out_conv5_embed) # conv5 Embedding Upsampling\n",
    "        out_embed = self.dropout(out_conv4_embed + out_conv5_embed_up) # Embed Dropout\n",
    "        out_embed_up = self.upsample_embed(out_embed) # 최종 Embedding Upsampling\n",
    "        out_score = self.conv_score(out_embed_up) # Class Score 계산\n",
    "        out_logsoftmax = log_softmax_high_dimension(out_score) # LogSoftmax 적용\n",
    "        out_prob = softmax_high_dimension(out_score) # Softmax 적용\n",
    "        out_label = torch.max(out_prob, dim=1)[1].type(torch.IntTensor).cuda() # Class Label 예측\n",
    "        out_weight = self.hard_label(out_prob, label_gt, torch.rand(out_prob.size()).cuda()) # Hard Label Weight 적용\n",
    "\n",
    "        \n",
    "        # Vertex Regression 활성 여부에 따른 값 적용\n",
    "        if cfg.TRAIN.VERTEX_REG:\n",
    "            # center regression branch\n",
    "            out_conv4_vertex_embed = self.conv4_vertex_embed(out_conv4_3)\n",
    "            out_conv5_vertex_embed = self.conv5_vertex_embed(out_conv5_3)\n",
    "            out_conv5_vertex_embed_up = self.upsample_conv5_vertex_embed(out_conv5_vertex_embed)\n",
    "            out_vertex_embed = self.dropout(out_conv4_vertex_embed + out_conv5_vertex_embed_up)\n",
    "            out_vertex_embed_up = self.upsample_vertex_embed(out_vertex_embed)\n",
    "            out_vertex = self.conv_vertex_score(out_vertex_embed_up)\n",
    "\n",
    "            # hough voting\n",
    "            if self.training: # 훈련모드\n",
    "                self.hough_voting.is_train = 1\n",
    "                self.hough_voting.label_threshold = cfg.TRAIN.HOUGH_LABEL_THRESHOLD\n",
    "                self.hough_voting.voting_threshold = cfg.TRAIN.HOUGH_VOTING_THRESHOLD\n",
    "                self.hough_voting.skip_pixels = cfg.TRAIN.HOUGH_SKIP_PIXELS\n",
    "                self.hough_voting.inlier_threshold = cfg.TRAIN.HOUGH_INLIER_THRESHOLD\n",
    "            else: # 평가모드\n",
    "                self.hough_voting.is_train = 0\n",
    "                self.hough_voting.label_threshold = cfg.TEST.HOUGH_LABEL_THRESHOLD\n",
    "                self.hough_voting.voting_threshold = cfg.TEST.HOUGH_VOTING_THRESHOLD\n",
    "                self.hough_voting.skip_pixels = cfg.TEST.HOUGH_SKIP_PIXELS\n",
    "                self.hough_voting.inlier_threshold = cfg.TEST.HOUGH_INLIER_THRESHOLD\n",
    "            out_box, out_pose = self.hough_voting(out_label, out_vertex, meta_data, extents)\n",
    "\n",
    "            # bounding box classification and regression branch\n",
    "            bbox_labels, bbox_targets, bbox_inside_weights, bbox_outside_weights = roi_target_layer(out_box, gt_boxes) # RoI Target에 따른 가중치 불러오기\n",
    "            out_roi_conv4 = self.roi_pool_conv4(out_conv4_3, out_box) # Conv4 RoI Pooling\n",
    "            out_roi_conv5 = self.roi_pool_conv5(out_conv5_3, out_box) # Conv5 RoI Pooling\n",
    "            out_roi = out_roi_conv4 + out_roi_conv5 # 위 두 Pooling 합\n",
    "            \n",
    "            out_roi_flatten = out_roi.view(out_roi.size(0), -1) # RoI 결과값 평탄화\n",
    "            out_fc7 = self.classifier(out_roi_flatten) # 분류기 통과\n",
    "            out_fc8 = self.fc8(out_fc7) # FC Layer 통과\n",
    "            out_logsoftmax_box = log_softmax_high_dimension(out_fc8) # Log Softmax 적용\n",
    "            bbox_prob = softmax_high_dimension(out_fc8) # Softmax 적용하여 확률 계산\n",
    "            bbox_label_weights = self.hard_label(bbox_prob, bbox_labels, torch.rand(bbox_prob.size()).cuda()) # Hard Label 가중치 계산\n",
    "            bbox_pred = self.fc9(out_fc7) # Bounding Box 예측\n",
    "\n",
    "            # rotation regression branch\n",
    "            rois, poses_target, poses_weight = pose_target_layer(out_box, bbox_prob, bbox_pred, gt_boxes, poses, self.training)\n",
    "            \n",
    "            # Pose Regression 활성 여부에 따른 값 설정\n",
    "            if cfg.TRAIN.POSE_REG:    \n",
    "                out_qt_conv4 = self.roi_pool_conv4(out_conv4_3, rois)\n",
    "                out_qt_conv5 = self.roi_pool_conv5(out_conv5_3, rois)\n",
    "                out_qt = out_qt_conv4 + out_qt_conv5\n",
    "                out_qt_flatten = out_qt.view(out_qt.size(0), -1)\n",
    "                out_qt_fc7 = self.classifier(out_qt_flatten)\n",
    "                out_quaternion = self.fc10(out_qt_fc7)\n",
    "                # point matching loss\n",
    "                poses_pred = nn.functional.normalize(torch.mul(out_quaternion, poses_weight))\n",
    "                if self.training:\n",
    "                    loss_pose = self.pml(poses_pred, poses_target, poses_weight, points, symmetry)\n",
    "        \n",
    "        # Training Mode\n",
    "        if self.training:\n",
    "            if cfg.TRAIN.VERTEX_REG:\n",
    "                if cfg.TRAIN.POSE_REG:\n",
    "                    return out_logsoftmax, out_weight, out_vertex, out_logsoftmax_box, bbox_label_weights, \\\n",
    "                           bbox_pred, bbox_targets, bbox_inside_weights, loss_pose, poses_weight\n",
    "                else:\n",
    "                    return out_logsoftmax, out_weight, out_vertex, out_logsoftmax_box, bbox_label_weights, \\\n",
    "                           bbox_pred, bbox_targets, bbox_inside_weights\n",
    "            else:\n",
    "                return out_logsoftmax, out_weight\n",
    "            \n",
    "        # Validating Mode\n",
    "        else:\n",
    "            if cfg.TRAIN.VERTEX_REG:\n",
    "                if cfg.TRAIN.POSE_REG:\n",
    "                    return out_label, out_vertex, rois, out_pose, out_quaternion\n",
    "                else:\n",
    "                    return out_label, out_vertex, rois, out_pose\n",
    "            else:\n",
    "                return out_label\n",
    "\n",
    "    # Weight Parameters 반환\n",
    "    def weight_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
    "\n",
    "    # Bias Parameters 반환\n",
    "    def bias_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'bias' in name]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posecnn(num_classes, num_units, data=None):\n",
    "\n",
    "    # PoseCNN 모델 생성\n",
    "    model = PoseCNN(num_classes, num_units)\n",
    "\n",
    "    # 데이터 제공 여부 확인\n",
    "    if data is not None:\n",
    "        model_dict = model.state_dict() # Model 상태 dict 가져오기\n",
    "        print('model keys')\n",
    "        print('=================================================')\n",
    "        for k, v in model_dict.items():\n",
    "            print(k)\n",
    "        print('=================================================')\n",
    "\n",
    "        print('data keys')\n",
    "        print('=================================================')\n",
    "        \n",
    "        # Model Key 값 출력\n",
    "        for k, v in data.items():\n",
    "            print(k)\n",
    "        print('=================================================')\n",
    "\n",
    "        # 데이터 내 모델 상태와 일치하는 Key-Value를 찾음\n",
    "        pretrained_dict = {k: v for k, v in data.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
    "        print('load the following keys from the pretrained model')\n",
    "        print('=================================================')\n",
    "        \n",
    "        # 불러올 Key 출력\n",
    "        for k, v in pretrained_dict.items():\n",
    "            print(k)\n",
    "        print('=================================================')\n",
    "        \n",
    "        # 모델 상태 Update\n",
    "        model_dict.update(pretrained_dict) \n",
    "        \n",
    "        # 사전 훈련 가충치 불러오기\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "    return model # 생성된 모델 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoseCNN Import Module\n",
    "\n",
    "from layers.hard_label import HardLabel\n",
    "\n",
    "from layers.hough_voting import HoughVoting\n",
    "\n",
    "from layers.roi_pooling import RoIPool\n",
    "\n",
    "from layers.point_matching_loss import PMLoss\n",
    "\n",
    "from layers.roi_target_layer import roi_target_layer\n",
    "\n",
    "from layers.pose_target_layer import pose_target_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from layers.hard_label import HardLabel'''\n",
    "\n",
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch\n",
    "import posecnn_cuda\n",
    "\n",
    "\n",
    "class HardLabelFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prob, label, rand, threshold, sample_percentage):\n",
    "        outputs = posecnn_cuda.hard_label_forward(threshold, sample_percentage, prob, label, rand)\n",
    "        top_data = outputs[0]\n",
    "        return top_data\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, top_diff):\n",
    "        outputs = posecnn_cuda.hard_label_backward(top_diff)\n",
    "        d_prob, d_label = outputs\n",
    "        return d_prob, d_label, None, None, None\n",
    "\n",
    "\n",
    "class HardLabel(nn.Module):\n",
    "    def __init__(self, threshold, sample_percentage):\n",
    "        super(HardLabel, self).__init__()\n",
    "        self.threshold = threshold\n",
    "        self.sample_percentage = sample_percentage\n",
    "\n",
    "    def forward(self, prob, label, rand):\n",
    "        return HardLabelFunction.apply(prob, label, rand, self.threshold, self.sample_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from layers.hough_voting import HoughVoting'''\n",
    "\n",
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch\n",
    "import posecnn_cuda\n",
    "\n",
    "\n",
    "class HoughVotingFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, label, vertex, meta_data, extents, is_train, skip_pixels, \\\n",
    "            label_threshold, inlier_threshold, voting_threshold, per_threshold):\n",
    "\n",
    "        outputs = posecnn_cuda.hough_voting_forward(label, vertex, meta_data, extents, is_train, skip_pixels, \\\n",
    "            label_threshold, inlier_threshold, voting_threshold, per_threshold)\n",
    "\n",
    "        top_box = outputs[0]\n",
    "        top_pose = outputs[1]\n",
    "        return top_box, top_pose\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, top_diff_box, top_diff_pose):\n",
    "        return None, None, None, None, None, None, None, None, None, None\n",
    "\n",
    "\n",
    "class HoughVoting(nn.Module):\n",
    "    def __init__(self, is_train=0, skip_pixels=10, label_threshold=100, inlier_threshold=0.9, voting_threshold=-1, per_threshold=0.01):\n",
    "        super(HoughVoting, self).__init__()\n",
    "        self.is_train = is_train\n",
    "        self.skip_pixels = skip_pixels\n",
    "        self.label_threshold = label_threshold\n",
    "        self.inlier_threshold = inlier_threshold\n",
    "        self.voting_threshold = voting_threshold\n",
    "        self.per_threshold = per_threshold\n",
    "\n",
    "    def forward(self, label, vertex, meta_data, extents):\n",
    "        return HoughVotingFunction.apply(label, vertex, meta_data, extents, self.is_train, self.skip_pixels, \\\n",
    "            self.label_threshold, self.inlier_threshold, self.voting_threshold, self.per_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from layers.roi_pooling import RoIPool'''\n",
    "\n",
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch\n",
    "import posecnn_cuda\n",
    "\n",
    "\n",
    "class RoIPoolFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, features, rois, pool_height, pool_width, spatial_scale):\n",
    "        outputs = posecnn_cuda.roi_pool_forward(pool_height, pool_width, spatial_scale, features, rois)\n",
    "        top_data = outputs[0]\n",
    "        variables = outputs[1:]\n",
    "        variables.append(rois)\n",
    "        ctx.feature_size = features.size()\n",
    "        ctx.spatial_scale = spatial_scale\n",
    "        ctx.save_for_backward(*variables)\n",
    "        return top_data\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, top_diff):\n",
    "        argmax_data = ctx.saved_variables[0]\n",
    "        rois = ctx.saved_variables[1]\n",
    "        batch_size, num_channels, data_height, data_width = ctx.feature_size\n",
    "        spatial_scale = ctx.spatial_scale\n",
    "        outputs = posecnn_cuda.roi_pool_backward(batch_size, data_height, data_width, spatial_scale, top_diff, rois, argmax_data)\n",
    "        d_features = outputs[0]\n",
    "        return d_features, None, None, None, None   \n",
    "\n",
    "class RoIPool(nn.Module):\n",
    "    def __init__(self, pool_height, pool_width, spatial_scale):\n",
    "        super(RoIPool, self).__init__()\n",
    "\n",
    "        self.pool_width = int(pool_width)\n",
    "        self.pool_height = int(pool_height)\n",
    "        self.spatial_scale = float(spatial_scale)\n",
    "\n",
    "    def forward(self, features, rois):\n",
    "        return RoIPoolFunction.apply(features, rois, self.pool_height, self.pool_width, self.spatial_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from layers.point_matching_loss import PMLoss'''\n",
    "\n",
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.autograd import Function\n",
    "import torch\n",
    "import posecnn_cuda\n",
    "\n",
    "class PMLossFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, prediction, target, weight, points, symmetry, hard_angle):\n",
    "        outputs = posecnn_cuda.pml_forward(prediction, target, weight, points, symmetry, hard_angle)\n",
    "        loss = outputs[0]\n",
    "        variables = outputs[1:]\n",
    "        ctx.save_for_backward(*variables)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_loss):\n",
    "        outputs = posecnn_cuda.pml_backward(grad_loss, *ctx.saved_variables)\n",
    "        d_rotation = outputs[0]\n",
    "\n",
    "        return d_rotation, None, None, None, None, None\n",
    "\n",
    "\n",
    "class PMLoss(nn.Module):\n",
    "    def __init__(self, hard_angle=15):\n",
    "        super(PMLoss, self).__init__()\n",
    "        self.hard_angle = hard_angle\n",
    "\n",
    "    def forward(self, prediction, target, weight, points, symmetry):\n",
    "        return PMLossFunction.apply(prediction, target, weight, points, symmetry, self.hard_angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from layers.roi_target_layer import roi_target_layer'''\n",
    "\n",
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from fcn.config import cfg\n",
    "from utils.bbox_transform import bbox_transform\n",
    "from utils.cython_bbox import bbox_overlaps\n",
    "\n",
    "# rpn_rois: (batch_ids, cls, x1, y1, x2, y2, scores)\n",
    "# gt_boxes: batch * num_classes * (x1, y1, x2, y2, cls)\n",
    "def roi_target_layer(rpn_rois, gt_boxes):\n",
    "    \"\"\"\n",
    "    Assign object detection proposals to ground-truth targets. Produces proposal\n",
    "    classification labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "\n",
    "    rpn_rois = rpn_rois.detach().cpu().numpy()\n",
    "    gt_boxes = gt_boxes.detach().cpu().numpy()\n",
    "    num_classes = gt_boxes.shape[1]\n",
    "\n",
    "    # convert boxes to (batch_ids, x1, y1, x2, y2, cls)\n",
    "    roi_blob = rpn_rois[:, (0, 2, 3, 4, 5, 1)]\n",
    "    gt_box_blob = np.zeros((0, 6), dtype=np.float32)\n",
    "    for i in range(gt_boxes.shape[0]):\n",
    "        for j in range(gt_boxes.shape[1]):\n",
    "            if gt_boxes[i, j, -1] > 0:\n",
    "                gt_box = np.zeros((1, 6), dtype=np.float32)\n",
    "                gt_box[0, 0] = i\n",
    "                gt_box[0, 1:5] = gt_boxes[i, j, :4]\n",
    "                gt_box[0, 5] = gt_boxes[i, j, 4]\n",
    "                gt_box_blob = np.concatenate((gt_box_blob, gt_box), axis=0)\n",
    "\n",
    "    # sample rois with classification labels and bounding box regression targets\n",
    "    labels, bbox_targets, bbox_inside_weights = _sample_rois(roi_blob, gt_box_blob, num_classes)\n",
    "    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n",
    "\n",
    "    # convert labels\n",
    "    num = labels.shape[0]\n",
    "    label_blob = np.zeros((num, num_classes), dtype=np.float32)\n",
    "    if np.any(roi_blob[:, -1] > 0):\n",
    "        for i in range(num):\n",
    "            label_blob[i, int(labels[i])] = 1.0\n",
    "\n",
    "    return torch.from_numpy(label_blob).cuda(), torch.from_numpy(bbox_targets).cuda(), \\\n",
    "        torch.from_numpy(bbox_inside_weights).cuda(), torch.from_numpy(bbox_outside_weights).cuda()\n",
    "\n",
    "\n",
    "def _get_bbox_regression_labels(bbox_target_data, num_classes):\n",
    "  \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "  compact form N x (class, tx, ty, tw, th)\n",
    "\n",
    "  This function expands those targets into the 4-of-4*K representation used\n",
    "  by the network (i.e. only one class has non-zero targets).\n",
    "\n",
    "  Returns:\n",
    "      bbox_target (ndarray): N x 4K blob of regression targets\n",
    "      bbox_inside_weights (ndarray): N x 4K blob of loss weights\n",
    "  \"\"\"\n",
    "\n",
    "  clss = bbox_target_data[:, 0]\n",
    "  bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n",
    "  bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n",
    "  inds = np.where(clss > 0)[0]\n",
    "  for ind in inds:\n",
    "    cls = clss[ind]\n",
    "    start = int(4 * cls)\n",
    "    end = start + 4\n",
    "    bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n",
    "    bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n",
    "  return bbox_targets, bbox_inside_weights\n",
    "\n",
    "\n",
    "def _compute_targets(ex_rois, gt_rois, labels):\n",
    "  \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "  assert ex_rois.shape[0] == gt_rois.shape[0]\n",
    "  assert ex_rois.shape[1] == 4\n",
    "  assert gt_rois.shape[1] == 4\n",
    "\n",
    "  targets = bbox_transform(ex_rois, gt_rois)\n",
    "  if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "    # Optionally normalize targets by a precomputed mean and stdev\n",
    "    targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n",
    "               / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n",
    "  return np.hstack(\n",
    "    (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def _sample_rois(all_rois, gt_boxes, num_classes):\n",
    "  \"\"\"Generate a random sample of RoIs comprising foreground and background\n",
    "  examples.\n",
    "  \"\"\"\n",
    "  # all_rois (batch_ids, x1, y1, x2, y2, cls)\n",
    "  # gt_boxes (batch_ids, x1, y1, x2, y2, cls)\n",
    "  # overlaps: (rois x gt_boxes)\n",
    "\n",
    "  if gt_boxes.shape[0] == 0:\n",
    "      num = all_rois.shape[0]\n",
    "      labels = np.zeros((num, 1), dtype=np.float32)\n",
    "      bbox_targets = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "      bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n",
    "  else:\n",
    "      overlaps = bbox_overlaps(\n",
    "        np.ascontiguousarray(all_rois[:, :5], dtype=np.float),\n",
    "        np.ascontiguousarray(gt_boxes[:, :5], dtype=np.float))\n",
    "\n",
    "      gt_assignment = overlaps.argmax(axis=1)\n",
    "      max_overlaps = overlaps.max(axis=1)\n",
    "      labels = gt_boxes[gt_assignment, 5]\n",
    "\n",
    "      # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "      # fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n",
    "      bg_inds = np.where(max_overlaps < cfg.TRAIN.FG_THRESH)[0]\n",
    "      labels[bg_inds] = 0\n",
    "\n",
    "      # print '{:d} rois, {:d} fg, {:d} bg'.format(all_rois.shape[0], all_rois.shape[0]-len(bg_inds), len(bg_inds))\n",
    "      # print all_rois\n",
    "\n",
    "      bbox_target_data = _compute_targets(\n",
    "        all_rois[:, 1:5], gt_boxes[gt_assignment, 1:5], labels)\n",
    "\n",
    "      bbox_targets, bbox_inside_weights = \\\n",
    "        _get_bbox_regression_labels(bbox_target_data, num_classes)\n",
    "\n",
    "  return labels, bbox_targets, bbox_inside_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from layers.pose_target_layer import pose_target_layer'''\n",
    "\n",
    "# Copyright (c) 2020 NVIDIA Corporation. All rights reserved.\n",
    "# This work is licensed under the NVIDIA Source Code License - Non-commercial. Full\n",
    "# text can be found in LICENSE.md\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from fcn.config import cfg\n",
    "from utils.bbox_transform import bbox_transform_inv\n",
    "from utils.cython_bbox import bbox_overlaps\n",
    "\n",
    "# rpn_rois: (batch_ids, cls, x1, y1, x2, y2, scores)\n",
    "# gt_boxes: batch * num_classes * (x1, y1, x2, y2, cls)\n",
    "def pose_target_layer(rois, bbox_prob, bbox_pred, gt_boxes, poses, is_training):\n",
    "\n",
    "    rois = rois.detach().cpu().numpy()\n",
    "    bbox_prob = bbox_prob.detach().cpu().numpy()\n",
    "    bbox_pred = bbox_pred.detach().cpu().numpy()\n",
    "    gt_boxes = gt_boxes.detach().cpu().numpy()\n",
    "    num_classes = bbox_prob.shape[1]\n",
    "  \n",
    "    # process boxes\n",
    "    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "        stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (num_classes))\n",
    "        means = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (num_classes))\n",
    "        bbox_pred *= stds\n",
    "        bbox_pred += means\n",
    "\n",
    "    boxes = rois[:, 2:6].copy()\n",
    "    pred_boxes = bbox_transform_inv(boxes, bbox_pred)\n",
    "\n",
    "    # assign boxes\n",
    "    for i in range(rois.shape[0]):\n",
    "        cls = int(rois[i, 1])\n",
    "        rois[i, 2:6] = pred_boxes[i, cls*4:cls*4+4]\n",
    "        rois[i, 6] = bbox_prob[i, cls]\n",
    "\n",
    "    # convert boxes to (batch_ids, x1, y1, x2, y2, cls)\n",
    "    roi_blob = rois[:, (0, 2, 3, 4, 5, 1)]\n",
    "    gt_box_blob = np.zeros((0, 6), dtype=np.float32)\n",
    "    pose_blob = np.zeros((0, 9), dtype=np.float32)\n",
    "    for i in range(gt_boxes.shape[0]):\n",
    "        for j in range(gt_boxes.shape[1]):\n",
    "            if gt_boxes[i, j, -1] > 0:\n",
    "                gt_box = np.zeros((1, 6), dtype=np.float32)\n",
    "                gt_box[0, 0] = i\n",
    "                gt_box[0, 1:5] = gt_boxes[i, j, :4]\n",
    "                gt_box[0, 5] = gt_boxes[i, j, 4]\n",
    "                gt_box_blob = np.concatenate((gt_box_blob, gt_box), axis=0)\n",
    "                poses[i, j, 0] = i\n",
    "                pose_blob = np.concatenate((pose_blob, poses[i, j, :].cpu().reshape(1, 9)), axis=0)\n",
    "\n",
    "    if gt_box_blob.shape[0] == 0:\n",
    "        num = rois.shape[0]\n",
    "        poses_target = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "        poses_weight = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "    else:\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "        overlaps = bbox_overlaps(\n",
    "            np.ascontiguousarray(roi_blob[:, :5], dtype=np.float),\n",
    "            np.ascontiguousarray(gt_box_blob[:, :5], dtype=np.float))\n",
    "\n",
    "        gt_assignment = overlaps.argmax(axis=1)\n",
    "        max_overlaps = overlaps.max(axis=1)\n",
    "        labels = gt_box_blob[gt_assignment, 5]\n",
    "        quaternions = pose_blob[gt_assignment, 2:6]\n",
    "\n",
    "        # Select foreground RoIs as those with >= FG_THRESH overlap\n",
    "        bg_inds = np.where(max_overlaps < cfg.TRAIN.FG_THRESH_POSE)[0]\n",
    "        labels[bg_inds] = 0\n",
    "\n",
    "        bg_inds = np.where(roi_blob[:, -1] != labels)[0]\n",
    "        labels[bg_inds] = 0\n",
    "\n",
    "        # in training, only use the positive boxes for pose regression\n",
    "        if is_training:\n",
    "            fg_inds = np.where(labels > 0)[0]\n",
    "            if len(fg_inds) > 0:\n",
    "                rois = rois[fg_inds, :]\n",
    "                quaternions = quaternions[fg_inds, :]\n",
    "                labels = labels[fg_inds]\n",
    "    \n",
    "        # pose regression targets and weights\n",
    "        poses_target, poses_weight = _compute_pose_targets(quaternions, labels, num_classes)\n",
    "\n",
    "    return torch.from_numpy(rois).cuda(), torch.from_numpy(poses_target).cuda(), torch.from_numpy(poses_weight).cuda()\n",
    "\n",
    "\n",
    "def _compute_pose_targets(quaternions, labels, num_classes):\n",
    "    \"\"\"Compute pose regression targets for an image.\"\"\"\n",
    "\n",
    "    num = quaternions.shape[0]\n",
    "    poses_target = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "    poses_weight = np.zeros((num, 4 * num_classes), dtype=np.float32)\n",
    "\n",
    "    for i in range(num):\n",
    "        cls = labels[i]\n",
    "        if cls > 0 and np.linalg.norm(quaternions[i, :]) > 0:\n",
    "            start = int(4 * cls)\n",
    "            end = start + 4\n",
    "            poses_target[i, start:end] = quaternions[i, :]\n",
    "            poses_weight[i, start:end] = 1.0\n",
    "\n",
    "    return poses_target, poses_weight\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
